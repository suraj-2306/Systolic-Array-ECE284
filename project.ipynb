{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "radical-fifty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building model...\n",
      "VGG_quant(\n",
      "  (features): Sequential(\n",
      "    (0): QuantConv2d(\n",
      "      3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): QuantConv2d(\n",
      "      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): QuantConv2d(\n",
      "      64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): QuantConv2d(\n",
      "      128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): QuantConv2d(\n",
      "      128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): QuantConv2d(\n",
      "      256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (25): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): QuantConv2d(\n",
      "      8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (28): ReLU(inplace=True)\n",
      "    (29): QuantConv2d(\n",
      "      8, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (30): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (33): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (38): ReLU(inplace=True)\n",
      "    (39): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (40): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (41): ReLU(inplace=True)\n",
      "    (42): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (43): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "     \n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import *\n",
    "\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('=> Building model...')\n",
    "    \n",
    "    \n",
    "    \n",
    "batch_size = 128\n",
    "model_name = \"VGG16_project\"\n",
    "model = VGG16_project()\n",
    "\n",
    "print(model)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "            \n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "         \n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "        \n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))  \n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "#all_params = checkpoint['state_dict']\n",
    "#model.load_state_dict(all_params, strict=False)\n",
    "#criterion = nn.CrossEntropyLoss().cuda()\n",
    "#validate(testloader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2107ceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    adjust_list = [20, 40]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1   \n",
    "#             param_group['momentum'] = param_group['momentum']*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "junior-reminder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/391]\tTime 1.716 (1.716)\tData 0.273 (0.273)\tLoss 2.5926 (2.5926)\tPrec 13.281% (13.281%)\n",
      "Epoch: [0][100/391]\tTime 0.047 (0.064)\tData 0.002 (0.005)\tLoss 2.2441 (3.5745)\tPrec 22.656% (12.028%)\n",
      "Epoch: [0][200/391]\tTime 0.044 (0.055)\tData 0.001 (0.004)\tLoss 2.1439 (2.9037)\tPrec 16.406% (14.039%)\n",
      "Epoch: [0][300/391]\tTime 0.044 (0.053)\tData 0.002 (0.003)\tLoss 2.1655 (2.6523)\tPrec 21.094% (15.742%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.241 (0.241)\tLoss 2.0231 (2.0231)\tPrec 26.562% (26.562%)\n",
      " * Prec 20.690% \n",
      "best acc: 20.690000\n",
      "Epoch: [1][0/391]\tTime 0.304 (0.304)\tData 0.262 (0.262)\tLoss 2.0229 (2.0229)\tPrec 26.562% (26.562%)\n",
      "Epoch: [1][100/391]\tTime 0.043 (0.049)\tData 0.005 (0.005)\tLoss 2.0300 (2.0375)\tPrec 25.781% (21.573%)\n",
      "Epoch: [1][200/391]\tTime 0.050 (0.048)\tData 0.002 (0.003)\tLoss 2.0300 (2.0273)\tPrec 20.312% (21.922%)\n",
      "Epoch: [1][300/391]\tTime 0.045 (0.047)\tData 0.001 (0.003)\tLoss 1.9418 (2.0069)\tPrec 25.000% (22.368%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.251 (0.251)\tLoss 1.8421 (1.8421)\tPrec 21.094% (21.094%)\n",
      " * Prec 26.440% \n",
      "best acc: 26.440000\n",
      "Epoch: [2][0/391]\tTime 0.304 (0.304)\tData 0.267 (0.267)\tLoss 1.8544 (1.8544)\tPrec 26.562% (26.562%)\n",
      "Epoch: [2][100/391]\tTime 0.048 (0.049)\tData 0.002 (0.005)\tLoss 1.8875 (1.8756)\tPrec 27.344% (26.114%)\n",
      "Epoch: [2][200/391]\tTime 0.053 (0.048)\tData 0.003 (0.003)\tLoss 1.7301 (1.8699)\tPrec 31.250% (26.349%)\n",
      "Epoch: [2][300/391]\tTime 0.043 (0.047)\tData 0.002 (0.003)\tLoss 1.6795 (1.8594)\tPrec 28.125% (26.749%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.227 (0.227)\tLoss 1.6894 (1.6894)\tPrec 36.719% (36.719%)\n",
      " * Prec 30.930% \n",
      "best acc: 30.930000\n",
      "Epoch: [3][0/391]\tTime 0.318 (0.318)\tData 0.271 (0.271)\tLoss 1.8469 (1.8469)\tPrec 27.344% (27.344%)\n",
      "Epoch: [3][100/391]\tTime 0.039 (0.049)\tData 0.002 (0.005)\tLoss 1.7068 (1.7830)\tPrec 28.125% (29.711%)\n",
      "Epoch: [3][200/391]\tTime 0.049 (0.048)\tData 0.002 (0.003)\tLoss 1.7035 (1.7767)\tPrec 28.125% (29.750%)\n",
      "Epoch: [3][300/391]\tTime 0.049 (0.047)\tData 0.002 (0.003)\tLoss 1.7123 (1.7595)\tPrec 32.031% (30.443%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.237 (0.237)\tLoss 1.6056 (1.6056)\tPrec 40.625% (40.625%)\n",
      " * Prec 34.260% \n",
      "best acc: 34.260000\n",
      "Epoch: [4][0/391]\tTime 0.305 (0.305)\tData 0.254 (0.254)\tLoss 1.6834 (1.6834)\tPrec 32.812% (32.812%)\n",
      "Epoch: [4][100/391]\tTime 0.042 (0.049)\tData 0.002 (0.005)\tLoss 1.5413 (1.6797)\tPrec 42.969% (33.292%)\n",
      "Epoch: [4][200/391]\tTime 0.045 (0.048)\tData 0.004 (0.003)\tLoss 1.6034 (1.6565)\tPrec 39.062% (34.608%)\n",
      "Epoch: [4][300/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 1.7438 (1.6446)\tPrec 28.906% (35.382%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.240 (0.240)\tLoss 1.4697 (1.4697)\tPrec 47.656% (47.656%)\n",
      " * Prec 39.450% \n",
      "best acc: 39.450000\n",
      "Epoch: [5][0/391]\tTime 0.305 (0.305)\tData 0.261 (0.261)\tLoss 1.5266 (1.5266)\tPrec 40.625% (40.625%)\n",
      "Epoch: [5][100/391]\tTime 0.043 (0.049)\tData 0.002 (0.005)\tLoss 1.4757 (1.5633)\tPrec 44.531% (39.635%)\n",
      "Epoch: [5][200/391]\tTime 0.049 (0.049)\tData 0.002 (0.003)\tLoss 1.6173 (1.5554)\tPrec 39.844% (40.026%)\n",
      "Epoch: [5][300/391]\tTime 0.057 (0.048)\tData 0.002 (0.003)\tLoss 1.4514 (1.5396)\tPrec 42.188% (41.009%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.235 (0.235)\tLoss 1.4107 (1.4107)\tPrec 47.656% (47.656%)\n",
      " * Prec 46.010% \n",
      "best acc: 46.010000\n",
      "Epoch: [6][0/391]\tTime 0.285 (0.285)\tData 0.242 (0.242)\tLoss 1.3494 (1.3494)\tPrec 47.656% (47.656%)\n",
      "Epoch: [6][100/391]\tTime 0.044 (0.049)\tData 0.004 (0.005)\tLoss 1.4596 (1.4614)\tPrec 42.188% (45.166%)\n",
      "Epoch: [6][200/391]\tTime 0.049 (0.048)\tData 0.002 (0.003)\tLoss 1.4560 (1.4345)\tPrec 49.219% (46.331%)\n",
      "Epoch: [6][300/391]\tTime 0.044 (0.047)\tData 0.002 (0.003)\tLoss 1.4033 (1.4098)\tPrec 43.750% (47.373%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.249 (0.249)\tLoss 1.3679 (1.3679)\tPrec 46.875% (46.875%)\n",
      " * Prec 51.670% \n",
      "best acc: 51.670000\n",
      "Epoch: [7][0/391]\tTime 0.303 (0.303)\tData 0.248 (0.248)\tLoss 1.3634 (1.3634)\tPrec 46.094% (46.094%)\n",
      "Epoch: [7][100/391]\tTime 0.040 (0.050)\tData 0.003 (0.004)\tLoss 1.1612 (1.3290)\tPrec 56.250% (51.346%)\n",
      "Epoch: [7][200/391]\tTime 0.046 (0.048)\tData 0.002 (0.003)\tLoss 1.0685 (1.3058)\tPrec 60.156% (51.761%)\n",
      "Epoch: [7][300/391]\tTime 0.044 (0.048)\tData 0.002 (0.003)\tLoss 1.2961 (1.2855)\tPrec 50.000% (52.614%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.237 (0.237)\tLoss 1.3783 (1.3783)\tPrec 49.219% (49.219%)\n",
      " * Prec 53.550% \n",
      "best acc: 53.550000\n",
      "Epoch: [8][0/391]\tTime 0.329 (0.329)\tData 0.284 (0.284)\tLoss 1.0813 (1.0813)\tPrec 58.594% (58.594%)\n",
      "Epoch: [8][100/391]\tTime 0.039 (0.049)\tData 0.003 (0.005)\tLoss 1.0133 (1.2113)\tPrec 61.719% (56.443%)\n",
      "Epoch: [8][200/391]\tTime 0.052 (0.048)\tData 0.002 (0.003)\tLoss 1.0234 (1.1884)\tPrec 62.500% (56.996%)\n",
      "Epoch: [8][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 1.1134 (1.1719)\tPrec 57.812% (57.792%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.235 (0.235)\tLoss 1.1739 (1.1739)\tPrec 58.594% (58.594%)\n",
      " * Prec 58.630% \n",
      "best acc: 58.630000\n",
      "Epoch: [9][0/391]\tTime 0.300 (0.300)\tData 0.251 (0.251)\tLoss 1.0798 (1.0798)\tPrec 61.719% (61.719%)\n",
      "Epoch: [9][100/391]\tTime 0.049 (0.049)\tData 0.001 (0.005)\tLoss 1.0961 (1.0870)\tPrec 57.812% (61.610%)\n",
      "Epoch: [9][200/391]\tTime 0.045 (0.048)\tData 0.002 (0.003)\tLoss 1.0919 (1.0663)\tPrec 58.594% (62.088%)\n",
      "Epoch: [9][300/391]\tTime 0.048 (0.048)\tData 0.002 (0.003)\tLoss 1.1868 (1.0512)\tPrec 56.250% (62.557%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.239 (0.239)\tLoss 0.9131 (0.9131)\tPrec 67.969% (67.969%)\n",
      " * Prec 64.580% \n",
      "best acc: 64.580000\n",
      "Epoch: [10][0/391]\tTime 0.303 (0.303)\tData 0.261 (0.261)\tLoss 0.9521 (0.9521)\tPrec 65.625% (65.625%)\n",
      "Epoch: [10][100/391]\tTime 0.051 (0.049)\tData 0.002 (0.005)\tLoss 0.8873 (0.9773)\tPrec 71.094% (65.401%)\n",
      "Epoch: [10][200/391]\tTime 0.039 (0.048)\tData 0.003 (0.003)\tLoss 0.8518 (0.9694)\tPrec 67.188% (65.800%)\n",
      "Epoch: [10][300/391]\tTime 0.054 (0.047)\tData 0.002 (0.003)\tLoss 0.8393 (0.9681)\tPrec 70.312% (65.890%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.241 (0.241)\tLoss 0.7935 (0.7935)\tPrec 67.969% (67.969%)\n",
      " * Prec 67.110% \n",
      "best acc: 67.110000\n",
      "Epoch: [11][0/391]\tTime 0.292 (0.292)\tData 0.258 (0.258)\tLoss 0.9756 (0.9756)\tPrec 64.844% (64.844%)\n",
      "Epoch: [11][100/391]\tTime 0.047 (0.049)\tData 0.002 (0.005)\tLoss 0.9139 (0.8847)\tPrec 67.969% (68.441%)\n",
      "Epoch: [11][200/391]\tTime 0.043 (0.048)\tData 0.002 (0.003)\tLoss 0.9019 (0.8902)\tPrec 69.531% (68.427%)\n",
      "Epoch: [11][300/391]\tTime 0.044 (0.047)\tData 0.002 (0.003)\tLoss 0.8179 (0.8855)\tPrec 68.750% (68.711%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.240 (0.240)\tLoss 0.7354 (0.7354)\tPrec 71.875% (71.875%)\n",
      " * Prec 69.890% \n",
      "best acc: 69.890000\n",
      "Epoch: [12][0/391]\tTime 0.329 (0.329)\tData 0.280 (0.280)\tLoss 0.8869 (0.8869)\tPrec 74.219% (74.219%)\n",
      "Epoch: [12][100/391]\tTime 0.046 (0.049)\tData 0.002 (0.005)\tLoss 0.8020 (0.8516)\tPrec 70.312% (69.872%)\n",
      "Epoch: [12][200/391]\tTime 0.053 (0.048)\tData 0.002 (0.003)\tLoss 0.7620 (0.8283)\tPrec 71.094% (70.725%)\n",
      "Epoch: [12][300/391]\tTime 0.052 (0.047)\tData 0.002 (0.003)\tLoss 0.7096 (0.8186)\tPrec 74.219% (71.244%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.225 (0.225)\tLoss 0.8365 (0.8365)\tPrec 68.750% (68.750%)\n",
      " * Prec 68.350% \n",
      "best acc: 69.890000\n",
      "Epoch: [13][0/391]\tTime 0.336 (0.336)\tData 0.292 (0.292)\tLoss 0.7394 (0.7394)\tPrec 77.344% (77.344%)\n",
      "Epoch: [13][100/391]\tTime 0.047 (0.049)\tData 0.002 (0.005)\tLoss 0.8235 (0.7859)\tPrec 74.219% (72.324%)\n",
      "Epoch: [13][200/391]\tTime 0.051 (0.048)\tData 0.001 (0.003)\tLoss 0.8507 (0.7768)\tPrec 70.312% (72.586%)\n",
      "Epoch: [13][300/391]\tTime 0.048 (0.047)\tData 0.002 (0.003)\tLoss 0.8244 (0.7744)\tPrec 71.875% (72.770%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.261 (0.261)\tLoss 0.7133 (0.7133)\tPrec 77.344% (77.344%)\n",
      " * Prec 72.900% \n",
      "best acc: 72.900000\n",
      "Epoch: [14][0/391]\tTime 0.330 (0.330)\tData 0.283 (0.283)\tLoss 0.6255 (0.6255)\tPrec 78.125% (78.125%)\n",
      "Epoch: [14][100/391]\tTime 0.054 (0.049)\tData 0.002 (0.005)\tLoss 0.6460 (0.7294)\tPrec 75.781% (74.667%)\n",
      "Epoch: [14][200/391]\tTime 0.049 (0.048)\tData 0.001 (0.003)\tLoss 0.7486 (0.7310)\tPrec 71.094% (74.697%)\n",
      "Epoch: [14][300/391]\tTime 0.044 (0.048)\tData 0.003 (0.003)\tLoss 0.6935 (0.7258)\tPrec 78.125% (74.907%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.260 (0.260)\tLoss 0.8063 (0.8063)\tPrec 70.312% (70.312%)\n",
      " * Prec 73.590% \n",
      "best acc: 73.590000\n",
      "Epoch: [15][0/391]\tTime 0.292 (0.292)\tData 0.248 (0.248)\tLoss 0.6539 (0.6539)\tPrec 75.781% (75.781%)\n",
      "Epoch: [15][100/391]\tTime 0.050 (0.049)\tData 0.002 (0.005)\tLoss 0.6468 (0.6835)\tPrec 77.344% (76.400%)\n",
      "Epoch: [15][200/391]\tTime 0.041 (0.048)\tData 0.003 (0.003)\tLoss 0.7109 (0.6849)\tPrec 74.219% (76.368%)\n",
      "Epoch: [15][300/391]\tTime 0.053 (0.047)\tData 0.002 (0.003)\tLoss 0.4782 (0.6784)\tPrec 83.594% (76.550%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.251 (0.251)\tLoss 0.6981 (0.6981)\tPrec 78.906% (78.906%)\n",
      " * Prec 74.820% \n",
      "best acc: 74.820000\n",
      "Epoch: [16][0/391]\tTime 0.319 (0.319)\tData 0.258 (0.258)\tLoss 0.4877 (0.4877)\tPrec 83.594% (83.594%)\n",
      "Epoch: [16][100/391]\tTime 0.046 (0.050)\tData 0.002 (0.005)\tLoss 0.7430 (0.6390)\tPrec 73.438% (78.164%)\n",
      "Epoch: [16][200/391]\tTime 0.046 (0.048)\tData 0.002 (0.003)\tLoss 0.6171 (0.6394)\tPrec 75.781% (78.098%)\n",
      "Epoch: [16][300/391]\tTime 0.052 (0.047)\tData 0.002 (0.003)\tLoss 0.6524 (0.6363)\tPrec 75.000% (78.291%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.252 (0.252)\tLoss 0.5389 (0.5389)\tPrec 83.594% (83.594%)\n",
      " * Prec 78.410% \n",
      "best acc: 78.410000\n",
      "Epoch: [17][0/391]\tTime 0.291 (0.291)\tData 0.244 (0.244)\tLoss 0.6501 (0.6501)\tPrec 75.781% (75.781%)\n",
      "Epoch: [17][100/391]\tTime 0.041 (0.049)\tData 0.002 (0.004)\tLoss 0.7401 (0.5992)\tPrec 71.875% (79.154%)\n",
      "Epoch: [17][200/391]\tTime 0.047 (0.048)\tData 0.002 (0.003)\tLoss 0.3759 (0.6034)\tPrec 88.281% (79.209%)\n",
      "Epoch: [17][300/391]\tTime 0.051 (0.048)\tData 0.002 (0.003)\tLoss 0.5760 (0.6068)\tPrec 81.250% (79.179%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.200 (0.200)\tLoss 0.8305 (0.8305)\tPrec 71.875% (71.875%)\n",
      " * Prec 71.420% \n",
      "best acc: 78.410000\n",
      "Epoch: [18][0/391]\tTime 0.299 (0.299)\tData 0.247 (0.247)\tLoss 0.4943 (0.4943)\tPrec 86.719% (86.719%)\n",
      "Epoch: [18][100/391]\tTime 0.041 (0.049)\tData 0.002 (0.005)\tLoss 0.5602 (0.5895)\tPrec 82.812% (79.680%)\n",
      "Epoch: [18][200/391]\tTime 0.048 (0.049)\tData 0.002 (0.003)\tLoss 0.4419 (0.5757)\tPrec 85.938% (80.204%)\n",
      "Epoch: [18][300/391]\tTime 0.046 (0.048)\tData 0.002 (0.003)\tLoss 0.5283 (0.5711)\tPrec 83.594% (80.417%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.234 (0.234)\tLoss 0.5276 (0.5276)\tPrec 79.688% (79.688%)\n",
      " * Prec 78.500% \n",
      "best acc: 78.500000\n",
      "Epoch: [19][0/391]\tTime 0.296 (0.296)\tData 0.246 (0.246)\tLoss 0.5039 (0.5039)\tPrec 82.031% (82.031%)\n",
      "Epoch: [19][100/391]\tTime 0.039 (0.049)\tData 0.003 (0.005)\tLoss 0.4409 (0.5405)\tPrec 85.938% (81.498%)\n",
      "Epoch: [19][200/391]\tTime 0.047 (0.048)\tData 0.002 (0.003)\tLoss 0.7562 (0.5448)\tPrec 74.219% (81.448%)\n",
      "Epoch: [19][300/391]\tTime 0.039 (0.047)\tData 0.003 (0.003)\tLoss 0.4942 (0.5449)\tPrec 82.812% (81.465%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.235 (0.235)\tLoss 0.5319 (0.5319)\tPrec 80.469% (80.469%)\n",
      " * Prec 79.440% \n",
      "best acc: 79.440000\n",
      "Epoch: [20][0/391]\tTime 0.324 (0.324)\tData 0.275 (0.275)\tLoss 0.3744 (0.3744)\tPrec 88.281% (88.281%)\n",
      "Epoch: [20][100/391]\tTime 0.046 (0.051)\tData 0.002 (0.005)\tLoss 0.3873 (0.4586)\tPrec 87.500% (84.452%)\n",
      "Epoch: [20][200/391]\tTime 0.052 (0.049)\tData 0.002 (0.003)\tLoss 0.3681 (0.4412)\tPrec 85.156% (85.098%)\n",
      "Epoch: [20][300/391]\tTime 0.049 (0.048)\tData 0.002 (0.003)\tLoss 0.3341 (0.4388)\tPrec 88.281% (85.167%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.205 (0.205)\tLoss 0.3468 (0.3468)\tPrec 86.719% (86.719%)\n",
      " * Prec 83.510% \n",
      "best acc: 83.510000\n",
      "Epoch: [21][0/391]\tTime 0.291 (0.291)\tData 0.243 (0.243)\tLoss 0.3464 (0.3464)\tPrec 87.500% (87.500%)\n",
      "Epoch: [21][100/391]\tTime 0.046 (0.049)\tData 0.002 (0.004)\tLoss 0.2852 (0.3949)\tPrec 89.062% (86.525%)\n",
      "Epoch: [21][200/391]\tTime 0.048 (0.048)\tData 0.002 (0.003)\tLoss 0.6008 (0.4093)\tPrec 79.688% (86.245%)\n",
      "Epoch: [21][300/391]\tTime 0.052 (0.047)\tData 0.002 (0.003)\tLoss 0.3536 (0.4080)\tPrec 92.188% (86.161%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.250 (0.250)\tLoss 0.3521 (0.3521)\tPrec 87.500% (87.500%)\n",
      " * Prec 84.130% \n",
      "best acc: 84.130000\n",
      "Epoch: [22][0/391]\tTime 0.299 (0.299)\tData 0.256 (0.256)\tLoss 0.4621 (0.4621)\tPrec 82.812% (82.812%)\n",
      "Epoch: [22][100/391]\tTime 0.047 (0.049)\tData 0.002 (0.005)\tLoss 0.3639 (0.3838)\tPrec 88.281% (86.943%)\n",
      "Epoch: [22][200/391]\tTime 0.040 (0.048)\tData 0.003 (0.003)\tLoss 0.4411 (0.3850)\tPrec 85.938% (86.921%)\n",
      "Epoch: [22][300/391]\tTime 0.049 (0.047)\tData 0.002 (0.003)\tLoss 0.2789 (0.3909)\tPrec 92.188% (86.654%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.258 (0.258)\tLoss 0.3185 (0.3185)\tPrec 88.281% (88.281%)\n",
      " * Prec 84.100% \n",
      "best acc: 84.130000\n",
      "Epoch: [23][0/391]\tTime 0.335 (0.335)\tData 0.287 (0.287)\tLoss 0.2924 (0.2924)\tPrec 92.188% (92.188%)\n",
      "Epoch: [23][100/391]\tTime 0.050 (0.052)\tData 0.002 (0.005)\tLoss 0.3471 (0.3881)\tPrec 89.844% (86.757%)\n",
      "Epoch: [23][200/391]\tTime 0.054 (0.049)\tData 0.002 (0.003)\tLoss 0.3282 (0.3859)\tPrec 85.156% (86.695%)\n",
      "Epoch: [23][300/391]\tTime 0.039 (0.048)\tData 0.002 (0.003)\tLoss 0.3214 (0.3875)\tPrec 89.844% (86.714%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.231 (0.231)\tLoss 0.3190 (0.3190)\tPrec 88.281% (88.281%)\n",
      " * Prec 84.330% \n",
      "best acc: 84.330000\n",
      "Epoch: [24][0/391]\tTime 0.309 (0.309)\tData 0.261 (0.261)\tLoss 0.4009 (0.4009)\tPrec 84.375% (84.375%)\n",
      "Epoch: [24][100/391]\tTime 0.058 (0.049)\tData 0.002 (0.005)\tLoss 0.2808 (0.3718)\tPrec 92.188% (87.608%)\n",
      "Epoch: [24][200/391]\tTime 0.040 (0.048)\tData 0.002 (0.003)\tLoss 0.3168 (0.3763)\tPrec 89.844% (87.411%)\n",
      "Epoch: [24][300/391]\tTime 0.048 (0.047)\tData 0.002 (0.003)\tLoss 0.2741 (0.3739)\tPrec 90.625% (87.412%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.249 (0.249)\tLoss 0.3297 (0.3297)\tPrec 87.500% (87.500%)\n",
      " * Prec 84.440% \n",
      "best acc: 84.440000\n",
      "Epoch: [25][0/391]\tTime 0.279 (0.279)\tData 0.231 (0.231)\tLoss 0.3829 (0.3829)\tPrec 85.156% (85.156%)\n",
      "Epoch: [25][100/391]\tTime 0.054 (0.048)\tData 0.001 (0.004)\tLoss 0.2822 (0.3695)\tPrec 87.500% (87.307%)\n",
      "Epoch: [25][200/391]\tTime 0.038 (0.047)\tData 0.002 (0.003)\tLoss 0.3746 (0.3638)\tPrec 87.500% (87.578%)\n",
      "Epoch: [25][300/391]\tTime 0.048 (0.047)\tData 0.002 (0.003)\tLoss 0.4423 (0.3678)\tPrec 85.938% (87.453%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.243 (0.243)\tLoss 0.3225 (0.3225)\tPrec 89.062% (89.062%)\n",
      " * Prec 84.760% \n",
      "best acc: 84.760000\n",
      "Epoch: [26][0/391]\tTime 0.293 (0.293)\tData 0.251 (0.251)\tLoss 0.3280 (0.3280)\tPrec 89.844% (89.844%)\n",
      "Epoch: [26][100/391]\tTime 0.048 (0.049)\tData 0.003 (0.005)\tLoss 0.3407 (0.3483)\tPrec 86.719% (87.833%)\n",
      "Epoch: [26][200/391]\tTime 0.047 (0.048)\tData 0.002 (0.003)\tLoss 0.3112 (0.3583)\tPrec 88.281% (87.628%)\n",
      "Epoch: [26][300/391]\tTime 0.039 (0.047)\tData 0.002 (0.003)\tLoss 0.2950 (0.3633)\tPrec 89.844% (87.490%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.203 (0.203)\tLoss 0.3279 (0.3279)\tPrec 88.281% (88.281%)\n",
      " * Prec 84.790% \n",
      "best acc: 84.790000\n",
      "Epoch: [27][0/391]\tTime 0.328 (0.328)\tData 0.276 (0.276)\tLoss 0.3232 (0.3232)\tPrec 89.844% (89.844%)\n",
      "Epoch: [27][100/391]\tTime 0.054 (0.049)\tData 0.002 (0.005)\tLoss 0.2838 (0.3373)\tPrec 87.500% (88.266%)\n",
      "Epoch: [27][200/391]\tTime 0.043 (0.048)\tData 0.002 (0.003)\tLoss 0.3929 (0.3515)\tPrec 85.938% (87.861%)\n",
      "Epoch: [27][300/391]\tTime 0.043 (0.047)\tData 0.003 (0.003)\tLoss 0.3408 (0.3581)\tPrec 86.719% (87.653%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.235 (0.235)\tLoss 0.2966 (0.2966)\tPrec 89.062% (89.062%)\n",
      " * Prec 84.680% \n",
      "best acc: 84.790000\n",
      "Epoch: [28][0/391]\tTime 0.333 (0.333)\tData 0.283 (0.283)\tLoss 0.3187 (0.3187)\tPrec 88.281% (88.281%)\n",
      "Epoch: [28][100/391]\tTime 0.051 (0.049)\tData 0.002 (0.005)\tLoss 0.2424 (0.3406)\tPrec 92.188% (87.995%)\n",
      "Epoch: [28][200/391]\tTime 0.047 (0.048)\tData 0.002 (0.003)\tLoss 0.2608 (0.3398)\tPrec 92.188% (88.079%)\n",
      "Epoch: [28][300/391]\tTime 0.045 (0.048)\tData 0.002 (0.003)\tLoss 0.4072 (0.3451)\tPrec 87.500% (87.946%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.271 (0.271)\tLoss 0.3537 (0.3537)\tPrec 87.500% (87.500%)\n",
      " * Prec 84.820% \n",
      "best acc: 84.820000\n",
      "Epoch: [29][0/391]\tTime 0.301 (0.301)\tData 0.251 (0.251)\tLoss 0.3117 (0.3117)\tPrec 89.062% (89.062%)\n",
      "Epoch: [29][100/391]\tTime 0.043 (0.049)\tData 0.002 (0.004)\tLoss 0.3680 (0.3605)\tPrec 86.719% (87.276%)\n",
      "Epoch: [29][200/391]\tTime 0.046 (0.048)\tData 0.002 (0.003)\tLoss 0.4384 (0.3497)\tPrec 85.938% (87.826%)\n",
      "Epoch: [29][300/391]\tTime 0.042 (0.047)\tData 0.002 (0.003)\tLoss 0.3442 (0.3442)\tPrec 90.625% (88.061%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.234 (0.234)\tLoss 0.3259 (0.3259)\tPrec 89.844% (89.844%)\n",
      " * Prec 85.150% \n",
      "best acc: 85.150000\n",
      "Epoch: [30][0/391]\tTime 0.366 (0.366)\tData 0.314 (0.314)\tLoss 0.3265 (0.3265)\tPrec 89.844% (89.844%)\n",
      "Epoch: [30][100/391]\tTime 0.043 (0.049)\tData 0.003 (0.005)\tLoss 0.2950 (0.3256)\tPrec 92.188% (88.869%)\n",
      "Epoch: [30][200/391]\tTime 0.045 (0.048)\tData 0.002 (0.004)\tLoss 0.2514 (0.3290)\tPrec 89.844% (88.732%)\n",
      "Epoch: [30][300/391]\tTime 0.049 (0.047)\tData 0.002 (0.003)\tLoss 0.3145 (0.3323)\tPrec 88.281% (88.595%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.227 (0.227)\tLoss 0.3850 (0.3850)\tPrec 88.281% (88.281%)\n",
      " * Prec 84.560% \n",
      "best acc: 85.150000\n",
      "Epoch: [31][0/391]\tTime 0.258 (0.258)\tData 0.213 (0.213)\tLoss 0.3568 (0.3568)\tPrec 87.500% (87.500%)\n",
      "Epoch: [31][100/391]\tTime 0.052 (0.048)\tData 0.001 (0.004)\tLoss 0.2909 (0.3328)\tPrec 87.500% (88.591%)\n",
      "Epoch: [31][200/391]\tTime 0.044 (0.047)\tData 0.001 (0.003)\tLoss 0.3230 (0.3366)\tPrec 89.062% (88.413%)\n",
      "Epoch: [31][300/391]\tTime 0.049 (0.047)\tData 0.002 (0.003)\tLoss 0.2353 (0.3329)\tPrec 88.281% (88.564%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.254 (0.254)\tLoss 0.3895 (0.3895)\tPrec 87.500% (87.500%)\n",
      " * Prec 84.690% \n",
      "best acc: 85.150000\n",
      "Epoch: [32][0/391]\tTime 0.620 (0.620)\tData 0.568 (0.568)\tLoss 0.2364 (0.2364)\tPrec 92.969% (92.969%)\n",
      "Epoch: [32][100/391]\tTime 0.042 (0.052)\tData 0.002 (0.008)\tLoss 0.3517 (0.3292)\tPrec 89.062% (88.266%)\n",
      "Epoch: [32][200/391]\tTime 0.049 (0.050)\tData 0.002 (0.005)\tLoss 0.3678 (0.3230)\tPrec 88.281% (88.616%)\n",
      "Epoch: [32][300/391]\tTime 0.046 (0.049)\tData 0.002 (0.004)\tLoss 0.3377 (0.3307)\tPrec 86.719% (88.424%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.218 (0.218)\tLoss 0.3752 (0.3752)\tPrec 87.500% (87.500%)\n",
      " * Prec 85.400% \n",
      "best acc: 85.400000\n",
      "Epoch: [33][0/391]\tTime 0.286 (0.286)\tData 0.236 (0.236)\tLoss 0.3621 (0.3621)\tPrec 86.719% (86.719%)\n",
      "Epoch: [33][100/391]\tTime 0.049 (0.049)\tData 0.002 (0.004)\tLoss 0.3076 (0.3304)\tPrec 89.062% (88.730%)\n",
      "Epoch: [33][200/391]\tTime 0.041 (0.048)\tData 0.003 (0.003)\tLoss 0.2717 (0.3299)\tPrec 91.406% (88.767%)\n",
      "Epoch: [33][300/391]\tTime 0.055 (0.048)\tData 0.002 (0.003)\tLoss 0.3148 (0.3293)\tPrec 88.281% (88.748%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.239 (0.239)\tLoss 0.3987 (0.3987)\tPrec 85.938% (85.938%)\n",
      " * Prec 85.260% \n",
      "best acc: 85.400000\n",
      "Epoch: [34][0/391]\tTime 0.283 (0.283)\tData 0.237 (0.237)\tLoss 0.3371 (0.3371)\tPrec 88.281% (88.281%)\n",
      "Epoch: [34][100/391]\tTime 0.046 (0.049)\tData 0.002 (0.004)\tLoss 0.3564 (0.3218)\tPrec 85.156% (89.016%)\n",
      "Epoch: [34][200/391]\tTime 0.047 (0.048)\tData 0.002 (0.003)\tLoss 0.2417 (0.3215)\tPrec 92.969% (89.012%)\n",
      "Epoch: [34][300/391]\tTime 0.042 (0.047)\tData 0.002 (0.003)\tLoss 0.1817 (0.3194)\tPrec 92.969% (89.039%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.265 (0.265)\tLoss 0.4148 (0.4148)\tPrec 87.500% (87.500%)\n",
      " * Prec 85.430% \n",
      "best acc: 85.430000\n",
      "Epoch: [35][0/391]\tTime 0.288 (0.288)\tData 0.245 (0.245)\tLoss 0.3346 (0.3346)\tPrec 88.281% (88.281%)\n",
      "Epoch: [35][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.004)\tLoss 0.2756 (0.3158)\tPrec 89.844% (89.055%)\n",
      "Epoch: [35][200/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.3321 (0.3156)\tPrec 89.844% (89.132%)\n",
      "Epoch: [35][300/391]\tTime 0.047 (0.047)\tData 0.001 (0.003)\tLoss 0.3527 (0.3121)\tPrec 87.500% (89.197%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.256 (0.256)\tLoss 0.3434 (0.3434)\tPrec 90.625% (90.625%)\n",
      " * Prec 85.710% \n",
      "best acc: 85.710000\n",
      "Epoch: [36][0/391]\tTime 0.329 (0.329)\tData 0.278 (0.278)\tLoss 0.3077 (0.3077)\tPrec 89.844% (89.844%)\n",
      "Epoch: [36][100/391]\tTime 0.041 (0.049)\tData 0.002 (0.005)\tLoss 0.3531 (0.3146)\tPrec 85.156% (89.356%)\n",
      "Epoch: [36][200/391]\tTime 0.051 (0.048)\tData 0.002 (0.004)\tLoss 0.2642 (0.3096)\tPrec 91.406% (89.300%)\n",
      "Epoch: [36][300/391]\tTime 0.044 (0.048)\tData 0.002 (0.003)\tLoss 0.5350 (0.3120)\tPrec 82.812% (89.286%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.225 (0.225)\tLoss 0.3186 (0.3186)\tPrec 90.625% (90.625%)\n",
      " * Prec 85.590% \n",
      "best acc: 85.710000\n",
      "Epoch: [37][0/391]\tTime 0.306 (0.306)\tData 0.249 (0.249)\tLoss 0.3814 (0.3814)\tPrec 85.938% (85.938%)\n",
      "Epoch: [37][100/391]\tTime 0.038 (0.049)\tData 0.003 (0.004)\tLoss 0.2606 (0.2921)\tPrec 92.188% (89.743%)\n",
      "Epoch: [37][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.003)\tLoss 0.2373 (0.2956)\tPrec 92.969% (89.704%)\n",
      "Epoch: [37][300/391]\tTime 0.050 (0.047)\tData 0.001 (0.003)\tLoss 0.3572 (0.2967)\tPrec 87.500% (89.807%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.210 (0.210)\tLoss 0.3474 (0.3474)\tPrec 89.844% (89.844%)\n",
      " * Prec 85.210% \n",
      "best acc: 85.710000\n",
      "Epoch: [38][0/391]\tTime 0.292 (0.292)\tData 0.253 (0.253)\tLoss 0.3729 (0.3729)\tPrec 88.281% (88.281%)\n",
      "Epoch: [38][100/391]\tTime 0.048 (0.049)\tData 0.002 (0.005)\tLoss 0.2222 (0.2995)\tPrec 91.406% (89.674%)\n",
      "Epoch: [38][200/391]\tTime 0.044 (0.047)\tData 0.002 (0.003)\tLoss 0.2384 (0.3013)\tPrec 89.844% (89.591%)\n",
      "Epoch: [38][300/391]\tTime 0.051 (0.047)\tData 0.002 (0.003)\tLoss 0.2975 (0.2964)\tPrec 91.406% (89.781%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.246 (0.246)\tLoss 0.3117 (0.3117)\tPrec 90.625% (90.625%)\n",
      " * Prec 85.710% \n",
      "best acc: 85.710000\n",
      "Epoch: [39][0/391]\tTime 0.287 (0.287)\tData 0.247 (0.247)\tLoss 0.3559 (0.3559)\tPrec 84.375% (84.375%)\n",
      "Epoch: [39][100/391]\tTime 0.051 (0.048)\tData 0.001 (0.004)\tLoss 0.2172 (0.2947)\tPrec 89.062% (89.759%)\n",
      "Epoch: [39][200/391]\tTime 0.048 (0.048)\tData 0.002 (0.003)\tLoss 0.2778 (0.2934)\tPrec 92.969% (89.910%)\n",
      "Epoch: [39][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.2978 (0.2964)\tPrec 88.281% (89.844%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.243 (0.243)\tLoss 0.3754 (0.3754)\tPrec 89.062% (89.062%)\n",
      " * Prec 85.860% \n",
      "best acc: 85.860000\n",
      "Epoch: [40][0/391]\tTime 0.522 (0.522)\tData 0.480 (0.480)\tLoss 0.2845 (0.2845)\tPrec 89.844% (89.844%)\n",
      "Epoch: [40][100/391]\tTime 0.040 (0.051)\tData 0.002 (0.007)\tLoss 0.3216 (0.2802)\tPrec 86.719% (90.223%)\n",
      "Epoch: [40][200/391]\tTime 0.040 (0.048)\tData 0.003 (0.004)\tLoss 0.2529 (0.2773)\tPrec 89.844% (90.563%)\n",
      "Epoch: [40][300/391]\tTime 0.045 (0.048)\tData 0.002 (0.004)\tLoss 0.2921 (0.2736)\tPrec 88.281% (90.661%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.228 (0.228)\tLoss 0.3427 (0.3427)\tPrec 88.281% (88.281%)\n",
      " * Prec 86.320% \n",
      "best acc: 86.320000\n",
      "Epoch: [41][0/391]\tTime 0.285 (0.285)\tData 0.246 (0.246)\tLoss 0.2504 (0.2504)\tPrec 90.625% (90.625%)\n",
      "Epoch: [41][100/391]\tTime 0.049 (0.049)\tData 0.002 (0.004)\tLoss 0.2299 (0.2657)\tPrec 91.406% (90.834%)\n",
      "Epoch: [41][200/391]\tTime 0.048 (0.047)\tData 0.002 (0.003)\tLoss 0.3253 (0.2671)\tPrec 90.625% (90.951%)\n",
      "Epoch: [41][300/391]\tTime 0.051 (0.047)\tData 0.002 (0.003)\tLoss 0.2550 (0.2684)\tPrec 90.625% (90.817%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.257 (0.257)\tLoss 0.3158 (0.3158)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.160% \n",
      "best acc: 86.320000\n",
      "Epoch: [42][0/391]\tTime 0.272 (0.272)\tData 0.230 (0.230)\tLoss 0.3014 (0.3014)\tPrec 91.406% (91.406%)\n",
      "Epoch: [42][100/391]\tTime 0.048 (0.049)\tData 0.002 (0.005)\tLoss 0.2721 (0.2604)\tPrec 89.844% (90.857%)\n",
      "Epoch: [42][200/391]\tTime 0.052 (0.047)\tData 0.001 (0.003)\tLoss 0.4105 (0.2558)\tPrec 87.500% (91.134%)\n",
      "Epoch: [42][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 0.2572 (0.2579)\tPrec 92.188% (91.191%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.213 (0.213)\tLoss 0.3756 (0.3756)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.250% \n",
      "best acc: 86.320000\n",
      "Epoch: [43][0/391]\tTime 0.278 (0.278)\tData 0.236 (0.236)\tLoss 0.3098 (0.3098)\tPrec 89.062% (89.062%)\n",
      "Epoch: [43][100/391]\tTime 0.044 (0.049)\tData 0.002 (0.004)\tLoss 0.2395 (0.2589)\tPrec 89.062% (91.228%)\n",
      "Epoch: [43][200/391]\tTime 0.047 (0.047)\tData 0.002 (0.003)\tLoss 0.3505 (0.2583)\tPrec 88.281% (91.231%)\n",
      "Epoch: [43][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.3273 (0.2607)\tPrec 88.281% (91.053%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.240 (0.240)\tLoss 0.3357 (0.3357)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.500% \n",
      "best acc: 86.500000\n",
      "Epoch: [44][0/391]\tTime 0.317 (0.317)\tData 0.269 (0.269)\tLoss 0.2236 (0.2236)\tPrec 91.406% (91.406%)\n",
      "Epoch: [44][100/391]\tTime 0.046 (0.049)\tData 0.002 (0.005)\tLoss 0.1824 (0.2656)\tPrec 92.188% (90.919%)\n",
      "Epoch: [44][200/391]\tTime 0.050 (0.047)\tData 0.001 (0.003)\tLoss 0.2841 (0.2592)\tPrec 89.844% (91.134%)\n",
      "Epoch: [44][300/391]\tTime 0.054 (0.047)\tData 0.002 (0.003)\tLoss 0.2631 (0.2643)\tPrec 91.406% (90.848%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.241 (0.241)\tLoss 0.3325 (0.3325)\tPrec 90.625% (90.625%)\n",
      " * Prec 86.480% \n",
      "best acc: 86.500000\n",
      "Epoch: [45][0/391]\tTime 0.256 (0.256)\tData 0.216 (0.216)\tLoss 0.3043 (0.3043)\tPrec 89.844% (89.844%)\n",
      "Epoch: [45][100/391]\tTime 0.041 (0.049)\tData 0.002 (0.004)\tLoss 0.2157 (0.2570)\tPrec 92.969% (91.228%)\n",
      "Epoch: [45][200/391]\tTime 0.048 (0.048)\tData 0.001 (0.003)\tLoss 0.4319 (0.2585)\tPrec 86.719% (91.157%)\n",
      "Epoch: [45][300/391]\tTime 0.043 (0.047)\tData 0.002 (0.003)\tLoss 0.3734 (0.2603)\tPrec 88.281% (91.095%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.219 (0.219)\tLoss 0.3410 (0.3410)\tPrec 89.062% (89.062%)\n",
      " * Prec 85.880% \n",
      "best acc: 86.500000\n",
      "Epoch: [46][0/391]\tTime 0.317 (0.317)\tData 0.278 (0.278)\tLoss 0.2688 (0.2688)\tPrec 87.500% (87.500%)\n",
      "Epoch: [46][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.005)\tLoss 0.3471 (0.2611)\tPrec 89.062% (91.043%)\n",
      "Epoch: [46][200/391]\tTime 0.043 (0.048)\tData 0.002 (0.004)\tLoss 0.1637 (0.2605)\tPrec 95.312% (91.053%)\n",
      "Epoch: [46][300/391]\tTime 0.044 (0.047)\tData 0.002 (0.003)\tLoss 0.3116 (0.2595)\tPrec 90.625% (91.051%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.213 (0.213)\tLoss 0.3478 (0.3478)\tPrec 90.625% (90.625%)\n",
      " * Prec 86.410% \n",
      "best acc: 86.500000\n",
      "Epoch: [47][0/391]\tTime 0.321 (0.321)\tData 0.268 (0.268)\tLoss 0.2884 (0.2884)\tPrec 89.062% (89.062%)\n",
      "Epoch: [47][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.005)\tLoss 0.3105 (0.2555)\tPrec 90.625% (91.282%)\n",
      "Epoch: [47][200/391]\tTime 0.044 (0.047)\tData 0.003 (0.003)\tLoss 0.2158 (0.2555)\tPrec 91.406% (91.305%)\n",
      "Epoch: [47][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 0.1785 (0.2576)\tPrec 94.531% (91.204%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.219 (0.219)\tLoss 0.4087 (0.4087)\tPrec 88.281% (88.281%)\n",
      " * Prec 86.320% \n",
      "best acc: 86.500000\n",
      "Epoch: [48][0/391]\tTime 0.339 (0.339)\tData 0.287 (0.287)\tLoss 0.3954 (0.3954)\tPrec 86.719% (86.719%)\n",
      "Epoch: [48][100/391]\tTime 0.044 (0.050)\tData 0.002 (0.005)\tLoss 0.2603 (0.2608)\tPrec 90.625% (90.865%)\n",
      "Epoch: [48][200/391]\tTime 0.054 (0.048)\tData 0.001 (0.003)\tLoss 0.2223 (0.2568)\tPrec 91.406% (91.150%)\n",
      "Epoch: [48][300/391]\tTime 0.047 (0.047)\tData 0.002 (0.003)\tLoss 0.1897 (0.2566)\tPrec 94.531% (91.183%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.233 (0.233)\tLoss 0.3512 (0.3512)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.280% \n",
      "best acc: 86.500000\n",
      "Epoch: [49][0/391]\tTime 0.312 (0.312)\tData 0.257 (0.257)\tLoss 0.2792 (0.2792)\tPrec 92.969% (92.969%)\n",
      "Epoch: [49][100/391]\tTime 0.038 (0.049)\tData 0.002 (0.005)\tLoss 0.2607 (0.2463)\tPrec 92.969% (91.375%)\n",
      "Epoch: [49][200/391]\tTime 0.056 (0.048)\tData 0.002 (0.003)\tLoss 0.2382 (0.2479)\tPrec 93.750% (91.426%)\n",
      "Epoch: [49][300/391]\tTime 0.045 (0.048)\tData 0.002 (0.003)\tLoss 0.2625 (0.2564)\tPrec 89.062% (91.266%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.221 (0.221)\tLoss 0.3089 (0.3089)\tPrec 90.625% (90.625%)\n",
      " * Prec 86.410% \n",
      "best acc: 86.500000\n",
      "Epoch: [50][0/391]\tTime 0.325 (0.325)\tData 0.279 (0.279)\tLoss 0.3706 (0.3706)\tPrec 85.156% (85.156%)\n",
      "Epoch: [50][100/391]\tTime 0.039 (0.049)\tData 0.003 (0.005)\tLoss 0.2002 (0.2484)\tPrec 92.969% (91.391%)\n",
      "Epoch: [50][200/391]\tTime 0.045 (0.048)\tData 0.002 (0.003)\tLoss 0.1882 (0.2576)\tPrec 92.969% (91.072%)\n",
      "Epoch: [50][300/391]\tTime 0.044 (0.047)\tData 0.003 (0.003)\tLoss 0.2742 (0.2553)\tPrec 93.750% (91.222%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.209 (0.209)\tLoss 0.3480 (0.3480)\tPrec 92.188% (92.188%)\n",
      " * Prec 86.270% \n",
      "best acc: 86.500000\n",
      "Epoch: [51][0/391]\tTime 0.288 (0.288)\tData 0.242 (0.242)\tLoss 0.1947 (0.1947)\tPrec 92.188% (92.188%)\n",
      "Epoch: [51][100/391]\tTime 0.046 (0.049)\tData 0.001 (0.005)\tLoss 0.2247 (0.2545)\tPrec 92.188% (91.159%)\n",
      "Epoch: [51][200/391]\tTime 0.045 (0.048)\tData 0.002 (0.003)\tLoss 0.1968 (0.2522)\tPrec 93.750% (91.348%)\n",
      "Epoch: [51][300/391]\tTime 0.044 (0.047)\tData 0.002 (0.003)\tLoss 0.2275 (0.2582)\tPrec 92.969% (91.162%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.268 (0.268)\tLoss 0.3475 (0.3475)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.150% \n",
      "best acc: 86.500000\n",
      "Epoch: [52][0/391]\tTime 0.331 (0.331)\tData 0.272 (0.272)\tLoss 0.2450 (0.2450)\tPrec 89.062% (89.062%)\n",
      "Epoch: [52][100/391]\tTime 0.048 (0.050)\tData 0.003 (0.005)\tLoss 0.1737 (0.2600)\tPrec 94.531% (90.849%)\n",
      "Epoch: [52][200/391]\tTime 0.051 (0.048)\tData 0.002 (0.004)\tLoss 0.2339 (0.2551)\tPrec 90.625% (91.169%)\n",
      "Epoch: [52][300/391]\tTime 0.047 (0.048)\tData 0.002 (0.003)\tLoss 0.2030 (0.2518)\tPrec 92.969% (91.225%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.224 (0.224)\tLoss 0.3305 (0.3305)\tPrec 88.281% (88.281%)\n",
      " * Prec 86.100% \n",
      "best acc: 86.500000\n",
      "Epoch: [53][0/391]\tTime 0.314 (0.314)\tData 0.261 (0.261)\tLoss 0.1998 (0.1998)\tPrec 95.312% (95.312%)\n",
      "Epoch: [53][100/391]\tTime 0.044 (0.049)\tData 0.002 (0.005)\tLoss 0.2586 (0.2521)\tPrec 90.625% (91.066%)\n",
      "Epoch: [53][200/391]\tTime 0.055 (0.048)\tData 0.002 (0.003)\tLoss 0.2688 (0.2596)\tPrec 92.188% (90.990%)\n",
      "Epoch: [53][300/391]\tTime 0.042 (0.048)\tData 0.003 (0.003)\tLoss 0.1645 (0.2557)\tPrec 92.969% (91.113%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.215 (0.215)\tLoss 0.3497 (0.3497)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.430% \n",
      "best acc: 86.500000\n",
      "Epoch: [54][0/391]\tTime 0.300 (0.300)\tData 0.254 (0.254)\tLoss 0.1826 (0.1826)\tPrec 91.406% (91.406%)\n",
      "Epoch: [54][100/391]\tTime 0.055 (0.049)\tData 0.002 (0.005)\tLoss 0.2395 (0.2445)\tPrec 90.625% (91.499%)\n",
      "Epoch: [54][200/391]\tTime 0.045 (0.048)\tData 0.003 (0.003)\tLoss 0.3830 (0.2490)\tPrec 87.500% (91.437%)\n",
      "Epoch: [54][300/391]\tTime 0.050 (0.048)\tData 0.001 (0.003)\tLoss 0.3174 (0.2528)\tPrec 89.844% (91.318%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.253 (0.253)\tLoss 0.3540 (0.3540)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.120% \n",
      "best acc: 86.500000\n",
      "Epoch: [55][0/391]\tTime 0.290 (0.290)\tData 0.248 (0.248)\tLoss 0.3030 (0.3030)\tPrec 88.281% (88.281%)\n",
      "Epoch: [55][100/391]\tTime 0.042 (0.049)\tData 0.001 (0.005)\tLoss 0.2614 (0.2626)\tPrec 89.844% (90.965%)\n",
      "Epoch: [55][200/391]\tTime 0.049 (0.048)\tData 0.002 (0.003)\tLoss 0.2633 (0.2545)\tPrec 90.625% (91.224%)\n",
      "Epoch: [55][300/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.2395 (0.2528)\tPrec 92.969% (91.318%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.242 (0.242)\tLoss 0.3677 (0.3677)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.070% \n",
      "best acc: 86.500000\n",
      "Epoch: [56][0/391]\tTime 0.257 (0.257)\tData 0.216 (0.216)\tLoss 0.1937 (0.1937)\tPrec 95.312% (95.312%)\n",
      "Epoch: [56][100/391]\tTime 0.042 (0.048)\tData 0.003 (0.004)\tLoss 0.2391 (0.2582)\tPrec 93.750% (91.321%)\n",
      "Epoch: [56][200/391]\tTime 0.040 (0.047)\tData 0.002 (0.003)\tLoss 0.2201 (0.2504)\tPrec 93.750% (91.360%)\n",
      "Epoch: [56][300/391]\tTime 0.050 (0.047)\tData 0.002 (0.003)\tLoss 0.1782 (0.2516)\tPrec 94.531% (91.287%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.271 (0.271)\tLoss 0.3871 (0.3871)\tPrec 87.500% (87.500%)\n",
      " * Prec 86.350% \n",
      "best acc: 86.500000\n",
      "Epoch: [57][0/391]\tTime 0.313 (0.313)\tData 0.268 (0.268)\tLoss 0.2355 (0.2355)\tPrec 91.406% (91.406%)\n",
      "Epoch: [57][100/391]\tTime 0.045 (0.050)\tData 0.005 (0.005)\tLoss 0.1531 (0.2435)\tPrec 96.094% (91.545%)\n",
      "Epoch: [57][200/391]\tTime 0.055 (0.048)\tData 0.002 (0.004)\tLoss 0.2067 (0.2484)\tPrec 90.625% (91.352%)\n",
      "Epoch: [57][300/391]\tTime 0.051 (0.047)\tData 0.002 (0.003)\tLoss 0.3624 (0.2493)\tPrec 88.281% (91.318%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.272 (0.272)\tLoss 0.3565 (0.3565)\tPrec 88.281% (88.281%)\n",
      " * Prec 86.190% \n",
      "best acc: 86.500000\n",
      "Epoch: [58][0/391]\tTime 0.319 (0.319)\tData 0.278 (0.278)\tLoss 0.3452 (0.3452)\tPrec 89.844% (89.844%)\n",
      "Epoch: [58][100/391]\tTime 0.038 (0.050)\tData 0.003 (0.005)\tLoss 0.2308 (0.2519)\tPrec 89.062% (91.182%)\n",
      "Epoch: [58][200/391]\tTime 0.041 (0.048)\tData 0.002 (0.004)\tLoss 0.2873 (0.2541)\tPrec 89.844% (91.154%)\n",
      "Epoch: [58][300/391]\tTime 0.046 (0.048)\tData 0.003 (0.003)\tLoss 0.4168 (0.2513)\tPrec 85.938% (91.315%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.249 (0.249)\tLoss 0.3006 (0.3006)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.370% \n",
      "best acc: 86.500000\n",
      "Epoch: [59][0/391]\tTime 0.328 (0.328)\tData 0.274 (0.274)\tLoss 0.2711 (0.2711)\tPrec 88.281% (88.281%)\n",
      "Epoch: [59][100/391]\tTime 0.046 (0.049)\tData 0.001 (0.005)\tLoss 0.2065 (0.2449)\tPrec 92.188% (91.368%)\n",
      "Epoch: [59][200/391]\tTime 0.040 (0.048)\tData 0.002 (0.003)\tLoss 0.3141 (0.2503)\tPrec 88.281% (91.278%)\n",
      "Epoch: [59][300/391]\tTime 0.059 (0.047)\tData 0.002 (0.003)\tLoss 0.2804 (0.2456)\tPrec 91.406% (91.510%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.261 (0.261)\tLoss 0.3821 (0.3821)\tPrec 88.281% (88.281%)\n",
      " * Prec 86.190% \n",
      "best acc: 86.500000\n",
      "Epoch: [60][0/391]\tTime 0.272 (0.272)\tData 0.224 (0.224)\tLoss 0.2662 (0.2662)\tPrec 92.188% (92.188%)\n",
      "Epoch: [60][100/391]\tTime 0.049 (0.048)\tData 0.002 (0.004)\tLoss 0.2102 (0.2398)\tPrec 92.969% (91.778%)\n",
      "Epoch: [60][200/391]\tTime 0.046 (0.047)\tData 0.003 (0.003)\tLoss 0.2728 (0.2421)\tPrec 90.625% (91.741%)\n",
      "Epoch: [60][300/391]\tTime 0.051 (0.047)\tData 0.002 (0.003)\tLoss 0.2391 (0.2440)\tPrec 91.406% (91.674%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.222 (0.222)\tLoss 0.3610 (0.3610)\tPrec 88.281% (88.281%)\n",
      " * Prec 86.610% \n",
      "best acc: 86.610000\n",
      "Epoch: [61][0/391]\tTime 0.325 (0.325)\tData 0.280 (0.280)\tLoss 0.1420 (0.1420)\tPrec 96.875% (96.875%)\n",
      "Epoch: [61][100/391]\tTime 0.043 (0.050)\tData 0.002 (0.005)\tLoss 0.2114 (0.2347)\tPrec 93.750% (91.870%)\n",
      "Epoch: [61][200/391]\tTime 0.056 (0.048)\tData 0.002 (0.003)\tLoss 0.2493 (0.2354)\tPrec 89.844% (91.741%)\n",
      "Epoch: [61][300/391]\tTime 0.043 (0.047)\tData 0.002 (0.003)\tLoss 0.1869 (0.2427)\tPrec 93.750% (91.661%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.222 (0.222)\tLoss 0.3881 (0.3881)\tPrec 88.281% (88.281%)\n",
      " * Prec 85.930% \n",
      "best acc: 86.610000\n",
      "Epoch: [62][0/391]\tTime 0.286 (0.286)\tData 0.251 (0.251)\tLoss 0.2345 (0.2345)\tPrec 92.969% (92.969%)\n",
      "Epoch: [62][100/391]\tTime 0.047 (0.049)\tData 0.002 (0.005)\tLoss 0.3096 (0.2519)\tPrec 87.500% (91.190%)\n",
      "Epoch: [62][200/391]\tTime 0.049 (0.048)\tData 0.002 (0.003)\tLoss 0.2100 (0.2450)\tPrec 90.625% (91.601%)\n",
      "Epoch: [62][300/391]\tTime 0.049 (0.048)\tData 0.002 (0.003)\tLoss 0.2067 (0.2441)\tPrec 90.625% (91.604%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.225 (0.225)\tLoss 0.3109 (0.3109)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.380% \n",
      "best acc: 86.610000\n",
      "Epoch: [63][0/391]\tTime 0.274 (0.274)\tData 0.239 (0.239)\tLoss 0.2053 (0.2053)\tPrec 92.969% (92.969%)\n",
      "Epoch: [63][100/391]\tTime 0.044 (0.049)\tData 0.002 (0.004)\tLoss 0.2357 (0.2410)\tPrec 92.969% (91.662%)\n",
      "Epoch: [63][200/391]\tTime 0.047 (0.048)\tData 0.002 (0.003)\tLoss 0.2376 (0.2415)\tPrec 90.625% (91.659%)\n",
      "Epoch: [63][300/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.2548 (0.2423)\tPrec 90.625% (91.635%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.232 (0.232)\tLoss 0.3898 (0.3898)\tPrec 87.500% (87.500%)\n",
      " * Prec 86.400% \n",
      "best acc: 86.610000\n",
      "Epoch: [64][0/391]\tTime 0.278 (0.278)\tData 0.231 (0.231)\tLoss 0.2534 (0.2534)\tPrec 91.406% (91.406%)\n",
      "Epoch: [64][100/391]\tTime 0.051 (0.049)\tData 0.002 (0.005)\tLoss 0.1451 (0.2319)\tPrec 96.094% (92.033%)\n",
      "Epoch: [64][200/391]\tTime 0.051 (0.047)\tData 0.002 (0.003)\tLoss 0.2030 (0.2393)\tPrec 95.312% (91.873%)\n",
      "Epoch: [64][300/391]\tTime 0.049 (0.047)\tData 0.002 (0.003)\tLoss 0.2327 (0.2438)\tPrec 92.969% (91.557%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.257 (0.257)\tLoss 0.3532 (0.3532)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.500% \n",
      "best acc: 86.610000\n",
      "Epoch: [65][0/391]\tTime 0.311 (0.311)\tData 0.262 (0.262)\tLoss 0.2560 (0.2560)\tPrec 89.062% (89.062%)\n",
      "Epoch: [65][100/391]\tTime 0.051 (0.049)\tData 0.002 (0.005)\tLoss 0.2081 (0.2344)\tPrec 92.969% (91.832%)\n",
      "Epoch: [65][200/391]\tTime 0.053 (0.048)\tData 0.003 (0.003)\tLoss 0.2142 (0.2416)\tPrec 92.188% (91.573%)\n",
      "Epoch: [65][300/391]\tTime 0.043 (0.047)\tData 0.002 (0.003)\tLoss 0.2006 (0.2412)\tPrec 93.750% (91.655%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.215 (0.215)\tLoss 0.3702 (0.3702)\tPrec 87.500% (87.500%)\n",
      " * Prec 86.210% \n",
      "best acc: 86.610000\n",
      "Epoch: [66][0/391]\tTime 0.266 (0.266)\tData 0.226 (0.226)\tLoss 0.2235 (0.2235)\tPrec 92.188% (92.188%)\n",
      "Epoch: [66][100/391]\tTime 0.047 (0.048)\tData 0.002 (0.004)\tLoss 0.2197 (0.2357)\tPrec 89.844% (91.901%)\n",
      "Epoch: [66][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.003)\tLoss 0.1195 (0.2393)\tPrec 96.875% (91.741%)\n",
      "Epoch: [66][300/391]\tTime 0.046 (0.047)\tData 0.002 (0.003)\tLoss 0.1395 (0.2409)\tPrec 96.094% (91.661%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.234 (0.234)\tLoss 0.3552 (0.3552)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.340% \n",
      "best acc: 86.610000\n",
      "Epoch: [67][0/391]\tTime 0.306 (0.306)\tData 0.258 (0.258)\tLoss 0.3173 (0.3173)\tPrec 87.500% (87.500%)\n",
      "Epoch: [67][100/391]\tTime 0.043 (0.049)\tData 0.002 (0.005)\tLoss 0.2799 (0.2420)\tPrec 92.188% (91.723%)\n",
      "Epoch: [67][200/391]\tTime 0.040 (0.047)\tData 0.001 (0.003)\tLoss 0.2540 (0.2410)\tPrec 90.625% (91.636%)\n",
      "Epoch: [67][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 0.2408 (0.2435)\tPrec 92.188% (91.554%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.228 (0.228)\tLoss 0.3424 (0.3424)\tPrec 88.281% (88.281%)\n",
      " * Prec 86.410% \n",
      "best acc: 86.610000\n",
      "Epoch: [68][0/391]\tTime 0.280 (0.280)\tData 0.236 (0.236)\tLoss 0.2778 (0.2778)\tPrec 92.188% (92.188%)\n",
      "Epoch: [68][100/391]\tTime 0.054 (0.050)\tData 0.001 (0.004)\tLoss 0.2397 (0.2447)\tPrec 91.406% (91.731%)\n",
      "Epoch: [68][200/391]\tTime 0.039 (0.048)\tData 0.003 (0.003)\tLoss 0.1452 (0.2433)\tPrec 95.312% (91.636%)\n",
      "Epoch: [68][300/391]\tTime 0.048 (0.048)\tData 0.002 (0.003)\tLoss 0.2010 (0.2429)\tPrec 92.188% (91.601%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.243 (0.243)\tLoss 0.3790 (0.3790)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.370% \n",
      "best acc: 86.610000\n",
      "Epoch: [69][0/391]\tTime 0.334 (0.334)\tData 0.276 (0.276)\tLoss 0.2214 (0.2214)\tPrec 92.969% (92.969%)\n",
      "Epoch: [69][100/391]\tTime 0.040 (0.050)\tData 0.002 (0.005)\tLoss 0.2721 (0.2246)\tPrec 89.844% (92.203%)\n",
      "Epoch: [69][200/391]\tTime 0.043 (0.048)\tData 0.002 (0.004)\tLoss 0.2050 (0.2319)\tPrec 92.969% (92.013%)\n",
      "Epoch: [69][300/391]\tTime 0.050 (0.048)\tData 0.002 (0.003)\tLoss 0.2297 (0.2359)\tPrec 92.969% (91.936%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.221 (0.221)\tLoss 0.3498 (0.3498)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.520% \n",
      "best acc: 86.610000\n",
      "Epoch: [70][0/391]\tTime 0.263 (0.263)\tData 0.223 (0.223)\tLoss 0.1846 (0.1846)\tPrec 93.750% (93.750%)\n",
      "Epoch: [70][100/391]\tTime 0.039 (0.048)\tData 0.002 (0.004)\tLoss 0.2132 (0.2300)\tPrec 91.406% (92.195%)\n",
      "Epoch: [70][200/391]\tTime 0.047 (0.048)\tData 0.001 (0.003)\tLoss 0.2804 (0.2355)\tPrec 90.625% (91.857%)\n",
      "Epoch: [70][300/391]\tTime 0.048 (0.047)\tData 0.002 (0.003)\tLoss 0.2156 (0.2359)\tPrec 92.188% (91.912%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.244 (0.244)\tLoss 0.3808 (0.3808)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.490% \n",
      "best acc: 86.610000\n",
      "Epoch: [71][0/391]\tTime 0.303 (0.303)\tData 0.252 (0.252)\tLoss 0.2655 (0.2655)\tPrec 88.281% (88.281%)\n",
      "Epoch: [71][100/391]\tTime 0.046 (0.049)\tData 0.002 (0.005)\tLoss 0.2409 (0.2336)\tPrec 92.969% (91.979%)\n",
      "Epoch: [71][200/391]\tTime 0.041 (0.048)\tData 0.002 (0.003)\tLoss 0.1857 (0.2338)\tPrec 94.531% (91.912%)\n",
      "Epoch: [71][300/391]\tTime 0.049 (0.048)\tData 0.002 (0.003)\tLoss 0.2768 (0.2361)\tPrec 88.281% (91.847%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.211 (0.211)\tLoss 0.3634 (0.3634)\tPrec 88.281% (88.281%)\n",
      " * Prec 86.200% \n",
      "best acc: 86.610000\n",
      "Epoch: [72][0/391]\tTime 0.285 (0.285)\tData 0.238 (0.238)\tLoss 0.1509 (0.1509)\tPrec 95.312% (95.312%)\n",
      "Epoch: [72][100/391]\tTime 0.047 (0.049)\tData 0.002 (0.004)\tLoss 0.2188 (0.2403)\tPrec 95.312% (91.917%)\n",
      "Epoch: [72][200/391]\tTime 0.047 (0.048)\tData 0.001 (0.003)\tLoss 0.2099 (0.2391)\tPrec 96.094% (91.919%)\n",
      "Epoch: [72][300/391]\tTime 0.048 (0.048)\tData 0.001 (0.003)\tLoss 0.2859 (0.2360)\tPrec 92.188% (91.962%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.252 (0.252)\tLoss 0.3365 (0.3365)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.530% \n",
      "best acc: 86.610000\n",
      "Epoch: [73][0/391]\tTime 0.341 (0.341)\tData 0.302 (0.302)\tLoss 0.4623 (0.4623)\tPrec 84.375% (84.375%)\n",
      "Epoch: [73][100/391]\tTime 0.047 (0.049)\tData 0.002 (0.005)\tLoss 0.1904 (0.2255)\tPrec 92.188% (92.157%)\n",
      "Epoch: [73][200/391]\tTime 0.047 (0.048)\tData 0.002 (0.003)\tLoss 0.2314 (0.2329)\tPrec 92.969% (92.009%)\n",
      "Epoch: [73][300/391]\tTime 0.052 (0.048)\tData 0.002 (0.003)\tLoss 0.2737 (0.2337)\tPrec 91.406% (91.969%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.258 (0.258)\tLoss 0.3748 (0.3748)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.310% \n",
      "best acc: 86.610000\n",
      "Epoch: [74][0/391]\tTime 0.321 (0.321)\tData 0.279 (0.279)\tLoss 0.2945 (0.2945)\tPrec 90.625% (90.625%)\n",
      "Epoch: [74][100/391]\tTime 0.049 (0.049)\tData 0.002 (0.005)\tLoss 0.1599 (0.2274)\tPrec 94.531% (92.025%)\n",
      "Epoch: [74][200/391]\tTime 0.047 (0.047)\tData 0.002 (0.003)\tLoss 0.2778 (0.2295)\tPrec 87.500% (91.958%)\n",
      "Epoch: [74][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 0.2075 (0.2293)\tPrec 90.625% (91.980%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.240 (0.240)\tLoss 0.3689 (0.3689)\tPrec 88.281% (88.281%)\n",
      " * Prec 86.240% \n",
      "best acc: 86.610000\n",
      "Epoch: [75][0/391]\tTime 0.281 (0.281)\tData 0.229 (0.229)\tLoss 0.2200 (0.2200)\tPrec 92.969% (92.969%)\n",
      "Epoch: [75][100/391]\tTime 0.051 (0.049)\tData 0.003 (0.004)\tLoss 0.2213 (0.2362)\tPrec 91.406% (91.731%)\n",
      "Epoch: [75][200/391]\tTime 0.049 (0.047)\tData 0.002 (0.003)\tLoss 0.2276 (0.2395)\tPrec 92.188% (91.686%)\n",
      "Epoch: [75][300/391]\tTime 0.044 (0.047)\tData 0.003 (0.003)\tLoss 0.3386 (0.2400)\tPrec 90.625% (91.671%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.242 (0.242)\tLoss 0.3858 (0.3858)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.500% \n",
      "best acc: 86.610000\n",
      "Epoch: [76][0/391]\tTime 0.311 (0.311)\tData 0.270 (0.270)\tLoss 0.2826 (0.2826)\tPrec 89.844% (89.844%)\n",
      "Epoch: [76][100/391]\tTime 0.051 (0.049)\tData 0.001 (0.005)\tLoss 0.2724 (0.2404)\tPrec 86.719% (91.313%)\n",
      "Epoch: [76][200/391]\tTime 0.044 (0.048)\tData 0.008 (0.003)\tLoss 0.2211 (0.2382)\tPrec 92.969% (91.601%)\n",
      "Epoch: [76][300/391]\tTime 0.050 (0.047)\tData 0.002 (0.003)\tLoss 0.2426 (0.2383)\tPrec 92.969% (91.694%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.294 (0.294)\tLoss 0.3887 (0.3887)\tPrec 86.719% (86.719%)\n",
      " * Prec 86.540% \n",
      "best acc: 86.610000\n",
      "Epoch: [77][0/391]\tTime 0.321 (0.321)\tData 0.273 (0.273)\tLoss 0.2263 (0.2263)\tPrec 92.188% (92.188%)\n",
      "Epoch: [77][100/391]\tTime 0.042 (0.049)\tData 0.005 (0.005)\tLoss 0.2560 (0.2285)\tPrec 90.625% (92.257%)\n",
      "Epoch: [77][200/391]\tTime 0.049 (0.048)\tData 0.002 (0.003)\tLoss 0.1646 (0.2331)\tPrec 93.750% (91.787%)\n",
      "Epoch: [77][300/391]\tTime 0.045 (0.048)\tData 0.002 (0.003)\tLoss 0.2657 (0.2339)\tPrec 89.844% (91.842%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.252 (0.252)\tLoss 0.3674 (0.3674)\tPrec 87.500% (87.500%)\n",
      " * Prec 86.330% \n",
      "best acc: 86.610000\n",
      "Epoch: [78][0/391]\tTime 0.512 (0.512)\tData 0.466 (0.466)\tLoss 0.2308 (0.2308)\tPrec 89.844% (89.844%)\n",
      "Epoch: [78][100/391]\tTime 0.050 (0.051)\tData 0.001 (0.007)\tLoss 0.2700 (0.2346)\tPrec 91.406% (91.816%)\n",
      "Epoch: [78][200/391]\tTime 0.044 (0.049)\tData 0.002 (0.004)\tLoss 0.3229 (0.2330)\tPrec 90.625% (91.814%)\n",
      "Epoch: [78][300/391]\tTime 0.046 (0.048)\tData 0.002 (0.004)\tLoss 0.1618 (0.2314)\tPrec 93.750% (91.941%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.211 (0.211)\tLoss 0.3523 (0.3523)\tPrec 87.500% (87.500%)\n",
      " * Prec 86.110% \n",
      "best acc: 86.610000\n",
      "Epoch: [79][0/391]\tTime 0.324 (0.324)\tData 0.279 (0.279)\tLoss 0.3169 (0.3169)\tPrec 87.500% (87.500%)\n",
      "Epoch: [79][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.005)\tLoss 0.2949 (0.2391)\tPrec 88.281% (91.545%)\n",
      "Epoch: [79][200/391]\tTime 0.045 (0.048)\tData 0.002 (0.003)\tLoss 0.3433 (0.2395)\tPrec 89.844% (91.667%)\n",
      "Epoch: [79][300/391]\tTime 0.040 (0.047)\tData 0.003 (0.003)\tLoss 0.2307 (0.2367)\tPrec 90.625% (91.879%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.226 (0.226)\tLoss 0.3451 (0.3451)\tPrec 90.625% (90.625%)\n",
      " * Prec 86.610% \n",
      "best acc: 86.610000\n",
      "Epoch: [80][0/391]\tTime 0.245 (0.245)\tData 0.194 (0.194)\tLoss 0.1377 (0.1377)\tPrec 93.750% (93.750%)\n",
      "Epoch: [80][100/391]\tTime 0.050 (0.048)\tData 0.002 (0.004)\tLoss 0.2205 (0.2313)\tPrec 92.969% (92.071%)\n",
      "Epoch: [80][200/391]\tTime 0.048 (0.047)\tData 0.002 (0.003)\tLoss 0.2521 (0.2262)\tPrec 92.969% (92.292%)\n",
      "Epoch: [80][300/391]\tTime 0.044 (0.047)\tData 0.002 (0.003)\tLoss 0.2773 (0.2298)\tPrec 92.188% (92.252%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.268 (0.268)\tLoss 0.3033 (0.3033)\tPrec 91.406% (91.406%)\n",
      " * Prec 86.310% \n",
      "best acc: 86.610000\n",
      "Epoch: [81][0/391]\tTime 0.317 (0.317)\tData 0.271 (0.271)\tLoss 0.1689 (0.1689)\tPrec 92.188% (92.188%)\n",
      "Epoch: [81][100/391]\tTime 0.053 (0.049)\tData 0.002 (0.005)\tLoss 0.2581 (0.2394)\tPrec 89.062% (91.785%)\n",
      "Epoch: [81][200/391]\tTime 0.048 (0.048)\tData 0.002 (0.003)\tLoss 0.1225 (0.2336)\tPrec 95.312% (92.020%)\n",
      "Epoch: [81][300/391]\tTime 0.045 (0.047)\tData 0.001 (0.003)\tLoss 0.2599 (0.2366)\tPrec 88.281% (91.892%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.220 (0.220)\tLoss 0.3243 (0.3243)\tPrec 90.625% (90.625%)\n",
      " * Prec 86.290% \n",
      "best acc: 86.610000\n",
      "Epoch: [82][0/391]\tTime 0.289 (0.289)\tData 0.241 (0.241)\tLoss 0.2215 (0.2215)\tPrec 92.188% (92.188%)\n",
      "Epoch: [82][100/391]\tTime 0.044 (0.049)\tData 0.002 (0.004)\tLoss 0.2224 (0.2222)\tPrec 93.750% (92.296%)\n",
      "Epoch: [82][200/391]\tTime 0.044 (0.048)\tData 0.002 (0.003)\tLoss 0.2838 (0.2319)\tPrec 92.188% (91.966%)\n",
      "Epoch: [82][300/391]\tTime 0.047 (0.047)\tData 0.002 (0.003)\tLoss 0.2411 (0.2337)\tPrec 94.531% (91.962%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.256 (0.256)\tLoss 0.3711 (0.3711)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.570% \n",
      "best acc: 86.610000\n",
      "Epoch: [83][0/391]\tTime 0.286 (0.286)\tData 0.241 (0.241)\tLoss 0.2117 (0.2117)\tPrec 93.750% (93.750%)\n",
      "Epoch: [83][100/391]\tTime 0.046 (0.049)\tData 0.002 (0.005)\tLoss 0.2150 (0.2302)\tPrec 91.406% (92.280%)\n",
      "Epoch: [83][200/391]\tTime 0.040 (0.048)\tData 0.003 (0.003)\tLoss 0.1826 (0.2347)\tPrec 92.188% (92.028%)\n",
      "Epoch: [83][300/391]\tTime 0.049 (0.047)\tData 0.002 (0.003)\tLoss 0.1359 (0.2360)\tPrec 92.969% (91.905%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.240 (0.240)\tLoss 0.3215 (0.3215)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.520% \n",
      "best acc: 86.610000\n",
      "Epoch: [84][0/391]\tTime 0.278 (0.278)\tData 0.233 (0.233)\tLoss 0.1695 (0.1695)\tPrec 94.531% (94.531%)\n",
      "Epoch: [84][100/391]\tTime 0.051 (0.049)\tData 0.002 (0.004)\tLoss 0.2499 (0.2226)\tPrec 92.188% (92.249%)\n",
      "Epoch: [84][200/391]\tTime 0.047 (0.047)\tData 0.002 (0.003)\tLoss 0.1988 (0.2267)\tPrec 93.750% (92.114%)\n",
      "Epoch: [84][300/391]\tTime 0.048 (0.047)\tData 0.002 (0.003)\tLoss 0.2647 (0.2293)\tPrec 91.406% (91.982%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.237 (0.237)\tLoss 0.3374 (0.3374)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.410% \n",
      "best acc: 86.610000\n",
      "Epoch: [85][0/391]\tTime 0.294 (0.294)\tData 0.242 (0.242)\tLoss 0.3021 (0.3021)\tPrec 92.188% (92.188%)\n",
      "Epoch: [85][100/391]\tTime 0.043 (0.049)\tData 0.005 (0.005)\tLoss 0.1626 (0.2318)\tPrec 95.312% (92.102%)\n",
      "Epoch: [85][200/391]\tTime 0.044 (0.048)\tData 0.002 (0.003)\tLoss 0.2129 (0.2361)\tPrec 90.625% (91.923%)\n",
      "Epoch: [85][300/391]\tTime 0.041 (0.048)\tData 0.002 (0.003)\tLoss 0.2301 (0.2347)\tPrec 92.969% (91.936%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.240 (0.240)\tLoss 0.3307 (0.3307)\tPrec 90.625% (90.625%)\n",
      " * Prec 86.590% \n",
      "best acc: 86.610000\n",
      "Epoch: [86][0/391]\tTime 0.293 (0.293)\tData 0.245 (0.245)\tLoss 0.2568 (0.2568)\tPrec 89.844% (89.844%)\n",
      "Epoch: [86][100/391]\tTime 0.049 (0.049)\tData 0.002 (0.005)\tLoss 0.3526 (0.2244)\tPrec 88.281% (92.443%)\n",
      "Epoch: [86][200/391]\tTime 0.042 (0.048)\tData 0.002 (0.003)\tLoss 0.2059 (0.2271)\tPrec 94.531% (92.090%)\n",
      "Epoch: [86][300/391]\tTime 0.047 (0.048)\tData 0.004 (0.003)\tLoss 0.1798 (0.2285)\tPrec 94.531% (92.094%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.226 (0.226)\tLoss 0.3454 (0.3454)\tPrec 91.406% (91.406%)\n",
      " * Prec 86.200% \n",
      "best acc: 86.610000\n",
      "Epoch: [87][0/391]\tTime 0.319 (0.319)\tData 0.272 (0.272)\tLoss 0.2524 (0.2524)\tPrec 90.625% (90.625%)\n",
      "Epoch: [87][100/391]\tTime 0.040 (0.049)\tData 0.003 (0.005)\tLoss 0.2255 (0.2228)\tPrec 90.625% (92.520%)\n",
      "Epoch: [87][200/391]\tTime 0.049 (0.048)\tData 0.002 (0.003)\tLoss 0.4024 (0.2275)\tPrec 88.281% (92.110%)\n",
      "Epoch: [87][300/391]\tTime 0.039 (0.047)\tData 0.002 (0.003)\tLoss 0.2359 (0.2253)\tPrec 92.969% (92.237%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.239 (0.239)\tLoss 0.3264 (0.3264)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.210% \n",
      "best acc: 86.610000\n",
      "Epoch: [88][0/391]\tTime 0.309 (0.309)\tData 0.255 (0.255)\tLoss 0.1895 (0.1895)\tPrec 92.188% (92.188%)\n",
      "Epoch: [88][100/391]\tTime 0.047 (0.049)\tData 0.001 (0.005)\tLoss 0.2837 (0.2203)\tPrec 89.844% (92.064%)\n",
      "Epoch: [88][200/391]\tTime 0.043 (0.048)\tData 0.001 (0.003)\tLoss 0.2442 (0.2241)\tPrec 89.844% (92.086%)\n",
      "Epoch: [88][300/391]\tTime 0.052 (0.047)\tData 0.002 (0.003)\tLoss 0.1653 (0.2251)\tPrec 93.750% (92.180%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.246 (0.246)\tLoss 0.3593 (0.3593)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.460% \n",
      "best acc: 86.610000\n",
      "Epoch: [89][0/391]\tTime 0.293 (0.293)\tData 0.245 (0.245)\tLoss 0.1951 (0.1951)\tPrec 93.750% (93.750%)\n",
      "Epoch: [89][100/391]\tTime 0.048 (0.050)\tData 0.002 (0.005)\tLoss 0.1379 (0.2297)\tPrec 95.312% (91.940%)\n",
      "Epoch: [89][200/391]\tTime 0.045 (0.048)\tData 0.001 (0.003)\tLoss 0.2606 (0.2295)\tPrec 89.844% (91.985%)\n",
      "Epoch: [89][300/391]\tTime 0.047 (0.048)\tData 0.003 (0.003)\tLoss 0.2629 (0.2275)\tPrec 90.625% (92.115%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.263 (0.263)\tLoss 0.3394 (0.3394)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.320% \n",
      "best acc: 86.610000\n",
      "Epoch: [90][0/391]\tTime 0.331 (0.331)\tData 0.291 (0.291)\tLoss 0.1819 (0.1819)\tPrec 94.531% (94.531%)\n",
      "Epoch: [90][100/391]\tTime 0.055 (0.049)\tData 0.003 (0.005)\tLoss 0.2721 (0.2321)\tPrec 92.969% (91.870%)\n",
      "Epoch: [90][200/391]\tTime 0.045 (0.048)\tData 0.001 (0.003)\tLoss 0.2077 (0.2254)\tPrec 93.750% (92.160%)\n",
      "Epoch: [90][300/391]\tTime 0.040 (0.048)\tData 0.002 (0.003)\tLoss 0.2427 (0.2278)\tPrec 91.406% (92.094%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.248 (0.248)\tLoss 0.3624 (0.3624)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.590% \n",
      "best acc: 86.610000\n",
      "Epoch: [91][0/391]\tTime 0.293 (0.293)\tData 0.250 (0.250)\tLoss 0.1458 (0.1458)\tPrec 96.094% (96.094%)\n",
      "Epoch: [91][100/391]\tTime 0.047 (0.049)\tData 0.002 (0.005)\tLoss 0.3102 (0.2248)\tPrec 87.500% (92.141%)\n",
      "Epoch: [91][200/391]\tTime 0.043 (0.047)\tData 0.003 (0.003)\tLoss 0.2879 (0.2290)\tPrec 92.188% (92.083%)\n",
      "Epoch: [91][300/391]\tTime 0.047 (0.047)\tData 0.002 (0.003)\tLoss 0.3344 (0.2314)\tPrec 89.062% (92.016%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.240 (0.240)\tLoss 0.3548 (0.3548)\tPrec 90.625% (90.625%)\n",
      " * Prec 86.330% \n",
      "best acc: 86.610000\n",
      "Epoch: [92][0/391]\tTime 0.288 (0.288)\tData 0.241 (0.241)\tLoss 0.1838 (0.1838)\tPrec 92.188% (92.188%)\n",
      "Epoch: [92][100/391]\tTime 0.044 (0.049)\tData 0.002 (0.004)\tLoss 0.1987 (0.2211)\tPrec 92.969% (92.489%)\n",
      "Epoch: [92][200/391]\tTime 0.043 (0.047)\tData 0.002 (0.003)\tLoss 0.2188 (0.2211)\tPrec 92.969% (92.530%)\n",
      "Epoch: [92][300/391]\tTime 0.046 (0.047)\tData 0.002 (0.003)\tLoss 0.3192 (0.2215)\tPrec 90.625% (92.486%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.219 (0.219)\tLoss 0.3444 (0.3444)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.270% \n",
      "best acc: 86.610000\n",
      "Epoch: [93][0/391]\tTime 0.306 (0.306)\tData 0.262 (0.262)\tLoss 0.1154 (0.1154)\tPrec 95.312% (95.312%)\n",
      "Epoch: [93][100/391]\tTime 0.039 (0.048)\tData 0.002 (0.005)\tLoss 0.3255 (0.2220)\tPrec 88.281% (92.327%)\n",
      "Epoch: [93][200/391]\tTime 0.048 (0.047)\tData 0.002 (0.003)\tLoss 0.2128 (0.2218)\tPrec 92.188% (92.238%)\n",
      "Epoch: [93][300/391]\tTime 0.051 (0.047)\tData 0.002 (0.003)\tLoss 0.2132 (0.2223)\tPrec 92.188% (92.252%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.247 (0.247)\tLoss 0.3432 (0.3432)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.560% \n",
      "best acc: 86.610000\n",
      "Epoch: [94][0/391]\tTime 0.300 (0.300)\tData 0.258 (0.258)\tLoss 0.3406 (0.3406)\tPrec 86.719% (86.719%)\n",
      "Epoch: [94][100/391]\tTime 0.051 (0.049)\tData 0.002 (0.005)\tLoss 0.1833 (0.2185)\tPrec 95.312% (92.536%)\n",
      "Epoch: [94][200/391]\tTime 0.051 (0.048)\tData 0.001 (0.003)\tLoss 0.2731 (0.2176)\tPrec 88.281% (92.483%)\n",
      "Epoch: [94][300/391]\tTime 0.050 (0.047)\tData 0.004 (0.003)\tLoss 0.2563 (0.2192)\tPrec 92.188% (92.483%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.230 (0.230)\tLoss 0.4078 (0.4078)\tPrec 87.500% (87.500%)\n",
      " * Prec 86.540% \n",
      "best acc: 86.610000\n",
      "Epoch: [95][0/391]\tTime 0.320 (0.320)\tData 0.267 (0.267)\tLoss 0.1886 (0.1886)\tPrec 92.969% (92.969%)\n",
      "Epoch: [95][100/391]\tTime 0.048 (0.049)\tData 0.002 (0.005)\tLoss 0.2666 (0.2161)\tPrec 92.188% (92.257%)\n",
      "Epoch: [95][200/391]\tTime 0.049 (0.049)\tData 0.002 (0.003)\tLoss 0.2176 (0.2204)\tPrec 95.312% (92.304%)\n",
      "Epoch: [95][300/391]\tTime 0.046 (0.048)\tData 0.002 (0.003)\tLoss 0.2066 (0.2221)\tPrec 92.188% (92.278%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.217 (0.217)\tLoss 0.3752 (0.3752)\tPrec 91.406% (91.406%)\n",
      " * Prec 86.480% \n",
      "best acc: 86.610000\n",
      "Epoch: [96][0/391]\tTime 0.299 (0.299)\tData 0.252 (0.252)\tLoss 0.2295 (0.2295)\tPrec 91.406% (91.406%)\n",
      "Epoch: [96][100/391]\tTime 0.041 (0.049)\tData 0.002 (0.005)\tLoss 0.1184 (0.2162)\tPrec 97.656% (92.613%)\n",
      "Epoch: [96][200/391]\tTime 0.051 (0.048)\tData 0.002 (0.003)\tLoss 0.2080 (0.2188)\tPrec 92.188% (92.576%)\n",
      "Epoch: [96][300/391]\tTime 0.040 (0.047)\tData 0.002 (0.003)\tLoss 0.1863 (0.2226)\tPrec 92.969% (92.468%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.230 (0.230)\tLoss 0.3481 (0.3481)\tPrec 92.188% (92.188%)\n",
      " * Prec 86.600% \n",
      "best acc: 86.610000\n",
      "Epoch: [97][0/391]\tTime 0.316 (0.316)\tData 0.259 (0.259)\tLoss 0.2535 (0.2535)\tPrec 93.750% (93.750%)\n",
      "Epoch: [97][100/391]\tTime 0.040 (0.049)\tData 0.003 (0.004)\tLoss 0.2368 (0.2248)\tPrec 92.188% (92.327%)\n",
      "Epoch: [97][200/391]\tTime 0.055 (0.048)\tData 0.001 (0.003)\tLoss 0.2266 (0.2251)\tPrec 90.625% (92.316%)\n",
      "Epoch: [97][300/391]\tTime 0.053 (0.047)\tData 0.002 (0.003)\tLoss 0.2684 (0.2216)\tPrec 92.188% (92.390%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.256 (0.256)\tLoss 0.3773 (0.3773)\tPrec 88.281% (88.281%)\n",
      " * Prec 86.370% \n",
      "best acc: 86.610000\n",
      "Epoch: [98][0/391]\tTime 0.307 (0.307)\tData 0.261 (0.261)\tLoss 0.2111 (0.2111)\tPrec 92.188% (92.188%)\n",
      "Epoch: [98][100/391]\tTime 0.046 (0.050)\tData 0.002 (0.005)\tLoss 0.1454 (0.2137)\tPrec 95.312% (92.582%)\n",
      "Epoch: [98][200/391]\tTime 0.042 (0.048)\tData 0.002 (0.003)\tLoss 0.1679 (0.2176)\tPrec 95.312% (92.487%)\n",
      "Epoch: [98][300/391]\tTime 0.043 (0.048)\tData 0.002 (0.003)\tLoss 0.2322 (0.2194)\tPrec 92.188% (92.419%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.257 (0.257)\tLoss 0.3839 (0.3839)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.440% \n",
      "best acc: 86.610000\n",
      "Epoch: [99][0/391]\tTime 0.262 (0.262)\tData 0.219 (0.219)\tLoss 0.2979 (0.2979)\tPrec 89.062% (89.062%)\n",
      "Epoch: [99][100/391]\tTime 0.057 (0.048)\tData 0.002 (0.004)\tLoss 0.1756 (0.2264)\tPrec 92.969% (92.249%)\n",
      "Epoch: [99][200/391]\tTime 0.041 (0.047)\tData 0.002 (0.003)\tLoss 0.2067 (0.2240)\tPrec 93.750% (92.269%)\n",
      "Epoch: [99][300/391]\tTime 0.047 (0.047)\tData 0.002 (0.003)\tLoss 0.1920 (0.2233)\tPrec 92.969% (92.333%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.263 (0.263)\tLoss 0.3452 (0.3452)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.370% \n",
      "best acc: 86.610000\n"
     ]
    }
   ],
   "source": [
    "# This cell won't be given, but students will complete the training\n",
    "\n",
    "lr = 4e-2\n",
    "weight_decay = 1e-4\n",
    "epochs = 100\n",
    "best_prec = 0\n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "#cudnn.benchmark = True\n",
    "\n",
    "if not os.path.exists('result'):\n",
    "    os.makedirs('result')\n",
    "fdir = 'result/'+str(model_name)\n",
    "if not os.path.exists(fdir):\n",
    "    os.makedirs(fdir)\n",
    "        \n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    train(trainloader, model, criterion, optimizer, epoch)\n",
    "    \n",
    "    # evaluate on test set\n",
    "    print(\"Validation starts\")\n",
    "    prec = validate(testloader, model, criterion)\n",
    "\n",
    "    # remember best precision and save checkpoint\n",
    "    is_best = prec > best_prec\n",
    "    best_prec = max(prec,best_prec)\n",
    "    print('best acc: {:1f}'.format(best_prec))\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec': best_prec,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, fdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "decreased-harris",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HW\n",
    "\n",
    "#  1. Train with 4 bits for both weight and activation to achieve >90% accuracy\n",
    "#  2. Find x_int and w_int for the 2nd convolution layer\n",
    "#  3. Check the recovered psum has similar value to the un-quantized original psum\n",
    "#     (such as example 1 in W3S2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "entertaining-queensland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 8661/10000 (87%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = \"result/VGG16_project/model_best.pth.tar\"\n",
    "model_name = \"VGG16_project\"\n",
    "model = VGG16_project()\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\") \n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ceramic-nigeria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 -th layer prehooked\n",
      "7 -th layer prehooked\n",
      "12 -th layer prehooked\n",
      "16 -th layer prehooked\n",
      "21 -th layer prehooked\n",
      "25 -th layer prehooked\n",
      "29 -th layer prehooked\n",
      "34 -th layer prehooked\n",
      "38 -th layer prehooked\n",
      "41 -th layer prehooked\n",
      "46 -th layer prehooked\n",
      "50 -th layer prehooked\n",
      "54 -th layer prehooked\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "    def __call__(self, module, module_in):\n",
    "        self.outputs.append(module_in)\n",
    "    def clear(self):\n",
    "        self.outputs = []  \n",
    "        \n",
    "######### Save inputs from selected layer ##########\n",
    "save_output = SaveOutput()\n",
    "i = 0\n",
    "\n",
    "for layer in model.modules():\n",
    "    i = i+1\n",
    "    if isinstance(layer, QuantConv2d):\n",
    "        print(i,\"-th layer prehooked\")\n",
    "        layer.register_forward_pre_hook(save_output)             \n",
    "####################################################\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "images = images.to(device)\n",
    "out = model(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "spoken-worst",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.0000,  1.0000, -1.0000],\n",
      "          [ 1.0000,  3.0000, -1.0000],\n",
      "          [-1.0000,  1.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000,  0.0000,  2.0000],\n",
      "          [-1.0000, -1.0000,  2.0000],\n",
      "          [ 1.0000,  0.0000,  2.0000]],\n",
      "\n",
      "         [[-1.0000, -0.0000,  0.0000],\n",
      "          [ 2.0000,  1.0000,  1.0000],\n",
      "          [ 2.0000,  3.0000,  3.0000]],\n",
      "\n",
      "         [[-1.0000, -0.0000,  0.0000],\n",
      "          [ 2.0000,  0.0000, -0.0000],\n",
      "          [ 2.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[-2.0000, -1.0000,  0.0000],\n",
      "          [-1.0000,  1.0000,  1.0000],\n",
      "          [-2.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[ 0.0000,  1.0000,  0.0000],\n",
      "          [-0.0000, -1.0000, -2.0000],\n",
      "          [-0.0000, -3.0000, -3.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -0.0000],\n",
      "          [ 0.0000, -1.0000, -1.0000],\n",
      "          [ 1.0000, -1.0000, -0.0000]],\n",
      "\n",
      "         [[-1.0000,  0.0000,  0.0000],\n",
      "          [ 1.0000,  1.0000,  0.0000],\n",
      "          [ 3.0000,  3.0000,  2.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0000,  1.0000, -1.0000],\n",
      "          [ 2.0000,  3.0000, -1.0000],\n",
      "          [ 1.0000, -0.0000, -1.0000]],\n",
      "\n",
      "         [[-0.0000, -1.0000, -1.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000]],\n",
      "\n",
      "         [[ 1.0000,  2.0000,  2.0000],\n",
      "          [ 2.0000,  4.0000,  2.0000],\n",
      "          [ 3.0000,  1.0000, -1.0000]],\n",
      "\n",
      "         [[-0.0000, -1.0000, -2.0000],\n",
      "          [-4.0000, -5.0000, -4.0000],\n",
      "          [-3.0000, -3.0000, -2.0000]],\n",
      "\n",
      "         [[-1.0000, -2.0000, -2.0000],\n",
      "          [-2.0000, -3.0000, -3.0000],\n",
      "          [-3.0000, -5.0000, -4.0000]],\n",
      "\n",
      "         [[ 1.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -2.0000, -1.0000]],\n",
      "\n",
      "         [[-1.0000, -2.0000, -2.0000],\n",
      "          [-4.0000, -5.0000, -3.0000],\n",
      "          [-4.0000, -3.0000, -1.0000]],\n",
      "\n",
      "         [[-0.0000, -2.0000, -2.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -1.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0000,  2.0000, -0.0000],\n",
      "          [ 2.0000,  3.0000,  0.0000],\n",
      "          [ 0.0000,  1.0000, -1.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[-1.0000, -0.0000,  0.0000],\n",
      "          [-1.0000, -0.0000,  1.0000],\n",
      "          [ 0.0000,  1.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000, -1.0000, -0.0000],\n",
      "          [ 1.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[-2.0000, -2.0000, -0.0000],\n",
      "          [-2.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[ 2.0000,  2.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  0.0000],\n",
      "          [-0.0000, -0.0000, -1.0000]],\n",
      "\n",
      "         [[-0.0000, -1.0000,  0.0000],\n",
      "          [-0.0000, -1.0000, -0.0000],\n",
      "          [ 0.0000, -1.0000, -0.0000]],\n",
      "\n",
      "         [[-1.0000, -2.0000, -1.0000],\n",
      "          [-0.0000, -1.0000, -2.0000],\n",
      "          [-1.0000, -1.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-1.0000, -2.0000,  0.0000],\n",
      "          [-1.0000,  0.0000,  1.0000],\n",
      "          [ 5.0000,  4.0000,  6.0000]],\n",
      "\n",
      "         [[ 4.0000,  4.0000,  2.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000],\n",
      "          [-0.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[-1.0000,  1.0000,  2.0000],\n",
      "          [-1.0000,  2.0000,  5.0000],\n",
      "          [ 5.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[-1.0000,  7.0000,  5.0000],\n",
      "          [-1.0000,  6.0000,  3.0000],\n",
      "          [-0.0000,  3.0000,  1.0000]],\n",
      "\n",
      "         [[-1.0000,  1.0000,  2.0000],\n",
      "          [-2.0000, -1.0000,  2.0000],\n",
      "          [-3.0000, -0.0000,  1.0000]],\n",
      "\n",
      "         [[ 2.0000,  0.0000,  2.0000],\n",
      "          [ 1.0000,  0.0000,  2.0000],\n",
      "          [ 1.0000,  1.0000,  0.0000]],\n",
      "\n",
      "         [[-1.0000,  5.0000,  3.0000],\n",
      "          [-2.0000,  5.0000,  2.0000],\n",
      "          [-0.0000,  5.0000,  1.0000]],\n",
      "\n",
      "         [[ 1.0000, -1.0000, -1.0000],\n",
      "          [-2.0000,  1.0000,  4.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0000,  2.0000,  1.0000],\n",
      "          [-0.0000,  1.0000,  2.0000],\n",
      "          [ 1.0000,  0.0000,  1.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -0.0000],\n",
      "          [-1.0000, -1.0000, -1.0000],\n",
      "          [-0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  1.0000,  2.0000],\n",
      "          [-0.0000,  1.0000,  1.0000],\n",
      "          [-2.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[-2.0000, -2.0000,  0.0000],\n",
      "          [-0.0000,  0.0000, -0.0000],\n",
      "          [ 4.0000,  1.0000, -1.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[-0.0000,  0.0000, -0.0000],\n",
      "          [-1.0000, -2.0000, -2.0000],\n",
      "          [-1.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[-1.0000, -0.0000,  0.0000],\n",
      "          [ 1.0000, -1.0000, -1.0000],\n",
      "          [ 2.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[-3.0000, -3.0000, -1.0000],\n",
      "          [-3.0000, -3.0000, -2.0000],\n",
      "          [-3.0000, -4.0000, -3.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0000,  6.0000,  5.0000],\n",
      "          [ 1.0000,  3.0000,  2.0000],\n",
      "          [ 0.0000,  0.0000,  2.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000,  0.0000],\n",
      "          [-1.0000, -1.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[ 2.0000,  7.0000,  7.0000],\n",
      "          [ 2.0000,  6.0000,  7.0000],\n",
      "          [-0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -1.0000],\n",
      "          [-3.0000, -2.0000, -2.0000],\n",
      "          [-2.0000, -2.0000, -1.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000, -0.0000, -1.0000],\n",
      "          [-0.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[-1.0000, -2.0000, -1.0000],\n",
      "          [-2.0000, -2.0000, -2.0000],\n",
      "          [-0.0000, -2.0000, -1.0000]],\n",
      "\n",
      "         [[ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [-1.0000, -1.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0000,  3.0000,  0.0000],\n",
      "          [ 2.0000,  2.0000, -1.0000],\n",
      "          [ 1.0000,  1.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000, -2.0000, -1.0000],\n",
      "          [ 0.0000, -5.0000,  0.0000],\n",
      "          [-0.0000, -1.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000,  1.0000,  2.0000],\n",
      "          [ 0.0000,  2.0000,  2.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000]],\n",
      "\n",
      "         [[-0.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -7.0000, -5.0000],\n",
      "          [-1.0000, -5.0000,  1.0000]],\n",
      "\n",
      "         [[-2.0000, -1.0000, -2.0000],\n",
      "          [-1.0000, -7.0000, -7.0000],\n",
      "          [-1.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         [[ 1.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -2.0000, -1.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -7.0000, -7.0000],\n",
      "          [-1.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         [[-0.0000, -1.0000, -1.0000],\n",
      "          [-0.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -2.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -1.0000,  0.0000],\n",
      "          [-0.0000, -1.0000, -0.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000, -1.0000, -0.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -1.0000],\n",
      "          [-0.0000, -1.0000, -1.0000],\n",
      "          [ 1.0000,  1.0000,  0.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -1.0000],\n",
      "          [ 1.0000, -0.0000, -0.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-1.0000, -2.0000, -2.0000],\n",
      "          [-1.0000, -2.0000, -3.0000],\n",
      "          [ 0.0000, -1.0000, -2.0000]],\n",
      "\n",
      "         [[ 1.0000, -0.0000,  0.0000],\n",
      "          [-0.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -2.0000, -1.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -1.0000],\n",
      "          [-0.0000, -1.0000, -1.0000],\n",
      "          [ 1.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -2.0000, -1.0000],\n",
      "          [-2.0000, -2.0000, -2.0000]]]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "w_bit = 4\n",
    "weight_q = model.features[27].weight_q # quantized value is stored during the training\n",
    "w_alpha = model.features[27].weight_quant.wgt_alpha   # alpha is defined in your model already. bring it out here\n",
    "w_delta = w_alpha/(2**(w_bit-1)-1)   # delta can be calculated by using alpha and w_bit\n",
    "weight_int = weight_q/w_delta # w_int can be calculated by weight_q and w_delta\n",
    "print(weight_int) # you should see clean integer numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "interior-oxygen",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[15.0000,  4.0000,  0.0000,  5.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [15.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [15.0000,  4.0000,  0.0000,  8.0000]],\n",
      "\n",
      "         [[ 3.0000,  0.0000,  2.0000,  4.0000],\n",
      "          [ 4.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 4.0000,  0.0000,  4.0000,  3.0000],\n",
      "          [ 3.0000,  0.0000,  3.0000,  3.0000]],\n",
      "\n",
      "         [[ 2.0000,  1.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 6.0000,  7.0000,  4.0000,  0.0000],\n",
      "          [ 9.0000, 12.0000,  9.0000,  2.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.0000,  6.0000,  8.0000,  7.0000],\n",
      "          [ 9.0000,  9.0000,  9.0000,  5.0000],\n",
      "          [ 5.0000,  3.0000,  3.0000,  2.0000],\n",
      "          [ 3.0000,  1.0000,  1.0000,  1.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 6.0000,  6.0000,  6.0000,  6.0000],\n",
      "          [ 4.0000,  4.0000,  5.0000,  5.0000],\n",
      "          [ 6.0000,  6.0000,  8.0000,  6.0000],\n",
      "          [ 4.0000,  3.0000,  5.0000,  4.0000]]],\n",
      "\n",
      "\n",
      "        [[[15.0000, 15.0000, 15.0000,  0.0000],\n",
      "          [15.0000, 15.0000,  0.0000,  0.0000],\n",
      "          [15.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [15.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 4.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 2.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[14.0000, 15.0000, 15.0000,  8.0000],\n",
      "          [15.0000, 15.0000,  8.0000,  0.0000],\n",
      "          [10.0000,  9.0000,  0.0000,  0.0000],\n",
      "          [ 7.0000,  8.0000,  1.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 3.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 2.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[15.0000, 15.0000, 11.0000,  6.0000],\n",
      "          [15.0000, 13.0000,  6.0000,  3.0000],\n",
      "          [11.0000,  8.0000,  3.0000,  1.0000],\n",
      "          [ 4.0000,  2.0000,  2.0000,  1.0000]]],\n",
      "\n",
      "\n",
      "        [[[15.0000, 15.0000, 15.0000, 12.0000],\n",
      "          [15.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [15.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [13.0000,  0.0000,  0.0000,  6.0000]],\n",
      "\n",
      "         [[ 6.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 2.0000,  0.0000,  1.0000,  2.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[10.0000, 15.0000, 15.0000,  7.0000],\n",
      "          [10.0000, 11.0000,  0.0000,  0.0000],\n",
      "          [ 4.0000,  3.0000,  0.0000,  0.0000],\n",
      "          [ 3.0000,  4.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.0000,  1.0000,  0.0000,  0.0000],\n",
      "          [ 6.0000,  3.0000,  3.0000,  1.0000],\n",
      "          [ 2.0000,  0.0000,  0.0000,  1.0000],\n",
      "          [ 2.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[13.0000,  9.0000,  6.0000,  5.0000],\n",
      "          [ 5.0000,  2.0000,  1.0000,  0.0000],\n",
      "          [ 6.0000,  4.0000,  2.0000,  0.0000],\n",
      "          [ 3.0000,  2.0000,  2.0000,  1.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[15.0000, 14.0000,  5.0000, 12.0000],\n",
      "          [ 7.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [15.0000,  0.0000,  0.0000,  4.0000],\n",
      "          [15.0000,  5.0000,  9.0000, 15.0000]],\n",
      "\n",
      "         [[ 3.0000,  0.0000,  1.0000,  3.0000],\n",
      "          [ 3.0000,  0.0000,  1.0000,  3.0000],\n",
      "          [ 6.0000,  0.0000,  7.0000,  5.0000],\n",
      "          [ 5.0000,  1.0000,  3.0000,  1.0000]],\n",
      "\n",
      "         [[ 5.0000,  6.0000,  6.0000,  1.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 3.0000,  6.0000,  1.0000,  0.0000],\n",
      "          [ 4.0000,  8.0000,  5.0000,  1.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.0000,  1.0000,  2.0000,  2.0000],\n",
      "          [ 5.0000,  0.0000,  0.0000,  2.0000],\n",
      "          [ 5.0000,  0.0000,  0.0000,  2.0000],\n",
      "          [ 5.0000,  2.0000,  2.0000,  2.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 7.0000,  7.0000,  7.0000,  6.0000],\n",
      "          [ 4.0000,  3.0000,  5.0000,  4.0000],\n",
      "          [ 4.0000,  4.0000,  5.0000,  4.0000],\n",
      "          [ 2.0000,  0.0000,  1.0000,  1.0000]]],\n",
      "\n",
      "\n",
      "        [[[15.0000, 15.0000, 15.0000,  0.0000],\n",
      "          [11.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [15.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [15.0000,  7.0000,  0.0000,  7.0000]],\n",
      "\n",
      "         [[ 3.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 4.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 4.0000,  0.0000,  1.0000,  0.0000]],\n",
      "\n",
      "         [[ 4.0000, 15.0000, 14.0000,  4.0000],\n",
      "          [ 5.0000, 11.0000,  1.0000,  0.0000],\n",
      "          [ 5.0000,  7.0000,  0.0000,  0.0000],\n",
      "          [ 5.0000,  7.0000,  4.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.0000,  2.0000,  0.0000,  0.0000],\n",
      "          [ 6.0000,  2.0000,  3.0000,  3.0000],\n",
      "          [ 3.0000,  0.0000,  2.0000,  2.0000],\n",
      "          [ 4.0000,  2.0000,  3.0000,  2.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[11.0000, 12.0000,  8.0000,  4.0000],\n",
      "          [ 3.0000,  1.0000,  0.0000,  0.0000],\n",
      "          [ 4.0000,  2.0000,  1.0000,  1.0000],\n",
      "          [ 3.0000,  0.0000,  0.0000,  1.0000]]],\n",
      "\n",
      "\n",
      "        [[[15.0000,  0.0000,  0.0000, 12.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [13.0000,  0.0000,  0.0000,  8.0000],\n",
      "          [15.0000,  2.0000,  4.0000, 15.0000]],\n",
      "\n",
      "         [[ 2.0000,  0.0000,  4.0000,  3.0000],\n",
      "          [ 1.0000,  0.0000,  4.0000,  2.0000],\n",
      "          [ 4.0000,  2.0000,  6.0000,  4.0000],\n",
      "          [ 3.0000,  2.0000,  3.0000,  1.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 3.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 6.0000,  5.0000,  4.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.0000,  6.0000,  6.0000,  4.0000],\n",
      "          [ 5.0000,  3.0000,  4.0000,  1.0000],\n",
      "          [ 4.0000,  2.0000,  2.0000,  1.0000],\n",
      "          [ 3.0000,  2.0000,  1.0000,  1.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 7.0000,  8.0000,  8.0000,  7.0000],\n",
      "          [ 4.0000,  4.0000,  4.0000,  4.0000],\n",
      "          [ 4.0000,  4.0000,  5.0000,  4.0000],\n",
      "          [ 3.0000,  2.0000,  2.0000,  2.0000]]]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x_bit = 4    \n",
    "x = save_output.outputs[8][0]  # input of the 2nd conv layer\n",
    "x_alpha  = model.features[27].act_alpha\n",
    "x_delta = x_alpha/(2**x_bit-1)\n",
    "\n",
    "act_quant_fn = act_quantization(x_bit) # define the quantization function\n",
    "x_q = act_quant_fn(x, x_alpha)         # create the quantized value for x\n",
    "\n",
    "x_int = x_q/x_delta\n",
    "print(x_int) # you should see clean integer numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "designed-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### input floating number / weight quantized version\n",
    "\n",
    "conv_ref = torch.nn.Conv2d(in_channels = 8, out_channels=8, kernel_size = 3, padding=1, bias = False)\n",
    "conv_ref.weight = torch.nn.parameter.Parameter(weight_int)\n",
    "output_int = conv_ref(x_int)\n",
    "\n",
    "output_recovered = output_int*x_delta*w_delta\n",
    "relu = nn.ReLU(inplace=True)\n",
    "relu_output_recovered = relu(output_recovered)\n",
    "\n",
    "output_ref = save_output.outputs[9][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "157dffd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.5242e-07, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "difference = abs( output_ref - relu_output_recovered )\n",
    "print(difference.mean())  ## It should be small, e.g.,2.3 in my trainned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-whole",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-significance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-witch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-barbados",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-serbia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "907cbb87cc825d52daa26d94a4f3471be4a7efbfbc56d778ed32b1cc3c9bdcfc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
