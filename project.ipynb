{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "radical-fifty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building model...\n",
      "VGG_quant(\n",
      "  (features): Sequential(\n",
      "    (0): QuantConv2d(\n",
      "      3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): QuantConv2d(\n",
      "      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): QuantConv2d(\n",
      "      64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): QuantConv2d(\n",
      "      128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): QuantConv2d(\n",
      "      128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): QuantConv2d(\n",
      "      256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (25): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): QuantConv2d(\n",
      "      8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (28): ReLU(inplace=True)\n",
      "    (29): QuantConv2d(\n",
      "      8, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (30): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (33): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (38): ReLU(inplace=True)\n",
      "    (39): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (40): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (41): ReLU(inplace=True)\n",
      "    (42): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (43): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "     \n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import *\n",
    "\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('=> Building model...')\n",
    "    \n",
    "    \n",
    "    \n",
    "batch_size = 128\n",
    "model_name = \"VGG16_project\"\n",
    "model = VGG16_project()\n",
    "\n",
    "print(model)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "            \n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "         \n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "        \n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))  \n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "#all_params = checkpoint['state_dict']\n",
    "#model.load_state_dict(all_params, strict=False)\n",
    "#criterion = nn.CrossEntropyLoss().cuda()\n",
    "#validate(testloader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2107ceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    adjust_list = [70, 80]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1\n",
    "    if epoch == 80:     \n",
    "        param_group['momentum'] = param_group['momentum']*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "junior-reminder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/391]\tTime 1.781 (1.781)\tData 0.343 (0.343)\tLoss 2.5205 (2.5205)\tPrec 8.594% (8.594%)\n",
      "Epoch: [0][100/391]\tTime 0.044 (0.058)\tData 0.003 (0.006)\tLoss 2.6475 (3.2194)\tPrec 12.500% (10.520%)\n",
      "Epoch: [0][200/391]\tTime 0.043 (0.049)\tData 0.003 (0.004)\tLoss 2.3073 (2.7701)\tPrec 7.031% (10.397%)\n",
      "Epoch: [0][300/391]\tTime 0.040 (0.046)\tData 0.002 (0.004)\tLoss 2.2677 (2.6142)\tPrec 11.719% (10.652%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.218 (0.218)\tLoss 2.2538 (2.2538)\tPrec 14.062% (14.062%)\n",
      " * Prec 10.440% \n",
      "best acc: 10.440000\n",
      "Epoch: [1][0/391]\tTime 0.441 (0.441)\tData 0.406 (0.406)\tLoss 2.2872 (2.2872)\tPrec 8.594% (8.594%)\n",
      "Epoch: [1][100/391]\tTime 0.042 (0.044)\tData 0.002 (0.006)\tLoss 2.1386 (2.2321)\tPrec 17.969% (14.472%)\n",
      "Epoch: [1][200/391]\tTime 0.039 (0.042)\tData 0.002 (0.004)\tLoss 2.0625 (2.1517)\tPrec 23.438% (16.830%)\n",
      "Epoch: [1][300/391]\tTime 0.037 (0.042)\tData 0.003 (0.004)\tLoss 1.9132 (2.0857)\tPrec 22.656% (18.602%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.222 (0.222)\tLoss 1.9264 (1.9264)\tPrec 28.125% (28.125%)\n",
      " * Prec 25.550% \n",
      "best acc: 25.550000\n",
      "Epoch: [2][0/391]\tTime 0.422 (0.422)\tData 0.368 (0.368)\tLoss 1.9370 (1.9370)\tPrec 19.531% (19.531%)\n",
      "Epoch: [2][100/391]\tTime 0.043 (0.044)\tData 0.002 (0.006)\tLoss 1.8946 (1.8836)\tPrec 25.000% (26.168%)\n",
      "Epoch: [2][200/391]\tTime 0.042 (0.042)\tData 0.002 (0.004)\tLoss 1.8055 (1.8555)\tPrec 30.469% (27.138%)\n",
      "Epoch: [2][300/391]\tTime 0.035 (0.040)\tData 0.002 (0.004)\tLoss 1.6823 (1.8320)\tPrec 32.812% (27.972%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.217 (0.217)\tLoss 1.7568 (1.7568)\tPrec 27.344% (27.344%)\n",
      " * Prec 31.770% \n",
      "best acc: 31.770000\n",
      "Epoch: [3][0/391]\tTime 0.500 (0.500)\tData 0.451 (0.451)\tLoss 1.8319 (1.8319)\tPrec 21.094% (21.094%)\n",
      "Epoch: [3][100/391]\tTime 0.040 (0.045)\tData 0.002 (0.007)\tLoss 1.6828 (1.7296)\tPrec 33.594% (32.201%)\n",
      "Epoch: [3][200/391]\tTime 0.040 (0.043)\tData 0.004 (0.005)\tLoss 1.6520 (1.6959)\tPrec 38.281% (33.901%)\n",
      "Epoch: [3][300/391]\tTime 0.038 (0.042)\tData 0.003 (0.004)\tLoss 1.6092 (1.6721)\tPrec 40.625% (34.795%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.226 (0.226)\tLoss 1.5733 (1.5733)\tPrec 37.500% (37.500%)\n",
      " * Prec 39.510% \n",
      "best acc: 39.510000\n",
      "Epoch: [4][0/391]\tTime 0.365 (0.365)\tData 0.315 (0.315)\tLoss 1.5530 (1.5530)\tPrec 36.719% (36.719%)\n",
      "Epoch: [4][100/391]\tTime 0.043 (0.044)\tData 0.003 (0.006)\tLoss 1.5460 (1.5754)\tPrec 41.406% (39.465%)\n",
      "Epoch: [4][200/391]\tTime 0.038 (0.042)\tData 0.003 (0.004)\tLoss 1.5630 (1.5342)\tPrec 38.281% (41.053%)\n",
      "Epoch: [4][300/391]\tTime 0.039 (0.041)\tData 0.002 (0.003)\tLoss 1.5425 (1.5148)\tPrec 44.531% (41.931%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.228 (0.228)\tLoss 1.2555 (1.2555)\tPrec 46.875% (46.875%)\n",
      " * Prec 49.830% \n",
      "best acc: 49.830000\n",
      "Epoch: [5][0/391]\tTime 0.434 (0.434)\tData 0.385 (0.385)\tLoss 1.4878 (1.4878)\tPrec 41.406% (41.406%)\n",
      "Epoch: [5][100/391]\tTime 0.041 (0.044)\tData 0.003 (0.006)\tLoss 1.4692 (1.3737)\tPrec 48.438% (49.049%)\n",
      "Epoch: [5][200/391]\tTime 0.040 (0.042)\tData 0.002 (0.004)\tLoss 1.1213 (1.3434)\tPrec 63.281% (50.124%)\n",
      "Epoch: [5][300/391]\tTime 0.041 (0.042)\tData 0.002 (0.004)\tLoss 1.1983 (1.3213)\tPrec 56.250% (51.241%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.211 (0.211)\tLoss 1.2142 (1.2142)\tPrec 54.688% (54.688%)\n",
      " * Prec 56.330% \n",
      "best acc: 56.330000\n",
      "Epoch: [6][0/391]\tTime 0.373 (0.373)\tData 0.324 (0.324)\tLoss 1.1281 (1.1281)\tPrec 60.938% (60.938%)\n",
      "Epoch: [6][100/391]\tTime 0.042 (0.044)\tData 0.003 (0.006)\tLoss 1.2140 (1.1968)\tPrec 51.562% (56.853%)\n",
      "Epoch: [6][200/391]\tTime 0.040 (0.042)\tData 0.002 (0.004)\tLoss 1.1467 (1.1795)\tPrec 54.688% (57.377%)\n",
      "Epoch: [6][300/391]\tTime 0.040 (0.041)\tData 0.003 (0.004)\tLoss 1.2731 (1.1637)\tPrec 57.812% (58.111%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.231 (0.231)\tLoss 1.0468 (1.0468)\tPrec 61.719% (61.719%)\n",
      " * Prec 61.810% \n",
      "best acc: 61.810000\n",
      "Epoch: [7][0/391]\tTime 0.416 (0.416)\tData 0.367 (0.367)\tLoss 1.3303 (1.3303)\tPrec 53.125% (53.125%)\n",
      "Epoch: [7][100/391]\tTime 0.039 (0.044)\tData 0.003 (0.006)\tLoss 0.9803 (1.0583)\tPrec 61.719% (61.796%)\n",
      "Epoch: [7][200/391]\tTime 0.043 (0.042)\tData 0.003 (0.004)\tLoss 1.1309 (1.0412)\tPrec 57.031% (62.457%)\n",
      "Epoch: [7][300/391]\tTime 0.039 (0.042)\tData 0.003 (0.004)\tLoss 1.1451 (1.0293)\tPrec 61.719% (62.871%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.231 (0.231)\tLoss 1.0499 (1.0499)\tPrec 61.719% (61.719%)\n",
      " * Prec 64.900% \n",
      "best acc: 64.900000\n",
      "Epoch: [8][0/391]\tTime 0.414 (0.414)\tData 0.364 (0.364)\tLoss 0.8609 (0.8609)\tPrec 70.312% (70.312%)\n",
      "Epoch: [8][100/391]\tTime 0.040 (0.044)\tData 0.002 (0.006)\tLoss 0.9394 (0.9402)\tPrec 64.062% (66.685%)\n",
      "Epoch: [8][200/391]\tTime 0.041 (0.042)\tData 0.002 (0.004)\tLoss 0.9299 (0.9327)\tPrec 67.188% (66.849%)\n",
      "Epoch: [8][300/391]\tTime 0.041 (0.042)\tData 0.003 (0.004)\tLoss 0.8829 (0.9244)\tPrec 67.188% (67.091%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.231 (0.231)\tLoss 0.9005 (0.9005)\tPrec 67.188% (67.188%)\n",
      " * Prec 67.330% \n",
      "best acc: 67.330000\n",
      "Epoch: [9][0/391]\tTime 0.428 (0.428)\tData 0.378 (0.378)\tLoss 0.8873 (0.8873)\tPrec 73.438% (73.438%)\n",
      "Epoch: [9][100/391]\tTime 0.040 (0.044)\tData 0.002 (0.006)\tLoss 0.8882 (0.8599)\tPrec 71.875% (69.407%)\n",
      "Epoch: [9][200/391]\tTime 0.040 (0.042)\tData 0.003 (0.004)\tLoss 0.7676 (0.8555)\tPrec 71.875% (69.387%)\n",
      "Epoch: [9][300/391]\tTime 0.041 (0.042)\tData 0.002 (0.004)\tLoss 0.9307 (0.8534)\tPrec 71.875% (69.627%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.244 (0.244)\tLoss 0.9805 (0.9805)\tPrec 64.844% (64.844%)\n",
      " * Prec 66.540% \n",
      "best acc: 67.330000\n",
      "Epoch: [10][0/391]\tTime 0.438 (0.438)\tData 0.388 (0.388)\tLoss 0.9212 (0.9212)\tPrec 66.406% (66.406%)\n",
      "Epoch: [10][100/391]\tTime 0.038 (0.045)\tData 0.002 (0.006)\tLoss 0.7351 (0.7737)\tPrec 75.000% (72.641%)\n",
      "Epoch: [10][200/391]\tTime 0.040 (0.043)\tData 0.002 (0.004)\tLoss 0.7676 (0.7758)\tPrec 74.219% (72.746%)\n",
      "Epoch: [10][300/391]\tTime 0.043 (0.042)\tData 0.003 (0.004)\tLoss 0.7974 (0.7626)\tPrec 71.094% (73.175%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.230 (0.230)\tLoss 0.7740 (0.7740)\tPrec 71.094% (71.094%)\n",
      " * Prec 70.420% \n",
      "best acc: 70.420000\n",
      "Epoch: [11][0/391]\tTime 0.550 (0.550)\tData 0.502 (0.502)\tLoss 0.6915 (0.6915)\tPrec 74.219% (74.219%)\n",
      "Epoch: [11][100/391]\tTime 0.042 (0.046)\tData 0.003 (0.008)\tLoss 0.7740 (0.7350)\tPrec 75.000% (74.118%)\n",
      "Epoch: [11][200/391]\tTime 0.045 (0.044)\tData 0.003 (0.005)\tLoss 0.5876 (0.7279)\tPrec 79.688% (74.456%)\n",
      "Epoch: [11][300/391]\tTime 0.042 (0.043)\tData 0.003 (0.004)\tLoss 0.5819 (0.7201)\tPrec 82.031% (74.712%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.229 (0.229)\tLoss 0.8523 (0.8523)\tPrec 71.094% (71.094%)\n",
      " * Prec 70.490% \n",
      "best acc: 70.490000\n",
      "Epoch: [12][0/391]\tTime 0.400 (0.400)\tData 0.350 (0.350)\tLoss 0.4977 (0.4977)\tPrec 84.375% (84.375%)\n",
      "Epoch: [12][100/391]\tTime 0.039 (0.044)\tData 0.002 (0.006)\tLoss 0.6647 (0.6771)\tPrec 72.656% (76.926%)\n",
      "Epoch: [12][200/391]\tTime 0.040 (0.042)\tData 0.002 (0.004)\tLoss 0.6998 (0.6796)\tPrec 76.562% (76.485%)\n",
      "Epoch: [12][300/391]\tTime 0.041 (0.042)\tData 0.002 (0.004)\tLoss 0.7817 (0.6746)\tPrec 72.656% (76.596%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.250 (0.250)\tLoss 0.6187 (0.6187)\tPrec 80.469% (80.469%)\n",
      " * Prec 77.120% \n",
      "best acc: 77.120000\n",
      "Epoch: [13][0/391]\tTime 0.456 (0.456)\tData 0.407 (0.407)\tLoss 0.6281 (0.6281)\tPrec 78.906% (78.906%)\n",
      "Epoch: [13][100/391]\tTime 0.042 (0.045)\tData 0.002 (0.007)\tLoss 0.7754 (0.6285)\tPrec 73.438% (78.481%)\n",
      "Epoch: [13][200/391]\tTime 0.042 (0.043)\tData 0.002 (0.005)\tLoss 0.6580 (0.6330)\tPrec 78.125% (78.292%)\n",
      "Epoch: [13][300/391]\tTime 0.042 (0.042)\tData 0.002 (0.004)\tLoss 0.7921 (0.6318)\tPrec 74.219% (78.340%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.215 (0.215)\tLoss 0.7008 (0.7008)\tPrec 76.562% (76.562%)\n",
      " * Prec 76.540% \n",
      "best acc: 77.120000\n",
      "Epoch: [14][0/391]\tTime 0.381 (0.381)\tData 0.332 (0.332)\tLoss 0.5468 (0.5468)\tPrec 83.594% (83.594%)\n",
      "Epoch: [14][100/391]\tTime 0.042 (0.044)\tData 0.003 (0.006)\tLoss 0.7016 (0.5943)\tPrec 75.781% (79.757%)\n",
      "Epoch: [14][200/391]\tTime 0.040 (0.043)\tData 0.002 (0.004)\tLoss 0.6374 (0.5886)\tPrec 78.906% (79.979%)\n",
      "Epoch: [14][300/391]\tTime 0.043 (0.042)\tData 0.003 (0.004)\tLoss 0.6817 (0.5877)\tPrec 75.000% (79.999%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.241 (0.241)\tLoss 0.5619 (0.5619)\tPrec 78.125% (78.125%)\n",
      " * Prec 77.960% \n",
      "best acc: 77.960000\n",
      "Epoch: [15][0/391]\tTime 0.482 (0.482)\tData 0.435 (0.435)\tLoss 0.4848 (0.4848)\tPrec 80.469% (80.469%)\n",
      "Epoch: [15][100/391]\tTime 0.042 (0.044)\tData 0.003 (0.007)\tLoss 0.6037 (0.5705)\tPrec 81.250% (80.639%)\n",
      "Epoch: [15][200/391]\tTime 0.037 (0.042)\tData 0.002 (0.005)\tLoss 0.5541 (0.5639)\tPrec 81.250% (80.768%)\n",
      "Epoch: [15][300/391]\tTime 0.039 (0.041)\tData 0.002 (0.004)\tLoss 0.5934 (0.5634)\tPrec 78.125% (80.853%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.214 (0.214)\tLoss 0.5813 (0.5813)\tPrec 78.125% (78.125%)\n",
      " * Prec 79.530% \n",
      "best acc: 79.530000\n",
      "Epoch: [16][0/391]\tTime 0.412 (0.412)\tData 0.363 (0.363)\tLoss 0.5938 (0.5938)\tPrec 77.344% (77.344%)\n",
      "Epoch: [16][100/391]\tTime 0.041 (0.044)\tData 0.003 (0.006)\tLoss 0.5671 (0.5242)\tPrec 79.688% (82.008%)\n",
      "Epoch: [16][200/391]\tTime 0.040 (0.042)\tData 0.002 (0.004)\tLoss 0.3452 (0.5310)\tPrec 88.281% (81.806%)\n",
      "Epoch: [16][300/391]\tTime 0.040 (0.042)\tData 0.002 (0.004)\tLoss 0.6642 (0.5330)\tPrec 79.688% (81.670%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.240 (0.240)\tLoss 0.6544 (0.6544)\tPrec 78.906% (78.906%)\n",
      " * Prec 78.060% \n",
      "best acc: 79.530000\n",
      "Epoch: [17][0/391]\tTime 0.475 (0.475)\tData 0.427 (0.427)\tLoss 0.5059 (0.5059)\tPrec 83.594% (83.594%)\n",
      "Epoch: [17][100/391]\tTime 0.039 (0.045)\tData 0.003 (0.007)\tLoss 0.4902 (0.4971)\tPrec 82.812% (82.650%)\n",
      "Epoch: [17][200/391]\tTime 0.038 (0.043)\tData 0.003 (0.005)\tLoss 0.4521 (0.4958)\tPrec 85.156% (82.890%)\n",
      "Epoch: [17][300/391]\tTime 0.043 (0.042)\tData 0.003 (0.004)\tLoss 0.5364 (0.4997)\tPrec 79.688% (82.797%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.251 (0.251)\tLoss 0.6236 (0.6236)\tPrec 79.688% (79.688%)\n",
      " * Prec 78.670% \n",
      "best acc: 79.530000\n",
      "Epoch: [18][0/391]\tTime 0.464 (0.464)\tData 0.414 (0.414)\tLoss 0.5643 (0.5643)\tPrec 82.812% (82.812%)\n",
      "Epoch: [18][100/391]\tTime 0.039 (0.045)\tData 0.002 (0.007)\tLoss 0.3390 (0.4768)\tPrec 86.719% (83.741%)\n",
      "Epoch: [18][200/391]\tTime 0.040 (0.042)\tData 0.002 (0.005)\tLoss 0.4406 (0.4780)\tPrec 83.594% (83.613%)\n",
      "Epoch: [18][300/391]\tTime 0.036 (0.042)\tData 0.003 (0.004)\tLoss 0.4835 (0.4750)\tPrec 84.375% (83.786%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.232 (0.232)\tLoss 0.4827 (0.4827)\tPrec 85.156% (85.156%)\n",
      " * Prec 80.360% \n",
      "best acc: 80.360000\n",
      "Epoch: [19][0/391]\tTime 0.508 (0.508)\tData 0.453 (0.453)\tLoss 0.3693 (0.3693)\tPrec 87.500% (87.500%)\n",
      "Epoch: [19][100/391]\tTime 0.042 (0.047)\tData 0.002 (0.007)\tLoss 0.4075 (0.4383)\tPrec 85.938% (84.909%)\n",
      "Epoch: [19][200/391]\tTime 0.036 (0.045)\tData 0.002 (0.005)\tLoss 0.4031 (0.4439)\tPrec 87.500% (84.725%)\n",
      "Epoch: [19][300/391]\tTime 0.044 (0.044)\tData 0.003 (0.004)\tLoss 0.4047 (0.4526)\tPrec 88.281% (84.474%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.232 (0.232)\tLoss 0.4699 (0.4699)\tPrec 83.594% (83.594%)\n",
      " * Prec 80.690% \n",
      "best acc: 80.690000\n",
      "Epoch: [20][0/391]\tTime 0.446 (0.446)\tData 0.393 (0.393)\tLoss 0.4085 (0.4085)\tPrec 85.938% (85.938%)\n",
      "Epoch: [20][100/391]\tTime 0.040 (0.047)\tData 0.003 (0.007)\tLoss 0.5214 (0.4424)\tPrec 82.031% (85.218%)\n",
      "Epoch: [20][200/391]\tTime 0.043 (0.045)\tData 0.003 (0.005)\tLoss 0.5499 (0.4388)\tPrec 81.250% (85.133%)\n",
      "Epoch: [20][300/391]\tTime 0.040 (0.045)\tData 0.003 (0.004)\tLoss 0.5596 (0.4380)\tPrec 79.688% (85.089%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.228 (0.228)\tLoss 0.5173 (0.5173)\tPrec 83.594% (83.594%)\n",
      " * Prec 78.700% \n",
      "best acc: 80.690000\n",
      "Epoch: [21][0/391]\tTime 0.391 (0.391)\tData 0.338 (0.338)\tLoss 0.4125 (0.4125)\tPrec 85.938% (85.938%)\n",
      "Epoch: [21][100/391]\tTime 0.046 (0.045)\tData 0.002 (0.006)\tLoss 0.4292 (0.4043)\tPrec 85.938% (86.224%)\n",
      "Epoch: [21][200/391]\tTime 0.039 (0.044)\tData 0.003 (0.004)\tLoss 0.3991 (0.4185)\tPrec 87.500% (85.615%)\n",
      "Epoch: [21][300/391]\tTime 0.040 (0.044)\tData 0.003 (0.004)\tLoss 0.4017 (0.4211)\tPrec 84.375% (85.491%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.234 (0.234)\tLoss 0.4922 (0.4922)\tPrec 85.156% (85.156%)\n",
      " * Prec 81.760% \n",
      "best acc: 81.760000\n",
      "Epoch: [22][0/391]\tTime 0.403 (0.403)\tData 0.349 (0.349)\tLoss 0.3003 (0.3003)\tPrec 89.844% (89.844%)\n",
      "Epoch: [22][100/391]\tTime 0.051 (0.046)\tData 0.003 (0.006)\tLoss 0.3882 (0.4055)\tPrec 85.156% (86.317%)\n",
      "Epoch: [22][200/391]\tTime 0.039 (0.045)\tData 0.003 (0.005)\tLoss 0.4650 (0.4061)\tPrec 84.375% (86.202%)\n",
      "Epoch: [22][300/391]\tTime 0.037 (0.043)\tData 0.002 (0.004)\tLoss 0.5822 (0.4082)\tPrec 81.250% (86.207%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.226 (0.226)\tLoss 0.4531 (0.4531)\tPrec 79.688% (79.688%)\n",
      " * Prec 83.410% \n",
      "best acc: 83.410000\n",
      "Epoch: [23][0/391]\tTime 0.452 (0.452)\tData 0.402 (0.402)\tLoss 0.3289 (0.3289)\tPrec 90.625% (90.625%)\n",
      "Epoch: [23][100/391]\tTime 0.041 (0.044)\tData 0.003 (0.006)\tLoss 0.2937 (0.3736)\tPrec 89.844% (86.935%)\n",
      "Epoch: [23][200/391]\tTime 0.039 (0.042)\tData 0.002 (0.004)\tLoss 0.4328 (0.3843)\tPrec 82.031% (86.653%)\n",
      "Epoch: [23][300/391]\tTime 0.040 (0.042)\tData 0.002 (0.004)\tLoss 0.3150 (0.3860)\tPrec 89.844% (86.719%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.228 (0.228)\tLoss 0.6045 (0.6045)\tPrec 80.469% (80.469%)\n",
      " * Prec 79.970% \n",
      "best acc: 83.410000\n",
      "Epoch: [24][0/391]\tTime 0.470 (0.470)\tData 0.422 (0.422)\tLoss 0.4347 (0.4347)\tPrec 83.594% (83.594%)\n",
      "Epoch: [24][100/391]\tTime 0.041 (0.045)\tData 0.002 (0.007)\tLoss 0.3433 (0.3612)\tPrec 86.719% (87.616%)\n",
      "Epoch: [24][200/391]\tTime 0.038 (0.043)\tData 0.002 (0.005)\tLoss 0.3480 (0.3671)\tPrec 90.625% (87.523%)\n",
      "Epoch: [24][300/391]\tTime 0.042 (0.042)\tData 0.002 (0.004)\tLoss 0.2565 (0.3634)\tPrec 90.625% (87.612%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.210 (0.210)\tLoss 0.4866 (0.4866)\tPrec 85.938% (85.938%)\n",
      " * Prec 82.100% \n",
      "best acc: 83.410000\n",
      "Epoch: [25][0/391]\tTime 0.459 (0.459)\tData 0.409 (0.409)\tLoss 0.3488 (0.3488)\tPrec 85.938% (85.938%)\n",
      "Epoch: [25][100/391]\tTime 0.037 (0.044)\tData 0.002 (0.007)\tLoss 0.3564 (0.3479)\tPrec 89.062% (88.111%)\n",
      "Epoch: [25][200/391]\tTime 0.041 (0.042)\tData 0.002 (0.004)\tLoss 0.3274 (0.3549)\tPrec 90.625% (87.924%)\n",
      "Epoch: [25][300/391]\tTime 0.042 (0.041)\tData 0.003 (0.004)\tLoss 0.4683 (0.3578)\tPrec 85.938% (87.739%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.198 (0.198)\tLoss 0.3559 (0.3559)\tPrec 87.500% (87.500%)\n",
      " * Prec 83.870% \n",
      "best acc: 83.870000\n",
      "Epoch: [26][0/391]\tTime 0.553 (0.553)\tData 0.505 (0.505)\tLoss 0.3162 (0.3162)\tPrec 87.500% (87.500%)\n",
      "Epoch: [26][100/391]\tTime 0.041 (0.046)\tData 0.002 (0.008)\tLoss 0.4124 (0.3387)\tPrec 83.594% (88.467%)\n",
      "Epoch: [26][200/391]\tTime 0.039 (0.043)\tData 0.002 (0.005)\tLoss 0.2973 (0.3467)\tPrec 87.500% (88.258%)\n",
      "Epoch: [26][300/391]\tTime 0.042 (0.042)\tData 0.002 (0.004)\tLoss 0.2478 (0.3428)\tPrec 90.625% (88.393%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.231 (0.231)\tLoss 0.3784 (0.3784)\tPrec 87.500% (87.500%)\n",
      " * Prec 83.540% \n",
      "best acc: 83.870000\n",
      "Epoch: [27][0/391]\tTime 0.485 (0.485)\tData 0.436 (0.436)\tLoss 0.2473 (0.2473)\tPrec 91.406% (91.406%)\n",
      "Epoch: [27][100/391]\tTime 0.039 (0.045)\tData 0.002 (0.007)\tLoss 0.3839 (0.3141)\tPrec 84.375% (89.240%)\n",
      "Epoch: [27][200/391]\tTime 0.041 (0.043)\tData 0.003 (0.005)\tLoss 0.3260 (0.3210)\tPrec 83.594% (89.000%)\n",
      "Epoch: [27][300/391]\tTime 0.041 (0.042)\tData 0.002 (0.004)\tLoss 0.4363 (0.3272)\tPrec 84.375% (88.816%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.232 (0.232)\tLoss 0.4883 (0.4883)\tPrec 85.938% (85.938%)\n",
      " * Prec 83.930% \n",
      "best acc: 83.930000\n",
      "Epoch: [28][0/391]\tTime 0.424 (0.424)\tData 0.377 (0.377)\tLoss 0.2306 (0.2306)\tPrec 91.406% (91.406%)\n",
      "Epoch: [28][100/391]\tTime 0.041 (0.044)\tData 0.002 (0.006)\tLoss 0.4007 (0.3099)\tPrec 86.719% (89.155%)\n",
      "Epoch: [28][200/391]\tTime 0.039 (0.042)\tData 0.002 (0.004)\tLoss 0.4974 (0.3145)\tPrec 86.719% (89.152%)\n",
      "Epoch: [28][300/391]\tTime 0.042 (0.042)\tData 0.002 (0.004)\tLoss 0.3708 (0.3139)\tPrec 85.938% (89.187%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.246 (0.246)\tLoss 0.4008 (0.4008)\tPrec 86.719% (86.719%)\n",
      " * Prec 83.410% \n",
      "best acc: 83.930000\n",
      "Epoch: [29][0/391]\tTime 0.483 (0.483)\tData 0.432 (0.432)\tLoss 0.3404 (0.3404)\tPrec 88.281% (88.281%)\n",
      "Epoch: [29][100/391]\tTime 0.039 (0.044)\tData 0.002 (0.007)\tLoss 0.2493 (0.2785)\tPrec 92.188% (90.161%)\n",
      "Epoch: [29][200/391]\tTime 0.041 (0.042)\tData 0.002 (0.004)\tLoss 0.3424 (0.2917)\tPrec 87.500% (89.902%)\n",
      "Epoch: [29][300/391]\tTime 0.037 (0.042)\tData 0.002 (0.004)\tLoss 0.3252 (0.2981)\tPrec 88.281% (89.634%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.199 (0.199)\tLoss 0.4186 (0.4186)\tPrec 84.375% (84.375%)\n",
      " * Prec 84.680% \n",
      "best acc: 84.680000\n",
      "Epoch: [30][0/391]\tTime 0.450 (0.450)\tData 0.401 (0.401)\tLoss 0.3016 (0.3016)\tPrec 89.844% (89.844%)\n",
      "Epoch: [30][100/391]\tTime 0.042 (0.045)\tData 0.003 (0.006)\tLoss 0.2577 (0.2650)\tPrec 88.281% (90.695%)\n",
      "Epoch: [30][200/391]\tTime 0.039 (0.042)\tData 0.002 (0.004)\tLoss 0.3314 (0.2838)\tPrec 86.719% (90.147%)\n",
      "Epoch: [30][300/391]\tTime 0.041 (0.042)\tData 0.002 (0.004)\tLoss 0.2422 (0.2881)\tPrec 92.969% (90.124%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.226 (0.226)\tLoss 0.3875 (0.3875)\tPrec 85.938% (85.938%)\n",
      " * Prec 85.780% \n",
      "best acc: 85.780000\n",
      "Epoch: [31][0/391]\tTime 0.435 (0.435)\tData 0.387 (0.387)\tLoss 0.3298 (0.3298)\tPrec 87.500% (87.500%)\n",
      "Epoch: [31][100/391]\tTime 0.041 (0.044)\tData 0.003 (0.006)\tLoss 0.3730 (0.2812)\tPrec 89.062% (90.231%)\n",
      "Epoch: [31][200/391]\tTime 0.037 (0.042)\tData 0.002 (0.004)\tLoss 0.3854 (0.2874)\tPrec 85.156% (90.019%)\n",
      "Epoch: [31][300/391]\tTime 0.039 (0.042)\tData 0.002 (0.004)\tLoss 0.3112 (0.2874)\tPrec 89.844% (90.059%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.233 (0.233)\tLoss 0.4307 (0.4307)\tPrec 86.719% (86.719%)\n",
      " * Prec 84.370% \n",
      "best acc: 85.780000\n",
      "Epoch: [32][0/391]\tTime 0.511 (0.511)\tData 0.460 (0.460)\tLoss 0.3511 (0.3511)\tPrec 89.062% (89.062%)\n",
      "Epoch: [32][100/391]\tTime 0.029 (0.041)\tData 0.001 (0.007)\tLoss 0.1983 (0.2742)\tPrec 93.750% (90.656%)\n",
      "Epoch: [32][200/391]\tTime 0.029 (0.036)\tData 0.002 (0.004)\tLoss 0.3256 (0.2786)\tPrec 89.844% (90.555%)\n",
      "Epoch: [32][300/391]\tTime 0.029 (0.034)\tData 0.001 (0.003)\tLoss 0.3405 (0.2784)\tPrec 92.969% (90.485%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.221 (0.221)\tLoss 0.3018 (0.3018)\tPrec 90.625% (90.625%)\n",
      " * Prec 86.040% \n",
      "best acc: 86.040000\n",
      "Epoch: [33][0/391]\tTime 0.532 (0.532)\tData 0.483 (0.483)\tLoss 0.1960 (0.1960)\tPrec 93.750% (93.750%)\n",
      "Epoch: [33][100/391]\tTime 0.039 (0.045)\tData 0.002 (0.007)\tLoss 0.2147 (0.2557)\tPrec 92.969% (91.097%)\n",
      "Epoch: [33][200/391]\tTime 0.042 (0.043)\tData 0.003 (0.005)\tLoss 0.3094 (0.2660)\tPrec 87.500% (90.714%)\n",
      "Epoch: [33][300/391]\tTime 0.040 (0.042)\tData 0.003 (0.004)\tLoss 0.2084 (0.2703)\tPrec 92.969% (90.716%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.231 (0.231)\tLoss 0.3669 (0.3669)\tPrec 88.281% (88.281%)\n",
      " * Prec 86.320% \n",
      "best acc: 86.320000\n",
      "Epoch: [34][0/391]\tTime 0.477 (0.477)\tData 0.429 (0.429)\tLoss 0.2245 (0.2245)\tPrec 94.531% (94.531%)\n",
      "Epoch: [34][100/391]\tTime 0.039 (0.045)\tData 0.003 (0.007)\tLoss 0.2236 (0.2531)\tPrec 93.750% (91.275%)\n",
      "Epoch: [34][200/391]\tTime 0.041 (0.043)\tData 0.002 (0.005)\tLoss 0.2806 (0.2641)\tPrec 89.062% (91.006%)\n",
      "Epoch: [34][300/391]\tTime 0.039 (0.042)\tData 0.002 (0.004)\tLoss 0.3037 (0.2668)\tPrec 91.406% (90.908%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.223 (0.223)\tLoss 0.3284 (0.3284)\tPrec 89.062% (89.062%)\n",
      " * Prec 84.890% \n",
      "best acc: 86.320000\n",
      "Epoch: [35][0/391]\tTime 0.578 (0.578)\tData 0.528 (0.528)\tLoss 0.2369 (0.2369)\tPrec 92.969% (92.969%)\n",
      "Epoch: [35][100/391]\tTime 0.042 (0.046)\tData 0.002 (0.008)\tLoss 0.4053 (0.2337)\tPrec 87.500% (92.102%)\n",
      "Epoch: [35][200/391]\tTime 0.038 (0.043)\tData 0.002 (0.005)\tLoss 0.2357 (0.2436)\tPrec 92.188% (91.787%)\n",
      "Epoch: [35][300/391]\tTime 0.042 (0.042)\tData 0.003 (0.004)\tLoss 0.2759 (0.2515)\tPrec 90.625% (91.489%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.227 (0.227)\tLoss 0.3635 (0.3635)\tPrec 85.938% (85.938%)\n",
      " * Prec 83.950% \n",
      "best acc: 86.320000\n",
      "Epoch: [36][0/391]\tTime 0.458 (0.458)\tData 0.410 (0.410)\tLoss 0.2948 (0.2948)\tPrec 92.188% (92.188%)\n",
      "Epoch: [36][100/391]\tTime 0.041 (0.045)\tData 0.003 (0.006)\tLoss 0.2129 (0.2467)\tPrec 91.406% (91.360%)\n",
      "Epoch: [36][200/391]\tTime 0.043 (0.043)\tData 0.002 (0.004)\tLoss 0.2793 (0.2490)\tPrec 90.625% (91.317%)\n",
      "Epoch: [36][300/391]\tTime 0.039 (0.042)\tData 0.002 (0.004)\tLoss 0.2901 (0.2471)\tPrec 89.844% (91.411%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.228 (0.228)\tLoss 0.4776 (0.4776)\tPrec 85.938% (85.938%)\n",
      " * Prec 83.550% \n",
      "best acc: 86.320000\n",
      "Epoch: [37][0/391]\tTime 0.510 (0.510)\tData 0.461 (0.461)\tLoss 0.1620 (0.1620)\tPrec 94.531% (94.531%)\n",
      "Epoch: [37][100/391]\tTime 0.042 (0.046)\tData 0.002 (0.007)\tLoss 0.2787 (0.2250)\tPrec 92.188% (92.149%)\n",
      "Epoch: [37][200/391]\tTime 0.039 (0.043)\tData 0.002 (0.005)\tLoss 0.2735 (0.2368)\tPrec 89.062% (91.787%)\n",
      "Epoch: [37][300/391]\tTime 0.041 (0.042)\tData 0.003 (0.004)\tLoss 0.2708 (0.2408)\tPrec 89.844% (91.648%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.233 (0.233)\tLoss 0.3806 (0.3806)\tPrec 87.500% (87.500%)\n",
      " * Prec 85.550% \n",
      "best acc: 86.320000\n",
      "Epoch: [38][0/391]\tTime 0.579 (0.579)\tData 0.532 (0.532)\tLoss 0.2770 (0.2770)\tPrec 89.844% (89.844%)\n",
      "Epoch: [38][100/391]\tTime 0.037 (0.047)\tData 0.003 (0.008)\tLoss 0.1785 (0.2229)\tPrec 92.969% (92.157%)\n",
      "Epoch: [38][200/391]\tTime 0.039 (0.044)\tData 0.002 (0.005)\tLoss 0.2251 (0.2267)\tPrec 91.406% (92.188%)\n",
      "Epoch: [38][300/391]\tTime 0.042 (0.042)\tData 0.003 (0.004)\tLoss 0.2022 (0.2338)\tPrec 94.531% (91.975%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.212 (0.212)\tLoss 0.3207 (0.3207)\tPrec 89.844% (89.844%)\n",
      " * Prec 87.460% \n",
      "best acc: 87.460000\n",
      "Epoch: [39][0/391]\tTime 0.531 (0.531)\tData 0.483 (0.483)\tLoss 0.1706 (0.1706)\tPrec 92.188% (92.188%)\n",
      "Epoch: [39][100/391]\tTime 0.041 (0.045)\tData 0.003 (0.007)\tLoss 0.2536 (0.2123)\tPrec 92.188% (92.713%)\n",
      "Epoch: [39][200/391]\tTime 0.039 (0.043)\tData 0.002 (0.005)\tLoss 0.3426 (0.2195)\tPrec 86.719% (92.467%)\n",
      "Epoch: [39][300/391]\tTime 0.043 (0.042)\tData 0.002 (0.004)\tLoss 0.2379 (0.2242)\tPrec 91.406% (92.385%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.212 (0.212)\tLoss 0.3606 (0.3606)\tPrec 86.719% (86.719%)\n",
      " * Prec 86.800% \n",
      "best acc: 87.460000\n",
      "Epoch: [40][0/391]\tTime 0.464 (0.464)\tData 0.415 (0.415)\tLoss 0.2299 (0.2299)\tPrec 95.312% (95.312%)\n",
      "Epoch: [40][100/391]\tTime 0.046 (0.044)\tData 0.004 (0.007)\tLoss 0.3694 (0.2068)\tPrec 86.719% (93.263%)\n",
      "Epoch: [40][200/391]\tTime 0.043 (0.042)\tData 0.003 (0.005)\tLoss 0.3694 (0.2109)\tPrec 86.719% (92.914%)\n",
      "Epoch: [40][300/391]\tTime 0.041 (0.042)\tData 0.003 (0.004)\tLoss 0.1787 (0.2153)\tPrec 91.406% (92.727%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.240 (0.240)\tLoss 0.3262 (0.3262)\tPrec 87.500% (87.500%)\n",
      " * Prec 85.160% \n",
      "best acc: 87.460000\n",
      "Epoch: [41][0/391]\tTime 0.503 (0.503)\tData 0.453 (0.453)\tLoss 0.1931 (0.1931)\tPrec 92.969% (92.969%)\n",
      "Epoch: [41][100/391]\tTime 0.043 (0.045)\tData 0.003 (0.007)\tLoss 0.1573 (0.2010)\tPrec 92.969% (93.162%)\n",
      "Epoch: [41][200/391]\tTime 0.041 (0.043)\tData 0.002 (0.005)\tLoss 0.1768 (0.2058)\tPrec 94.531% (93.004%)\n",
      "Epoch: [41][300/391]\tTime 0.042 (0.042)\tData 0.002 (0.004)\tLoss 0.2600 (0.2107)\tPrec 92.188% (92.740%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.213 (0.213)\tLoss 0.2813 (0.2813)\tPrec 89.062% (89.062%)\n",
      " * Prec 87.030% \n",
      "best acc: 87.460000\n",
      "Epoch: [42][0/391]\tTime 0.499 (0.499)\tData 0.451 (0.451)\tLoss 0.1663 (0.1663)\tPrec 94.531% (94.531%)\n",
      "Epoch: [42][100/391]\tTime 0.041 (0.045)\tData 0.002 (0.007)\tLoss 0.1562 (0.2061)\tPrec 94.531% (93.209%)\n",
      "Epoch: [42][200/391]\tTime 0.040 (0.043)\tData 0.003 (0.005)\tLoss 0.2405 (0.2047)\tPrec 92.969% (93.136%)\n",
      "Epoch: [42][300/391]\tTime 0.042 (0.042)\tData 0.003 (0.004)\tLoss 0.1596 (0.2090)\tPrec 92.188% (92.945%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.231 (0.231)\tLoss 0.2171 (0.2171)\tPrec 94.531% (94.531%)\n",
      " * Prec 86.790% \n",
      "best acc: 87.460000\n",
      "Epoch: [43][0/391]\tTime 0.639 (0.639)\tData 0.591 (0.591)\tLoss 0.2209 (0.2209)\tPrec 91.406% (91.406%)\n",
      "Epoch: [43][100/391]\tTime 0.038 (0.047)\tData 0.002 (0.008)\tLoss 0.1819 (0.1874)\tPrec 92.188% (93.843%)\n",
      "Epoch: [43][200/391]\tTime 0.040 (0.044)\tData 0.002 (0.005)\tLoss 0.1554 (0.1894)\tPrec 96.094% (93.731%)\n",
      "Epoch: [43][300/391]\tTime 0.040 (0.042)\tData 0.002 (0.004)\tLoss 0.1795 (0.1978)\tPrec 92.969% (93.317%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.222 (0.222)\tLoss 0.2295 (0.2295)\tPrec 91.406% (91.406%)\n",
      " * Prec 87.630% \n",
      "best acc: 87.630000\n",
      "Epoch: [44][0/391]\tTime 0.414 (0.414)\tData 0.366 (0.366)\tLoss 0.1178 (0.1178)\tPrec 96.094% (96.094%)\n",
      "Epoch: [44][100/391]\tTime 0.041 (0.044)\tData 0.002 (0.006)\tLoss 0.2536 (0.1906)\tPrec 90.625% (93.526%)\n",
      "Epoch: [44][200/391]\tTime 0.040 (0.042)\tData 0.002 (0.004)\tLoss 0.2291 (0.1909)\tPrec 91.406% (93.385%)\n",
      "Epoch: [44][300/391]\tTime 0.040 (0.042)\tData 0.003 (0.004)\tLoss 0.1562 (0.1944)\tPrec 94.531% (93.306%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.224 (0.224)\tLoss 0.3009 (0.3009)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.480% \n",
      "best acc: 87.630000\n",
      "Epoch: [45][0/391]\tTime 0.462 (0.462)\tData 0.412 (0.412)\tLoss 0.0971 (0.0971)\tPrec 97.656% (97.656%)\n",
      "Epoch: [45][100/391]\tTime 0.042 (0.044)\tData 0.002 (0.006)\tLoss 0.2274 (0.1805)\tPrec 92.969% (93.912%)\n",
      "Epoch: [45][200/391]\tTime 0.041 (0.042)\tData 0.002 (0.004)\tLoss 0.1112 (0.1894)\tPrec 93.750% (93.505%)\n",
      "Epoch: [45][300/391]\tTime 0.041 (0.042)\tData 0.003 (0.004)\tLoss 0.2017 (0.1938)\tPrec 92.969% (93.363%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.212 (0.212)\tLoss 0.3661 (0.3661)\tPrec 86.719% (86.719%)\n",
      " * Prec 86.830% \n",
      "best acc: 87.630000\n",
      "Epoch: [46][0/391]\tTime 0.479 (0.479)\tData 0.429 (0.429)\tLoss 0.1269 (0.1269)\tPrec 97.656% (97.656%)\n",
      "Epoch: [46][100/391]\tTime 0.041 (0.045)\tData 0.003 (0.007)\tLoss 0.2050 (0.1812)\tPrec 92.188% (93.789%)\n",
      "Epoch: [46][200/391]\tTime 0.040 (0.043)\tData 0.003 (0.005)\tLoss 0.0657 (0.1822)\tPrec 98.438% (93.766%)\n",
      "Epoch: [46][300/391]\tTime 0.039 (0.042)\tData 0.003 (0.004)\tLoss 0.2075 (0.1853)\tPrec 93.750% (93.664%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.223 (0.223)\tLoss 0.2481 (0.2481)\tPrec 91.406% (91.406%)\n",
      " * Prec 88.410% \n",
      "best acc: 88.410000\n",
      "Epoch: [47][0/391]\tTime 0.450 (0.450)\tData 0.401 (0.401)\tLoss 0.1234 (0.1234)\tPrec 96.094% (96.094%)\n",
      "Epoch: [47][100/391]\tTime 0.038 (0.045)\tData 0.003 (0.006)\tLoss 0.0949 (0.1770)\tPrec 96.875% (93.866%)\n",
      "Epoch: [47][200/391]\tTime 0.041 (0.042)\tData 0.002 (0.004)\tLoss 0.1503 (0.1733)\tPrec 96.094% (93.968%)\n",
      "Epoch: [47][300/391]\tTime 0.041 (0.042)\tData 0.002 (0.004)\tLoss 0.1567 (0.1778)\tPrec 93.750% (93.846%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.219 (0.219)\tLoss 0.3265 (0.3265)\tPrec 89.062% (89.062%)\n",
      " * Prec 87.090% \n",
      "best acc: 88.410000\n",
      "Epoch: [48][0/391]\tTime 0.417 (0.417)\tData 0.367 (0.367)\tLoss 0.1338 (0.1338)\tPrec 94.531% (94.531%)\n",
      "Epoch: [48][100/391]\tTime 0.040 (0.044)\tData 0.002 (0.006)\tLoss 0.2869 (0.1717)\tPrec 87.500% (94.129%)\n",
      "Epoch: [48][200/391]\tTime 0.038 (0.042)\tData 0.002 (0.004)\tLoss 0.1685 (0.1711)\tPrec 96.094% (94.166%)\n",
      "Epoch: [48][300/391]\tTime 0.037 (0.042)\tData 0.002 (0.004)\tLoss 0.0955 (0.1710)\tPrec 96.094% (94.093%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.247 (0.247)\tLoss 0.3803 (0.3803)\tPrec 86.719% (86.719%)\n",
      " * Prec 86.510% \n",
      "best acc: 88.410000\n",
      "Epoch: [49][0/391]\tTime 0.505 (0.505)\tData 0.457 (0.457)\tLoss 0.1618 (0.1618)\tPrec 92.969% (92.969%)\n",
      "Epoch: [49][100/391]\tTime 0.041 (0.045)\tData 0.002 (0.007)\tLoss 0.1823 (0.1644)\tPrec 94.531% (94.338%)\n",
      "Epoch: [49][200/391]\tTime 0.037 (0.043)\tData 0.002 (0.005)\tLoss 0.2791 (0.1678)\tPrec 90.625% (94.321%)\n",
      "Epoch: [49][300/391]\tTime 0.037 (0.042)\tData 0.002 (0.004)\tLoss 0.2252 (0.1744)\tPrec 92.188% (94.067%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.254 (0.254)\tLoss 0.3002 (0.3002)\tPrec 92.188% (92.188%)\n",
      " * Prec 87.510% \n",
      "best acc: 88.410000\n",
      "Epoch: [50][0/391]\tTime 0.434 (0.434)\tData 0.396 (0.396)\tLoss 0.1710 (0.1710)\tPrec 96.875% (96.875%)\n",
      "Epoch: [50][100/391]\tTime 0.029 (0.033)\tData 0.002 (0.006)\tLoss 0.1063 (0.1551)\tPrec 98.438% (94.841%)\n",
      "Epoch: [50][200/391]\tTime 0.029 (0.031)\tData 0.002 (0.004)\tLoss 0.1509 (0.1569)\tPrec 94.531% (94.617%)\n",
      "Epoch: [50][300/391]\tTime 0.033 (0.031)\tData 0.001 (0.003)\tLoss 0.3194 (0.1625)\tPrec 90.625% (94.376%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.212 (0.212)\tLoss 0.3266 (0.3266)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.910% \n",
      "best acc: 88.410000\n",
      "Epoch: [51][0/391]\tTime 0.575 (0.575)\tData 0.526 (0.526)\tLoss 0.2184 (0.2184)\tPrec 92.969% (92.969%)\n",
      "Epoch: [51][100/391]\tTime 0.043 (0.046)\tData 0.003 (0.008)\tLoss 0.1063 (0.1553)\tPrec 95.312% (94.670%)\n",
      "Epoch: [51][200/391]\tTime 0.041 (0.044)\tData 0.003 (0.005)\tLoss 0.1131 (0.1629)\tPrec 96.875% (94.337%)\n",
      "Epoch: [51][300/391]\tTime 0.040 (0.043)\tData 0.002 (0.004)\tLoss 0.2400 (0.1676)\tPrec 92.969% (94.150%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.206 (0.206)\tLoss 0.2700 (0.2700)\tPrec 92.188% (92.188%)\n",
      " * Prec 87.060% \n",
      "best acc: 88.410000\n",
      "Epoch: [52][0/391]\tTime 0.417 (0.417)\tData 0.366 (0.366)\tLoss 0.1798 (0.1798)\tPrec 93.750% (93.750%)\n",
      "Epoch: [52][100/391]\tTime 0.039 (0.045)\tData 0.002 (0.006)\tLoss 0.1161 (0.1430)\tPrec 97.656% (94.887%)\n",
      "Epoch: [52][200/391]\tTime 0.042 (0.043)\tData 0.003 (0.004)\tLoss 0.1458 (0.1579)\tPrec 92.969% (94.492%)\n",
      "Epoch: [52][300/391]\tTime 0.041 (0.042)\tData 0.003 (0.004)\tLoss 0.1595 (0.1572)\tPrec 94.531% (94.547%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.227 (0.227)\tLoss 0.4715 (0.4715)\tPrec 86.719% (86.719%)\n",
      " * Prec 86.550% \n",
      "best acc: 88.410000\n",
      "Epoch: [53][0/391]\tTime 0.420 (0.420)\tData 0.370 (0.370)\tLoss 0.1409 (0.1409)\tPrec 94.531% (94.531%)\n",
      "Epoch: [53][100/391]\tTime 0.040 (0.044)\tData 0.002 (0.006)\tLoss 0.0898 (0.1455)\tPrec 97.656% (95.096%)\n",
      "Epoch: [53][200/391]\tTime 0.040 (0.042)\tData 0.002 (0.004)\tLoss 0.2239 (0.1523)\tPrec 90.625% (94.726%)\n",
      "Epoch: [53][300/391]\tTime 0.041 (0.042)\tData 0.003 (0.004)\tLoss 0.1351 (0.1579)\tPrec 96.094% (94.534%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.210 (0.210)\tLoss 0.3727 (0.3727)\tPrec 87.500% (87.500%)\n",
      " * Prec 85.350% \n",
      "best acc: 88.410000\n",
      "Epoch: [54][0/391]\tTime 0.561 (0.561)\tData 0.509 (0.509)\tLoss 0.1744 (0.1744)\tPrec 96.094% (96.094%)\n",
      "Epoch: [54][100/391]\tTime 0.039 (0.046)\tData 0.002 (0.007)\tLoss 0.1177 (0.1512)\tPrec 96.875% (94.872%)\n",
      "Epoch: [54][200/391]\tTime 0.041 (0.043)\tData 0.003 (0.005)\tLoss 0.1609 (0.1572)\tPrec 93.750% (94.625%)\n",
      "Epoch: [54][300/391]\tTime 0.039 (0.042)\tData 0.002 (0.004)\tLoss 0.1780 (0.1569)\tPrec 95.312% (94.653%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.202 (0.202)\tLoss 0.3740 (0.3740)\tPrec 90.625% (90.625%)\n",
      " * Prec 87.090% \n",
      "best acc: 88.410000\n",
      "Epoch: [55][0/391]\tTime 0.529 (0.529)\tData 0.479 (0.479)\tLoss 0.1518 (0.1518)\tPrec 94.531% (94.531%)\n",
      "Epoch: [55][100/391]\tTime 0.037 (0.046)\tData 0.003 (0.007)\tLoss 0.2254 (0.1469)\tPrec 92.969% (94.957%)\n",
      "Epoch: [55][200/391]\tTime 0.041 (0.043)\tData 0.002 (0.005)\tLoss 0.1043 (0.1488)\tPrec 96.875% (94.788%)\n",
      "Epoch: [55][300/391]\tTime 0.041 (0.042)\tData 0.002 (0.004)\tLoss 0.1629 (0.1526)\tPrec 94.531% (94.687%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.229 (0.229)\tLoss 0.4518 (0.4518)\tPrec 85.938% (85.938%)\n",
      " * Prec 86.600% \n",
      "best acc: 88.410000\n",
      "Epoch: [56][0/391]\tTime 0.459 (0.459)\tData 0.411 (0.411)\tLoss 0.1223 (0.1223)\tPrec 94.531% (94.531%)\n",
      "Epoch: [56][100/391]\tTime 0.040 (0.044)\tData 0.002 (0.007)\tLoss 0.1764 (0.1402)\tPrec 94.531% (95.019%)\n",
      "Epoch: [56][200/391]\tTime 0.041 (0.042)\tData 0.002 (0.004)\tLoss 0.0783 (0.1409)\tPrec 96.094% (95.072%)\n",
      "Epoch: [56][300/391]\tTime 0.041 (0.042)\tData 0.002 (0.004)\tLoss 0.0623 (0.1450)\tPrec 99.219% (94.879%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.234 (0.234)\tLoss 0.2055 (0.2055)\tPrec 96.094% (96.094%)\n",
      " * Prec 88.930% \n",
      "best acc: 88.930000\n",
      "Epoch: [57][0/391]\tTime 0.425 (0.425)\tData 0.374 (0.374)\tLoss 0.1273 (0.1273)\tPrec 92.969% (92.969%)\n",
      "Epoch: [57][100/391]\tTime 0.042 (0.044)\tData 0.003 (0.006)\tLoss 0.0473 (0.1358)\tPrec 100.000% (95.320%)\n",
      "Epoch: [57][200/391]\tTime 0.039 (0.042)\tData 0.003 (0.004)\tLoss 0.0990 (0.1397)\tPrec 96.875% (95.165%)\n",
      "Epoch: [57][300/391]\tTime 0.042 (0.042)\tData 0.003 (0.004)\tLoss 0.1335 (0.1453)\tPrec 95.312% (94.926%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.205 (0.205)\tLoss 0.1975 (0.1975)\tPrec 92.188% (92.188%)\n",
      " * Prec 88.280% \n",
      "best acc: 88.930000\n",
      "Epoch: [58][0/391]\tTime 0.508 (0.508)\tData 0.461 (0.461)\tLoss 0.1432 (0.1432)\tPrec 94.531% (94.531%)\n",
      "Epoch: [58][100/391]\tTime 0.040 (0.045)\tData 0.002 (0.007)\tLoss 0.2123 (0.1438)\tPrec 93.750% (95.088%)\n",
      "Epoch: [58][200/391]\tTime 0.040 (0.043)\tData 0.002 (0.005)\tLoss 0.0969 (0.1458)\tPrec 96.875% (95.060%)\n",
      "Epoch: [58][300/391]\tTime 0.042 (0.042)\tData 0.003 (0.004)\tLoss 0.1157 (0.1450)\tPrec 96.094% (95.040%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.213 (0.213)\tLoss 0.3487 (0.3487)\tPrec 88.281% (88.281%)\n",
      " * Prec 88.530% \n",
      "best acc: 88.930000\n",
      "Epoch: [59][0/391]\tTime 0.513 (0.513)\tData 0.463 (0.463)\tLoss 0.1674 (0.1674)\tPrec 96.094% (96.094%)\n",
      "Epoch: [59][100/391]\tTime 0.041 (0.045)\tData 0.002 (0.007)\tLoss 0.1926 (0.1335)\tPrec 92.969% (95.506%)\n",
      "Epoch: [59][200/391]\tTime 0.041 (0.043)\tData 0.002 (0.005)\tLoss 0.1422 (0.1335)\tPrec 94.531% (95.515%)\n",
      "Epoch: [59][300/391]\tTime 0.040 (0.042)\tData 0.003 (0.004)\tLoss 0.2461 (0.1381)\tPrec 88.281% (95.294%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.200 (0.200)\tLoss 0.3835 (0.3835)\tPrec 87.500% (87.500%)\n",
      " * Prec 88.330% \n",
      "best acc: 88.930000\n",
      "Epoch: [60][0/391]\tTime 0.490 (0.490)\tData 0.444 (0.444)\tLoss 0.0547 (0.0547)\tPrec 100.000% (100.000%)\n",
      "Epoch: [60][100/391]\tTime 0.041 (0.045)\tData 0.002 (0.007)\tLoss 0.1704 (0.1300)\tPrec 92.969% (95.622%)\n",
      "Epoch: [60][200/391]\tTime 0.045 (0.042)\tData 0.003 (0.005)\tLoss 0.1890 (0.1332)\tPrec 95.312% (95.414%)\n",
      "Epoch: [60][300/391]\tTime 0.037 (0.042)\tData 0.002 (0.004)\tLoss 0.1368 (0.1348)\tPrec 96.875% (95.341%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.249 (0.249)\tLoss 0.3998 (0.3998)\tPrec 88.281% (88.281%)\n",
      " * Prec 87.110% \n",
      "best acc: 88.930000\n",
      "Epoch: [61][0/391]\tTime 0.437 (0.437)\tData 0.388 (0.388)\tLoss 0.2166 (0.2166)\tPrec 92.188% (92.188%)\n",
      "Epoch: [61][100/391]\tTime 0.041 (0.044)\tData 0.003 (0.006)\tLoss 0.0786 (0.1326)\tPrec 97.656% (95.475%)\n",
      "Epoch: [61][200/391]\tTime 0.040 (0.042)\tData 0.002 (0.004)\tLoss 0.1141 (0.1380)\tPrec 96.094% (95.347%)\n",
      "Epoch: [61][300/391]\tTime 0.041 (0.041)\tData 0.003 (0.004)\tLoss 0.0724 (0.1348)\tPrec 98.438% (95.450%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.197 (0.197)\tLoss 0.3507 (0.3507)\tPrec 89.062% (89.062%)\n",
      " * Prec 87.530% \n",
      "best acc: 88.930000\n",
      "Epoch: [62][0/391]\tTime 0.518 (0.518)\tData 0.468 (0.468)\tLoss 0.1970 (0.1970)\tPrec 90.625% (90.625%)\n",
      "Epoch: [62][100/391]\tTime 0.030 (0.041)\tData 0.001 (0.007)\tLoss 0.1219 (0.1251)\tPrec 96.094% (95.606%)\n",
      "Epoch: [62][200/391]\tTime 0.031 (0.036)\tData 0.002 (0.004)\tLoss 0.0571 (0.1228)\tPrec 98.438% (95.732%)\n",
      "Epoch: [62][300/391]\tTime 0.030 (0.034)\tData 0.001 (0.003)\tLoss 0.1478 (0.1293)\tPrec 93.750% (95.577%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.246 (0.246)\tLoss 0.3655 (0.3655)\tPrec 89.844% (89.844%)\n",
      " * Prec 87.530% \n",
      "best acc: 88.930000\n",
      "Epoch: [63][0/391]\tTime 0.548 (0.548)\tData 0.496 (0.496)\tLoss 0.1237 (0.1237)\tPrec 95.312% (95.312%)\n",
      "Epoch: [63][100/391]\tTime 0.036 (0.045)\tData 0.002 (0.007)\tLoss 0.0445 (0.1361)\tPrec 98.438% (95.359%)\n",
      "Epoch: [63][200/391]\tTime 0.040 (0.043)\tData 0.002 (0.005)\tLoss 0.1146 (0.1319)\tPrec 96.094% (95.526%)\n",
      "Epoch: [63][300/391]\tTime 0.041 (0.042)\tData 0.002 (0.004)\tLoss 0.1248 (0.1305)\tPrec 96.875% (95.549%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.223 (0.223)\tLoss 0.4045 (0.4045)\tPrec 89.844% (89.844%)\n",
      " * Prec 87.300% \n",
      "best acc: 88.930000\n",
      "Epoch: [64][0/391]\tTime 0.529 (0.529)\tData 0.479 (0.479)\tLoss 0.0744 (0.0744)\tPrec 97.656% (97.656%)\n",
      "Epoch: [64][100/391]\tTime 0.041 (0.045)\tData 0.002 (0.007)\tLoss 0.0606 (0.1217)\tPrec 98.438% (95.900%)\n",
      "Epoch: [64][200/391]\tTime 0.040 (0.043)\tData 0.002 (0.005)\tLoss 0.1590 (0.1261)\tPrec 94.531% (95.658%)\n",
      "Epoch: [64][300/391]\tTime 0.041 (0.042)\tData 0.003 (0.004)\tLoss 0.0546 (0.1259)\tPrec 96.875% (95.655%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.208 (0.208)\tLoss 0.3343 (0.3343)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.530% \n",
      "best acc: 88.930000\n",
      "Epoch: [65][0/391]\tTime 0.527 (0.527)\tData 0.479 (0.479)\tLoss 0.0861 (0.0861)\tPrec 98.438% (98.438%)\n",
      "Epoch: [65][100/391]\tTime 0.043 (0.045)\tData 0.002 (0.007)\tLoss 0.0798 (0.1160)\tPrec 96.875% (96.016%)\n",
      "Epoch: [65][200/391]\tTime 0.043 (0.043)\tData 0.003 (0.005)\tLoss 0.1953 (0.1232)\tPrec 92.969% (95.779%)\n",
      "Epoch: [65][300/391]\tTime 0.040 (0.042)\tData 0.002 (0.004)\tLoss 0.2593 (0.1273)\tPrec 92.969% (95.598%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.230 (0.230)\tLoss 0.4029 (0.4029)\tPrec 89.062% (89.062%)\n",
      " * Prec 88.280% \n",
      "best acc: 88.930000\n",
      "Epoch: [66][0/391]\tTime 0.405 (0.405)\tData 0.357 (0.357)\tLoss 0.0718 (0.0718)\tPrec 98.438% (98.438%)\n",
      "Epoch: [66][100/391]\tTime 0.040 (0.044)\tData 0.002 (0.006)\tLoss 0.1396 (0.1098)\tPrec 95.312% (96.318%)\n",
      "Epoch: [66][200/391]\tTime 0.041 (0.042)\tData 0.002 (0.004)\tLoss 0.0482 (0.1152)\tPrec 97.656% (96.203%)\n",
      "Epoch: [66][300/391]\tTime 0.041 (0.042)\tData 0.002 (0.004)\tLoss 0.1113 (0.1207)\tPrec 98.438% (95.969%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.243 (0.243)\tLoss 0.4858 (0.4858)\tPrec 86.719% (86.719%)\n",
      " * Prec 85.980% \n",
      "best acc: 88.930000\n",
      "Epoch: [67][0/391]\tTime 0.405 (0.405)\tData 0.356 (0.356)\tLoss 0.0912 (0.0912)\tPrec 96.875% (96.875%)\n",
      "Epoch: [67][100/391]\tTime 0.039 (0.044)\tData 0.002 (0.006)\tLoss 0.1326 (0.1132)\tPrec 96.094% (96.225%)\n",
      "Epoch: [67][200/391]\tTime 0.039 (0.043)\tData 0.002 (0.004)\tLoss 0.1521 (0.1170)\tPrec 96.094% (96.043%)\n",
      "Epoch: [67][300/391]\tTime 0.043 (0.042)\tData 0.002 (0.004)\tLoss 0.2102 (0.1205)\tPrec 92.188% (95.943%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.213 (0.213)\tLoss 0.2510 (0.2510)\tPrec 90.625% (90.625%)\n",
      " * Prec 88.030% \n",
      "best acc: 88.930000\n",
      "Epoch: [68][0/391]\tTime 0.541 (0.541)\tData 0.490 (0.490)\tLoss 0.0868 (0.0868)\tPrec 97.656% (97.656%)\n",
      "Epoch: [68][100/391]\tTime 0.043 (0.046)\tData 0.003 (0.007)\tLoss 0.1430 (0.1144)\tPrec 96.094% (96.094%)\n",
      "Epoch: [68][200/391]\tTime 0.040 (0.043)\tData 0.002 (0.005)\tLoss 0.1279 (0.1211)\tPrec 95.312% (95.841%)\n",
      "Epoch: [68][300/391]\tTime 0.039 (0.042)\tData 0.003 (0.004)\tLoss 0.0779 (0.1216)\tPrec 96.094% (95.821%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.226 (0.226)\tLoss 0.2557 (0.2557)\tPrec 89.844% (89.844%)\n",
      " * Prec 87.910% \n",
      "best acc: 88.930000\n",
      "Epoch: [69][0/391]\tTime 0.473 (0.473)\tData 0.426 (0.426)\tLoss 0.1380 (0.1380)\tPrec 95.312% (95.312%)\n",
      "Epoch: [69][100/391]\tTime 0.041 (0.045)\tData 0.003 (0.007)\tLoss 0.0770 (0.1101)\tPrec 97.656% (96.225%)\n",
      "Epoch: [69][200/391]\tTime 0.041 (0.043)\tData 0.003 (0.005)\tLoss 0.1945 (0.1137)\tPrec 94.531% (96.070%)\n",
      "Epoch: [69][300/391]\tTime 0.042 (0.042)\tData 0.003 (0.004)\tLoss 0.1118 (0.1179)\tPrec 96.094% (95.912%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.224 (0.224)\tLoss 0.2539 (0.2539)\tPrec 89.844% (89.844%)\n",
      " * Prec 88.480% \n",
      "best acc: 88.930000\n",
      "Epoch: [70][0/391]\tTime 0.430 (0.430)\tData 0.382 (0.382)\tLoss 0.0987 (0.0987)\tPrec 96.094% (96.094%)\n",
      "Epoch: [70][100/391]\tTime 0.038 (0.044)\tData 0.003 (0.006)\tLoss 0.1022 (0.0840)\tPrec 96.875% (97.138%)\n",
      "Epoch: [70][200/391]\tTime 0.051 (0.042)\tData 0.011 (0.004)\tLoss 0.0689 (0.0746)\tPrec 97.656% (97.512%)\n",
      "Epoch: [70][300/391]\tTime 0.041 (0.042)\tData 0.002 (0.004)\tLoss 0.0566 (0.0708)\tPrec 98.438% (97.617%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.221 (0.221)\tLoss 0.1900 (0.1900)\tPrec 94.531% (94.531%)\n",
      " * Prec 90.760% \n",
      "best acc: 90.760000\n",
      "Epoch: [71][0/391]\tTime 0.446 (0.446)\tData 0.395 (0.395)\tLoss 0.0360 (0.0360)\tPrec 98.438% (98.438%)\n",
      "Epoch: [71][100/391]\tTime 0.039 (0.045)\tData 0.003 (0.007)\tLoss 0.0093 (0.0456)\tPrec 100.000% (98.461%)\n",
      "Epoch: [71][200/391]\tTime 0.038 (0.043)\tData 0.002 (0.005)\tLoss 0.1062 (0.0444)\tPrec 96.875% (98.515%)\n",
      "Epoch: [71][300/391]\tTime 0.041 (0.042)\tData 0.002 (0.004)\tLoss 0.0712 (0.0442)\tPrec 96.094% (98.515%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.217 (0.217)\tLoss 0.2130 (0.2130)\tPrec 92.969% (92.969%)\n",
      " * Prec 90.790% \n",
      "best acc: 90.790000\n",
      "Epoch: [72][0/391]\tTime 0.449 (0.449)\tData 0.399 (0.399)\tLoss 0.0517 (0.0517)\tPrec 98.438% (98.438%)\n",
      "Epoch: [72][100/391]\tTime 0.040 (0.045)\tData 0.002 (0.006)\tLoss 0.0265 (0.0399)\tPrec 98.438% (98.747%)\n",
      "Epoch: [72][200/391]\tTime 0.042 (0.043)\tData 0.002 (0.004)\tLoss 0.0779 (0.0364)\tPrec 97.656% (98.869%)\n",
      "Epoch: [72][300/391]\tTime 0.049 (0.042)\tData 0.004 (0.004)\tLoss 0.0273 (0.0375)\tPrec 99.219% (98.803%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.238 (0.238)\tLoss 0.2366 (0.2366)\tPrec 93.750% (93.750%)\n",
      " * Prec 90.990% \n",
      "best acc: 90.990000\n",
      "Epoch: [73][0/391]\tTime 0.651 (0.651)\tData 0.596 (0.596)\tLoss 0.0328 (0.0328)\tPrec 98.438% (98.438%)\n",
      "Epoch: [73][100/391]\tTime 0.040 (0.047)\tData 0.002 (0.008)\tLoss 0.0480 (0.0290)\tPrec 98.438% (99.103%)\n",
      "Epoch: [73][200/391]\tTime 0.042 (0.044)\tData 0.003 (0.005)\tLoss 0.0266 (0.0313)\tPrec 99.219% (98.997%)\n",
      "Epoch: [73][300/391]\tTime 0.039 (0.043)\tData 0.003 (0.004)\tLoss 0.0081 (0.0311)\tPrec 100.000% (98.954%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.350 (0.350)\tLoss 0.2294 (0.2294)\tPrec 95.312% (95.312%)\n",
      " * Prec 91.040% \n",
      "best acc: 91.040000\n",
      "Epoch: [74][0/391]\tTime 0.455 (0.455)\tData 0.403 (0.403)\tLoss 0.0428 (0.0428)\tPrec 97.656% (97.656%)\n",
      "Epoch: [74][100/391]\tTime 0.041 (0.045)\tData 0.002 (0.007)\tLoss 0.0153 (0.0288)\tPrec 100.000% (99.056%)\n",
      "Epoch: [74][200/391]\tTime 0.043 (0.043)\tData 0.003 (0.005)\tLoss 0.0157 (0.0286)\tPrec 99.219% (99.005%)\n",
      "Epoch: [74][300/391]\tTime 0.039 (0.042)\tData 0.003 (0.004)\tLoss 0.0322 (0.0279)\tPrec 99.219% (99.053%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.237 (0.237)\tLoss 0.2275 (0.2275)\tPrec 92.969% (92.969%)\n",
      " * Prec 91.230% \n",
      "best acc: 91.230000\n",
      "Epoch: [75][0/391]\tTime 0.498 (0.498)\tData 0.450 (0.450)\tLoss 0.0417 (0.0417)\tPrec 98.438% (98.438%)\n",
      "Epoch: [75][100/391]\tTime 0.039 (0.045)\tData 0.002 (0.007)\tLoss 0.0439 (0.0261)\tPrec 97.656% (99.087%)\n",
      "Epoch: [75][200/391]\tTime 0.041 (0.043)\tData 0.002 (0.005)\tLoss 0.0420 (0.0285)\tPrec 97.656% (99.001%)\n",
      "Epoch: [75][300/391]\tTime 0.041 (0.042)\tData 0.002 (0.004)\tLoss 0.0095 (0.0280)\tPrec 100.000% (99.034%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.228 (0.228)\tLoss 0.2123 (0.2123)\tPrec 94.531% (94.531%)\n",
      " * Prec 91.140% \n",
      "best acc: 91.230000\n",
      "Epoch: [76][0/391]\tTime 0.379 (0.379)\tData 0.315 (0.315)\tLoss 0.0356 (0.0356)\tPrec 98.438% (98.438%)\n",
      "Epoch: [76][100/391]\tTime 0.041 (0.044)\tData 0.003 (0.006)\tLoss 0.0091 (0.0233)\tPrec 100.000% (99.273%)\n",
      "Epoch: [76][200/391]\tTime 0.042 (0.042)\tData 0.003 (0.004)\tLoss 0.0051 (0.0252)\tPrec 100.000% (99.149%)\n",
      "Epoch: [76][300/391]\tTime 0.042 (0.042)\tData 0.003 (0.004)\tLoss 0.0428 (0.0256)\tPrec 97.656% (99.159%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.234 (0.234)\tLoss 0.2319 (0.2319)\tPrec 93.750% (93.750%)\n",
      " * Prec 91.290% \n",
      "best acc: 91.290000\n",
      "Epoch: [77][0/391]\tTime 0.445 (0.445)\tData 0.397 (0.397)\tLoss 0.0216 (0.0216)\tPrec 99.219% (99.219%)\n",
      "Epoch: [77][100/391]\tTime 0.039 (0.045)\tData 0.003 (0.007)\tLoss 0.0222 (0.0222)\tPrec 99.219% (99.327%)\n",
      "Epoch: [77][200/391]\tTime 0.036 (0.043)\tData 0.003 (0.005)\tLoss 0.0327 (0.0218)\tPrec 98.438% (99.312%)\n",
      "Epoch: [77][300/391]\tTime 0.042 (0.042)\tData 0.003 (0.004)\tLoss 0.0255 (0.0227)\tPrec 99.219% (99.265%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.433 (0.433)\tLoss 0.2843 (0.2843)\tPrec 94.531% (94.531%)\n",
      " * Prec 91.000% \n",
      "best acc: 91.290000\n",
      "Epoch: [78][0/391]\tTime 0.476 (0.476)\tData 0.427 (0.427)\tLoss 0.0132 (0.0132)\tPrec 99.219% (99.219%)\n",
      "Epoch: [78][100/391]\tTime 0.041 (0.045)\tData 0.003 (0.007)\tLoss 0.0796 (0.0241)\tPrec 98.438% (99.110%)\n",
      "Epoch: [78][200/391]\tTime 0.038 (0.043)\tData 0.002 (0.005)\tLoss 0.0267 (0.0221)\tPrec 99.219% (99.219%)\n",
      "Epoch: [78][300/391]\tTime 0.042 (0.042)\tData 0.002 (0.004)\tLoss 0.0136 (0.0219)\tPrec 100.000% (99.255%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.227 (0.227)\tLoss 0.2762 (0.2762)\tPrec 94.531% (94.531%)\n",
      " * Prec 91.220% \n",
      "best acc: 91.290000\n",
      "Epoch: [79][0/391]\tTime 0.537 (0.537)\tData 0.488 (0.488)\tLoss 0.0596 (0.0596)\tPrec 98.438% (98.438%)\n",
      "Epoch: [79][100/391]\tTime 0.039 (0.045)\tData 0.002 (0.007)\tLoss 0.0142 (0.0200)\tPrec 99.219% (99.265%)\n",
      "Epoch: [79][200/391]\tTime 0.041 (0.043)\tData 0.002 (0.005)\tLoss 0.0109 (0.0198)\tPrec 100.000% (99.289%)\n",
      "Epoch: [79][300/391]\tTime 0.042 (0.042)\tData 0.002 (0.004)\tLoss 0.0056 (0.0206)\tPrec 100.000% (99.294%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.201 (0.201)\tLoss 0.2746 (0.2746)\tPrec 94.531% (94.531%)\n",
      " * Prec 91.190% \n",
      "best acc: 91.290000\n",
      "Epoch: [80][0/391]\tTime 0.509 (0.509)\tData 0.461 (0.461)\tLoss 0.0196 (0.0196)\tPrec 99.219% (99.219%)\n",
      "Epoch: [80][100/391]\tTime 0.049 (0.046)\tData 0.003 (0.007)\tLoss 0.0067 (0.0201)\tPrec 100.000% (99.350%)\n",
      "Epoch: [80][200/391]\tTime 0.039 (0.043)\tData 0.002 (0.005)\tLoss 0.0066 (0.0199)\tPrec 100.000% (99.366%)\n",
      "Epoch: [80][300/391]\tTime 0.041 (0.042)\tData 0.002 (0.004)\tLoss 0.0123 (0.0200)\tPrec 100.000% (99.367%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.236 (0.236)\tLoss 0.2549 (0.2549)\tPrec 93.750% (93.750%)\n",
      " * Prec 91.260% \n",
      "best acc: 91.290000\n",
      "Epoch: [81][0/391]\tTime 0.569 (0.569)\tData 0.519 (0.519)\tLoss 0.0126 (0.0126)\tPrec 100.000% (100.000%)\n",
      "Epoch: [81][100/391]\tTime 0.040 (0.046)\tData 0.002 (0.008)\tLoss 0.0399 (0.0204)\tPrec 98.438% (99.319%)\n",
      "Epoch: [81][200/391]\tTime 0.037 (0.043)\tData 0.003 (0.005)\tLoss 0.0160 (0.0196)\tPrec 99.219% (99.331%)\n",
      "Epoch: [81][300/391]\tTime 0.037 (0.042)\tData 0.002 (0.004)\tLoss 0.0240 (0.0196)\tPrec 99.219% (99.343%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.222 (0.222)\tLoss 0.2526 (0.2526)\tPrec 94.531% (94.531%)\n",
      " * Prec 91.360% \n",
      "best acc: 91.360000\n",
      "Epoch: [82][0/391]\tTime 0.435 (0.435)\tData 0.384 (0.384)\tLoss 0.0352 (0.0352)\tPrec 99.219% (99.219%)\n",
      "Epoch: [82][100/391]\tTime 0.041 (0.044)\tData 0.002 (0.006)\tLoss 0.0214 (0.0175)\tPrec 99.219% (99.420%)\n",
      "Epoch: [82][200/391]\tTime 0.041 (0.042)\tData 0.002 (0.004)\tLoss 0.0126 (0.0176)\tPrec 99.219% (99.460%)\n",
      "Epoch: [82][300/391]\tTime 0.040 (0.042)\tData 0.002 (0.003)\tLoss 0.0805 (0.0187)\tPrec 97.656% (99.390%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.213 (0.213)\tLoss 0.2298 (0.2298)\tPrec 93.750% (93.750%)\n",
      " * Prec 91.210% \n",
      "best acc: 91.360000\n",
      "Epoch: [83][0/391]\tTime 0.475 (0.475)\tData 0.424 (0.424)\tLoss 0.0053 (0.0053)\tPrec 100.000% (100.000%)\n",
      "Epoch: [83][100/391]\tTime 0.040 (0.045)\tData 0.002 (0.007)\tLoss 0.0136 (0.0181)\tPrec 99.219% (99.466%)\n",
      "Epoch: [83][200/391]\tTime 0.041 (0.043)\tData 0.002 (0.005)\tLoss 0.0056 (0.0177)\tPrec 100.000% (99.433%)\n",
      "Epoch: [83][300/391]\tTime 0.041 (0.042)\tData 0.003 (0.004)\tLoss 0.0128 (0.0182)\tPrec 99.219% (99.403%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.232 (0.232)\tLoss 0.2610 (0.2610)\tPrec 95.312% (95.312%)\n",
      " * Prec 91.180% \n",
      "best acc: 91.360000\n",
      "Epoch: [84][0/391]\tTime 0.542 (0.542)\tData 0.494 (0.494)\tLoss 0.0587 (0.0587)\tPrec 97.656% (97.656%)\n",
      "Epoch: [84][100/391]\tTime 0.040 (0.045)\tData 0.002 (0.007)\tLoss 0.0055 (0.0167)\tPrec 100.000% (99.497%)\n",
      "Epoch: [84][200/391]\tTime 0.041 (0.043)\tData 0.002 (0.005)\tLoss 0.0076 (0.0179)\tPrec 100.000% (99.456%)\n",
      "Epoch: [84][300/391]\tTime 0.041 (0.042)\tData 0.002 (0.004)\tLoss 0.0127 (0.0176)\tPrec 99.219% (99.452%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.232 (0.232)\tLoss 0.2295 (0.2295)\tPrec 96.094% (96.094%)\n",
      " * Prec 91.340% \n",
      "best acc: 91.360000\n",
      "Epoch: [85][0/391]\tTime 0.380 (0.380)\tData 0.330 (0.330)\tLoss 0.0080 (0.0080)\tPrec 100.000% (100.000%)\n",
      "Epoch: [85][100/391]\tTime 0.040 (0.044)\tData 0.002 (0.006)\tLoss 0.0033 (0.0193)\tPrec 100.000% (99.474%)\n",
      "Epoch: [85][200/391]\tTime 0.042 (0.042)\tData 0.003 (0.004)\tLoss 0.0433 (0.0199)\tPrec 98.438% (99.405%)\n",
      "Epoch: [85][300/391]\tTime 0.033 (0.041)\tData 0.002 (0.004)\tLoss 0.0039 (0.0194)\tPrec 100.000% (99.390%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.238 (0.238)\tLoss 0.2478 (0.2478)\tPrec 94.531% (94.531%)\n",
      " * Prec 91.290% \n",
      "best acc: 91.360000\n",
      "Epoch: [86][0/391]\tTime 0.440 (0.440)\tData 0.393 (0.393)\tLoss 0.0264 (0.0264)\tPrec 99.219% (99.219%)\n",
      "Epoch: [86][100/391]\tTime 0.041 (0.045)\tData 0.002 (0.007)\tLoss 0.0036 (0.0166)\tPrec 100.000% (99.520%)\n",
      "Epoch: [86][200/391]\tTime 0.042 (0.043)\tData 0.002 (0.004)\tLoss 0.0386 (0.0179)\tPrec 99.219% (99.452%)\n",
      "Epoch: [86][300/391]\tTime 0.042 (0.042)\tData 0.002 (0.004)\tLoss 0.0414 (0.0184)\tPrec 98.438% (99.408%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.243 (0.243)\tLoss 0.2460 (0.2460)\tPrec 92.969% (92.969%)\n",
      " * Prec 91.250% \n",
      "best acc: 91.360000\n",
      "Epoch: [87][0/391]\tTime 0.454 (0.454)\tData 0.399 (0.399)\tLoss 0.0042 (0.0042)\tPrec 100.000% (100.000%)\n",
      "Epoch: [87][100/391]\tTime 0.044 (0.047)\tData 0.003 (0.007)\tLoss 0.0066 (0.0161)\tPrec 100.000% (99.435%)\n",
      "Epoch: [87][200/391]\tTime 0.037 (0.044)\tData 0.003 (0.005)\tLoss 0.0363 (0.0160)\tPrec 98.438% (99.429%)\n",
      "Epoch: [87][300/391]\tTime 0.044 (0.044)\tData 0.003 (0.004)\tLoss 0.0093 (0.0164)\tPrec 99.219% (99.421%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.245 (0.245)\tLoss 0.2341 (0.2341)\tPrec 93.750% (93.750%)\n",
      " * Prec 91.440% \n",
      "best acc: 91.440000\n",
      "Epoch: [88][0/391]\tTime 0.543 (0.543)\tData 0.490 (0.490)\tLoss 0.0515 (0.0515)\tPrec 99.219% (99.219%)\n",
      "Epoch: [88][100/391]\tTime 0.041 (0.046)\tData 0.002 (0.007)\tLoss 0.0152 (0.0166)\tPrec 99.219% (99.420%)\n",
      "Epoch: [88][200/391]\tTime 0.038 (0.043)\tData 0.002 (0.005)\tLoss 0.0058 (0.0164)\tPrec 100.000% (99.483%)\n",
      "Epoch: [88][300/391]\tTime 0.039 (0.042)\tData 0.003 (0.004)\tLoss 0.0039 (0.0168)\tPrec 100.000% (99.473%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.224 (0.224)\tLoss 0.2394 (0.2394)\tPrec 92.969% (92.969%)\n",
      " * Prec 91.200% \n",
      "best acc: 91.440000\n",
      "Epoch: [89][0/391]\tTime 0.612 (0.612)\tData 0.562 (0.562)\tLoss 0.0271 (0.0271)\tPrec 99.219% (99.219%)\n",
      "Epoch: [89][100/391]\tTime 0.038 (0.045)\tData 0.002 (0.008)\tLoss 0.0071 (0.0179)\tPrec 100.000% (99.428%)\n",
      "Epoch: [89][200/391]\tTime 0.039 (0.043)\tData 0.003 (0.005)\tLoss 0.0250 (0.0165)\tPrec 99.219% (99.468%)\n",
      "Epoch: [89][300/391]\tTime 0.036 (0.042)\tData 0.003 (0.004)\tLoss 0.0045 (0.0165)\tPrec 100.000% (99.478%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.220 (0.220)\tLoss 0.2214 (0.2214)\tPrec 94.531% (94.531%)\n",
      " * Prec 91.170% \n",
      "best acc: 91.440000\n",
      "Epoch: [90][0/391]\tTime 0.449 (0.449)\tData 0.401 (0.401)\tLoss 0.0032 (0.0032)\tPrec 100.000% (100.000%)\n",
      "Epoch: [90][100/391]\tTime 0.038 (0.044)\tData 0.002 (0.006)\tLoss 0.0075 (0.0160)\tPrec 100.000% (99.497%)\n",
      "Epoch: [90][200/391]\tTime 0.042 (0.042)\tData 0.002 (0.004)\tLoss 0.0246 (0.0166)\tPrec 99.219% (99.471%)\n",
      "Epoch: [90][300/391]\tTime 0.041 (0.042)\tData 0.002 (0.004)\tLoss 0.0160 (0.0164)\tPrec 99.219% (99.491%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.230 (0.230)\tLoss 0.2382 (0.2382)\tPrec 95.312% (95.312%)\n",
      " * Prec 91.330% \n",
      "best acc: 91.440000\n",
      "Epoch: [91][0/391]\tTime 0.462 (0.462)\tData 0.413 (0.413)\tLoss 0.0071 (0.0071)\tPrec 100.000% (100.000%)\n",
      "Epoch: [91][100/391]\tTime 0.040 (0.045)\tData 0.002 (0.007)\tLoss 0.0330 (0.0175)\tPrec 98.438% (99.397%)\n",
      "Epoch: [91][200/391]\tTime 0.038 (0.043)\tData 0.002 (0.005)\tLoss 0.0159 (0.0177)\tPrec 99.219% (99.413%)\n",
      "Epoch: [91][300/391]\tTime 0.040 (0.042)\tData 0.002 (0.004)\tLoss 0.0262 (0.0172)\tPrec 99.219% (99.447%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.226 (0.226)\tLoss 0.2445 (0.2445)\tPrec 93.750% (93.750%)\n",
      " * Prec 91.290% \n",
      "best acc: 91.440000\n",
      "Epoch: [92][0/391]\tTime 0.508 (0.508)\tData 0.458 (0.458)\tLoss 0.0099 (0.0099)\tPrec 99.219% (99.219%)\n",
      "Epoch: [92][100/391]\tTime 0.041 (0.045)\tData 0.002 (0.007)\tLoss 0.0316 (0.0187)\tPrec 99.219% (99.389%)\n",
      "Epoch: [92][200/391]\tTime 0.041 (0.043)\tData 0.003 (0.005)\tLoss 0.0143 (0.0171)\tPrec 99.219% (99.464%)\n",
      "Epoch: [92][300/391]\tTime 0.033 (0.042)\tData 0.002 (0.004)\tLoss 0.0424 (0.0180)\tPrec 99.219% (99.429%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.224 (0.224)\tLoss 0.2220 (0.2220)\tPrec 94.531% (94.531%)\n",
      " * Prec 91.380% \n",
      "best acc: 91.440000\n",
      "Epoch: [93][0/391]\tTime 0.488 (0.488)\tData 0.440 (0.440)\tLoss 0.0106 (0.0106)\tPrec 99.219% (99.219%)\n",
      "Epoch: [93][100/391]\tTime 0.042 (0.045)\tData 0.003 (0.007)\tLoss 0.0206 (0.0180)\tPrec 99.219% (99.451%)\n",
      "Epoch: [93][200/391]\tTime 0.042 (0.043)\tData 0.002 (0.005)\tLoss 0.0056 (0.0170)\tPrec 100.000% (99.456%)\n",
      "Epoch: [93][300/391]\tTime 0.041 (0.042)\tData 0.003 (0.004)\tLoss 0.0040 (0.0167)\tPrec 100.000% (99.452%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.206 (0.206)\tLoss 0.2816 (0.2816)\tPrec 93.750% (93.750%)\n",
      " * Prec 91.500% \n",
      "best acc: 91.500000\n",
      "Epoch: [94][0/391]\tTime 0.446 (0.446)\tData 0.396 (0.396)\tLoss 0.0032 (0.0032)\tPrec 100.000% (100.000%)\n",
      "Epoch: [94][100/391]\tTime 0.041 (0.045)\tData 0.002 (0.007)\tLoss 0.0118 (0.0196)\tPrec 99.219% (99.389%)\n",
      "Epoch: [94][200/391]\tTime 0.041 (0.043)\tData 0.002 (0.005)\tLoss 0.0326 (0.0196)\tPrec 97.656% (99.339%)\n",
      "Epoch: [94][300/391]\tTime 0.042 (0.042)\tData 0.003 (0.004)\tLoss 0.0252 (0.0193)\tPrec 99.219% (99.354%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.222 (0.222)\tLoss 0.2719 (0.2719)\tPrec 92.969% (92.969%)\n",
      " * Prec 91.380% \n",
      "best acc: 91.500000\n",
      "Epoch: [95][0/391]\tTime 0.471 (0.471)\tData 0.422 (0.422)\tLoss 0.0280 (0.0280)\tPrec 99.219% (99.219%)\n",
      "Epoch: [95][100/391]\tTime 0.040 (0.045)\tData 0.002 (0.007)\tLoss 0.0195 (0.0161)\tPrec 99.219% (99.520%)\n",
      "Epoch: [95][200/391]\tTime 0.043 (0.043)\tData 0.002 (0.004)\tLoss 0.0041 (0.0159)\tPrec 100.000% (99.514%)\n",
      "Epoch: [95][300/391]\tTime 0.043 (0.042)\tData 0.003 (0.004)\tLoss 0.0268 (0.0160)\tPrec 98.438% (99.512%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.224 (0.224)\tLoss 0.2768 (0.2768)\tPrec 92.969% (92.969%)\n",
      " * Prec 91.250% \n",
      "best acc: 91.500000\n",
      "Epoch: [96][0/391]\tTime 0.468 (0.468)\tData 0.422 (0.422)\tLoss 0.0064 (0.0064)\tPrec 100.000% (100.000%)\n",
      "Epoch: [96][100/391]\tTime 0.040 (0.045)\tData 0.002 (0.007)\tLoss 0.0456 (0.0153)\tPrec 99.219% (99.621%)\n",
      "Epoch: [96][200/391]\tTime 0.038 (0.043)\tData 0.002 (0.004)\tLoss 0.0136 (0.0157)\tPrec 100.000% (99.526%)\n",
      "Epoch: [96][300/391]\tTime 0.042 (0.042)\tData 0.002 (0.004)\tLoss 0.0074 (0.0153)\tPrec 100.000% (99.538%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.207 (0.207)\tLoss 0.2816 (0.2816)\tPrec 93.750% (93.750%)\n",
      " * Prec 91.310% \n",
      "best acc: 91.500000\n",
      "Epoch: [97][0/391]\tTime 0.487 (0.487)\tData 0.442 (0.442)\tLoss 0.0051 (0.0051)\tPrec 100.000% (100.000%)\n",
      "Epoch: [97][100/391]\tTime 0.043 (0.045)\tData 0.003 (0.007)\tLoss 0.0095 (0.0178)\tPrec 100.000% (99.404%)\n",
      "Epoch: [97][200/391]\tTime 0.041 (0.043)\tData 0.002 (0.005)\tLoss 0.0102 (0.0185)\tPrec 100.000% (99.374%)\n",
      "Epoch: [97][300/391]\tTime 0.040 (0.042)\tData 0.002 (0.004)\tLoss 0.0047 (0.0176)\tPrec 100.000% (99.424%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.248 (0.248)\tLoss 0.2710 (0.2710)\tPrec 94.531% (94.531%)\n",
      " * Prec 91.220% \n",
      "best acc: 91.500000\n",
      "Epoch: [98][0/391]\tTime 0.509 (0.509)\tData 0.472 (0.472)\tLoss 0.0234 (0.0234)\tPrec 99.219% (99.219%)\n",
      "Epoch: [98][100/391]\tTime 0.043 (0.043)\tData 0.003 (0.007)\tLoss 0.0237 (0.0170)\tPrec 98.438% (99.459%)\n",
      "Epoch: [98][200/391]\tTime 0.041 (0.042)\tData 0.003 (0.005)\tLoss 0.0396 (0.0172)\tPrec 99.219% (99.468%)\n",
      "Epoch: [98][300/391]\tTime 0.041 (0.041)\tData 0.002 (0.004)\tLoss 0.0094 (0.0169)\tPrec 100.000% (99.460%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.226 (0.226)\tLoss 0.2532 (0.2532)\tPrec 93.750% (93.750%)\n",
      " * Prec 91.240% \n",
      "best acc: 91.500000\n",
      "Epoch: [99][0/391]\tTime 0.443 (0.443)\tData 0.397 (0.397)\tLoss 0.0215 (0.0215)\tPrec 99.219% (99.219%)\n",
      "Epoch: [99][100/391]\tTime 0.039 (0.045)\tData 0.003 (0.006)\tLoss 0.0205 (0.0159)\tPrec 99.219% (99.451%)\n",
      "Epoch: [99][200/391]\tTime 0.040 (0.043)\tData 0.002 (0.004)\tLoss 0.0035 (0.0174)\tPrec 100.000% (99.409%)\n",
      "Epoch: [99][300/391]\tTime 0.041 (0.042)\tData 0.003 (0.004)\tLoss 0.0102 (0.0172)\tPrec 99.219% (99.406%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.218 (0.218)\tLoss 0.2393 (0.2393)\tPrec 94.531% (94.531%)\n",
      " * Prec 91.160% \n",
      "best acc: 91.500000\n"
     ]
    }
   ],
   "source": [
    "# This cell won't be given, but students will complete the training\n",
    "\n",
    "lr = 4e-2\n",
    "weight_decay = 1e-4\n",
    "epochs = 100\n",
    "best_prec = 0\n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "#cudnn.benchmark = True\n",
    "\n",
    "if not os.path.exists('result'):\n",
    "    os.makedirs('result')\n",
    "fdir = 'result/'+str(model_name)\n",
    "if not os.path.exists(fdir):\n",
    "    os.makedirs(fdir)\n",
    "        \n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    train(trainloader, model, criterion, optimizer, epoch)\n",
    "    \n",
    "    # evaluate on test set\n",
    "    print(\"Validation starts\")\n",
    "    prec = validate(testloader, model, criterion)\n",
    "\n",
    "    # remember best precision and save checkpoint\n",
    "    is_best = prec > best_prec\n",
    "    best_prec = max(prec,best_prec)\n",
    "    print('best acc: {:1f}'.format(best_prec))\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec': best_prec,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, fdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-harris",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HW\n",
    "\n",
    "#  1. Train with 4 bits for both weight and activation to achieve >90% accuracy\n",
    "#  2. Find x_int and w_int for the 2nd convolution layer\n",
    "#  3. Check the recovered psum has similar value to the un-quantized original psum\n",
    "#     (such as example 1 in W3S2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "entertaining-queensland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 9150/10000 (92%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = \"result/VGG16_project/model_best.pth.tar\"\n",
    "model_name = \"VGG16_project\"\n",
    "model = VGG16_project()\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\") \n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ceramic-nigeria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 -th layer prehooked\n",
      "7 -th layer prehooked\n",
      "12 -th layer prehooked\n",
      "16 -th layer prehooked\n",
      "21 -th layer prehooked\n",
      "25 -th layer prehooked\n",
      "29 -th layer prehooked\n",
      "34 -th layer prehooked\n",
      "38 -th layer prehooked\n",
      "41 -th layer prehooked\n",
      "46 -th layer prehooked\n",
      "50 -th layer prehooked\n",
      "54 -th layer prehooked\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "    def __call__(self, module, module_in):\n",
    "        self.outputs.append(module_in)\n",
    "    def clear(self):\n",
    "        self.outputs = []  \n",
    "        \n",
    "######### Save inputs from selected layer ##########\n",
    "save_output = SaveOutput()\n",
    "i = 0\n",
    "\n",
    "for layer in model.modules():\n",
    "    i = i+1\n",
    "    if isinstance(layer, QuantConv2d):\n",
    "        print(i,\"-th layer prehooked\")\n",
    "        layer.register_forward_pre_hook(save_output)             \n",
    "####################################################\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "images = images.to(device)\n",
    "out = model(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "spoken-worst",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.0000,  2.0000,  2.0000],\n",
      "          [-0.0000, -0.0000,  1.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-2.0000,  7.0000,  7.0000],\n",
      "          [ 3.0000,  7.0000,  2.0000],\n",
      "          [-1.0000, -5.0000, -7.0000]],\n",
      "\n",
      "         [[ 2.0000,  0.0000,  0.0000],\n",
      "          [ 3.0000, -2.0000, -3.0000],\n",
      "          [-2.0000, -5.0000,  7.0000]],\n",
      "\n",
      "         [[ 1.0000,  0.0000,  1.0000],\n",
      "          [-3.0000, -3.0000, -1.0000],\n",
      "          [-2.0000, -2.0000,  0.0000]],\n",
      "\n",
      "         [[ 7.0000,  5.0000,  1.0000],\n",
      "          [ 5.0000, -0.0000, -2.0000],\n",
      "          [ 2.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -3.0000, -3.0000],\n",
      "          [-0.0000,  1.0000, -0.0000]],\n",
      "\n",
      "         [[ 3.0000, -4.0000,  2.0000],\n",
      "          [ 0.0000, -2.0000,  0.0000],\n",
      "          [ 4.0000, -7.0000,  7.0000]],\n",
      "\n",
      "         [[-4.0000, -3.0000,  0.0000],\n",
      "          [-2.0000, -3.0000, -2.0000],\n",
      "          [-1.0000,  1.0000,  1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-7.0000, -3.0000,  0.0000],\n",
      "          [-3.0000,  0.0000,  0.0000],\n",
      "          [-1.0000,  0.0000,  2.0000]],\n",
      "\n",
      "         [[-1.0000,  2.0000,  2.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000],\n",
      "          [ 1.0000,  0.0000,  1.0000]],\n",
      "\n",
      "         [[-1.0000, -7.0000, -7.0000],\n",
      "          [-1.0000, -5.0000, -7.0000],\n",
      "          [ 0.0000, -5.0000, -5.0000]],\n",
      "\n",
      "         [[-7.0000,  0.0000,  1.0000],\n",
      "          [-1.0000, -2.0000,  0.0000],\n",
      "          [-1.0000, -0.0000,  1.0000]],\n",
      "\n",
      "         [[-1.0000,  1.0000,  1.0000],\n",
      "          [ 0.0000,  2.0000,  1.0000],\n",
      "          [ 0.0000,  1.0000,  1.0000]],\n",
      "\n",
      "         [[-1.0000, -2.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000],\n",
      "          [ 1.0000, -0.0000,  1.0000]],\n",
      "\n",
      "         [[-7.0000, -5.0000, -2.0000],\n",
      "          [-4.0000, -5.0000, -2.0000],\n",
      "          [-3.0000, -1.0000, -0.0000]],\n",
      "\n",
      "         [[-6.0000, -1.0000,  1.0000],\n",
      "          [-2.0000, -1.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-3.0000, -2.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000],\n",
      "          [-1.0000, -1.0000,  0.0000]],\n",
      "\n",
      "         [[-4.0000,  1.0000, -1.0000],\n",
      "          [-2.0000, -2.0000,  0.0000],\n",
      "          [ 2.0000,  4.0000, -7.0000]],\n",
      "\n",
      "         [[ 3.0000, -2.0000, -2.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [ 0.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[-5.0000, -0.0000,  1.0000],\n",
      "          [ 1.0000, -0.0000,  1.0000],\n",
      "          [ 0.0000, -0.0000,  1.0000]],\n",
      "\n",
      "         [[-7.0000, -1.0000, -6.0000],\n",
      "          [-0.0000,  4.0000,  2.0000],\n",
      "          [-2.0000,  3.0000,  0.0000]],\n",
      "\n",
      "         [[ 1.0000,  0.0000, -0.0000],\n",
      "          [ 1.0000, -0.0000,  1.0000],\n",
      "          [ 0.0000, -0.0000,  1.0000]],\n",
      "\n",
      "         [[ 5.0000,  7.0000, -7.0000],\n",
      "          [-1.0000,  7.0000, -7.0000],\n",
      "          [ 4.0000, -7.0000,  4.0000]],\n",
      "\n",
      "         [[-4.0000, -4.0000, -5.0000],\n",
      "          [-5.0000, -2.0000, -1.0000],\n",
      "          [ 1.0000,  2.0000,  2.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -1.0000, -0.0000],\n",
      "          [ 1.0000, -1.0000, -1.0000],\n",
      "          [ 2.0000,  0.0000, -1.0000]],\n",
      "\n",
      "         [[ 7.0000,  7.0000, -1.0000],\n",
      "          [ 7.0000,  7.0000,  2.0000],\n",
      "          [ 7.0000,  2.0000, -0.0000]],\n",
      "\n",
      "         [[ 1.0000, -1.0000, -4.0000],\n",
      "          [ 2.0000,  3.0000,  1.0000],\n",
      "          [ 1.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[-1.0000, -2.0000, -1.0000],\n",
      "          [-0.0000, -3.0000, -1.0000],\n",
      "          [ 0.0000, -3.0000, -2.0000]],\n",
      "\n",
      "         [[ 1.0000,  3.0000,  4.0000],\n",
      "          [-4.0000,  0.0000, -0.0000],\n",
      "          [-1.0000,  5.0000,  1.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000, -1.0000],\n",
      "          [-1.0000, -2.0000, -2.0000],\n",
      "          [-0.0000,  0.0000, -1.0000]],\n",
      "\n",
      "         [[ 6.0000,  1.0000, -0.0000],\n",
      "          [-3.0000, -3.0000, -7.0000],\n",
      "          [ 1.0000,  0.0000, -5.0000]],\n",
      "\n",
      "         [[-2.0000, -1.0000, -3.0000],\n",
      "          [-0.0000,  0.0000, -7.0000],\n",
      "          [ 1.0000, -3.0000, -7.0000]]],\n",
      "\n",
      "\n",
      "        [[[-1.0000, -1.0000,  1.0000],\n",
      "          [ 0.0000,  1.0000,  2.0000],\n",
      "          [ 1.0000,  1.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -1.0000,  1.0000],\n",
      "          [ 4.0000,  2.0000, -7.0000],\n",
      "          [ 0.0000,  2.0000, -6.0000]],\n",
      "\n",
      "         [[-3.0000, -5.0000, -1.0000],\n",
      "          [ 3.0000,  0.0000,  4.0000],\n",
      "          [-3.0000,  1.0000,  2.0000]],\n",
      "\n",
      "         [[-3.0000, -2.0000,  1.0000],\n",
      "          [ 0.0000,  0.0000,  1.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[-1.0000, -2.0000, -5.0000],\n",
      "          [-7.0000,  3.0000,  7.0000],\n",
      "          [-5.0000, -3.0000, -3.0000]],\n",
      "\n",
      "         [[ 1.0000, -1.0000, -1.0000],\n",
      "          [-0.0000,  0.0000, -0.0000],\n",
      "          [-1.0000, -1.0000, -2.0000]],\n",
      "\n",
      "         [[-4.0000,  7.0000,  1.0000],\n",
      "          [ 4.0000,  7.0000,  7.0000],\n",
      "          [-3.0000,  1.0000,  2.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -2.0000],\n",
      "          [ 1.0000,  4.0000,  1.0000],\n",
      "          [ 6.0000,  3.0000, -3.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0000, -0.0000,  0.0000],\n",
      "          [ 7.0000,  3.0000,  0.0000],\n",
      "          [ 5.0000,  3.0000,  0.0000]],\n",
      "\n",
      "         [[ 2.0000,  4.0000,  3.0000],\n",
      "          [ 2.0000,  2.0000,  1.0000],\n",
      "          [ 0.0000,  1.0000,  1.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -2.0000],\n",
      "          [-7.0000, -7.0000,  5.0000],\n",
      "          [-7.0000, -7.0000,  6.0000]],\n",
      "\n",
      "         [[-1.0000, -3.0000, -1.0000],\n",
      "          [-1.0000,  3.0000,  0.0000],\n",
      "          [ 2.0000,  1.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000, -1.0000, -1.0000],\n",
      "          [ 4.0000,  6.0000,  4.0000],\n",
      "          [ 3.0000,  3.0000,  3.0000]],\n",
      "\n",
      "         [[-0.0000, -1.0000, -1.0000],\n",
      "          [-3.0000, -2.0000,  4.0000],\n",
      "          [-4.0000, -2.0000, -0.0000]],\n",
      "\n",
      "         [[ 1.0000,  4.0000, -1.0000],\n",
      "          [-7.0000, -7.0000, -1.0000],\n",
      "          [-7.0000, -1.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0000, -2.0000],\n",
      "          [ 5.0000,  7.0000,  1.0000],\n",
      "          [ 5.0000,  5.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 4.0000,  2.0000,  1.0000],\n",
      "          [ 3.0000,  3.0000,  4.0000],\n",
      "          [ 2.0000,  4.0000,  5.0000]],\n",
      "\n",
      "         [[ 1.0000,  5.0000,  4.0000],\n",
      "          [-2.0000, -1.0000,  2.0000],\n",
      "          [-1.0000, -3.0000,  7.0000]],\n",
      "\n",
      "         [[ 7.0000,  1.0000,  2.0000],\n",
      "          [ 3.0000, -3.0000, -6.0000],\n",
      "          [ 1.0000,  3.0000, -3.0000]],\n",
      "\n",
      "         [[ 1.0000,  2.0000,  2.0000],\n",
      "          [-1.0000,  4.0000,  4.0000],\n",
      "          [-0.0000,  5.0000,  1.0000]],\n",
      "\n",
      "         [[ 3.0000,  1.0000,  2.0000],\n",
      "          [-0.0000,  7.0000,  4.0000],\n",
      "          [-0.0000,  4.0000,  6.0000]],\n",
      "\n",
      "         [[ 2.0000,  1.0000,  1.0000],\n",
      "          [ 0.0000,  0.0000,  2.0000],\n",
      "          [ 1.0000, -3.0000, -3.0000]],\n",
      "\n",
      "         [[ 0.0000,  7.0000,  4.0000],\n",
      "          [-7.0000,  7.0000,  4.0000],\n",
      "          [-7.0000, -1.0000, -7.0000]],\n",
      "\n",
      "         [[ 6.0000,  7.0000, -1.0000],\n",
      "          [ 0.0000, -2.0000, -1.0000],\n",
      "          [ 6.0000,  5.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000, -0.0000, -0.0000],\n",
      "          [ 1.0000,  0.0000,  1.0000],\n",
      "          [ 2.0000,  1.0000,  1.0000]],\n",
      "\n",
      "         [[-0.0000,  0.0000,  1.0000],\n",
      "          [ 2.0000,  7.0000,  0.0000],\n",
      "          [ 1.0000,  7.0000,  1.0000]],\n",
      "\n",
      "         [[-0.0000, -1.0000, -3.0000],\n",
      "          [-1.0000,  1.0000, -3.0000],\n",
      "          [-1.0000,  1.0000, -3.0000]],\n",
      "\n",
      "         [[-2.0000, -1.0000,  0.0000],\n",
      "          [ 0.0000, -1.0000, -0.0000],\n",
      "          [ 1.0000, -1.0000,  1.0000]],\n",
      "\n",
      "         [[-0.0000,  1.0000,  0.0000],\n",
      "          [-1.0000,  0.0000,  1.0000],\n",
      "          [ 2.0000,  1.0000,  1.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000, -1.0000],\n",
      "          [-0.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[-4.0000, -3.0000, -4.0000],\n",
      "          [-0.0000,  1.0000, -2.0000],\n",
      "          [-1.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[-1.0000, -2.0000, -1.0000],\n",
      "          [-1.0000, -1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  1.0000]]]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "w_bit = 4\n",
    "weight_q = model.features[27].weight_q # quantized value is stored during the training\n",
    "w_alpha = model.features[27].weight_quant.wgt_alpha   # alpha is defined in your model already. bring it out here\n",
    "w_delta = w_alpha/(2**(w_bit-1)-1)   # delta can be calculated by using alpha and w_bit\n",
    "weight_int = weight_q/w_delta # w_int can be calculated by weight_q and w_delta\n",
    "print(weight_int) # you should see clean integer numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "interior-oxygen",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 1.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  1.0000,  1.0000,  3.0000],\n",
      "          [ 3.0000,  6.0000,  5.0000,  6.0000],\n",
      "          [ 2.0000,  3.0000,  3.0000,  3.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[10.0000, 14.0000,  6.0000,  1.0000],\n",
      "          [ 7.0000,  6.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 2.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  1.0000,  3.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 8.0000, 15.0000, 15.0000, 13.0000],\n",
      "          [ 9.0000, 15.0000, 15.0000, 10.0000],\n",
      "          [ 5.0000,  5.0000,  5.0000,  2.0000],\n",
      "          [ 5.0000,  7.0000,  7.0000,  3.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  4.0000,  8.0000],\n",
      "          [ 0.0000,  0.0000,  5.0000,  5.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 5.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 1.0000,  3.0000,  8.0000,  8.0000],\n",
      "          [ 0.0000,  0.0000,  4.0000,  2.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  4.0000,  5.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 6.0000,  9.0000,  7.0000,  5.0000],\n",
      "          [ 8.0000, 10.0000,  7.0000,  4.0000],\n",
      "          [ 5.0000,  4.0000,  4.0000,  3.0000],\n",
      "          [ 2.0000,  3.0000,  5.0000,  4.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  3.0000],\n",
      "          [ 0.0000,  0.0000,  3.0000,  4.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 6.0000,  2.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  2.0000,  3.0000],\n",
      "          [ 0.0000,  0.0000,  2.0000,  2.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  1.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  2.0000,  7.0000,  5.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  4.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  5.0000]],\n",
      "\n",
      "         [[ 3.0000,  2.0000,  0.0000,  0.0000],\n",
      "          [ 3.0000,  3.0000,  0.0000,  0.0000],\n",
      "          [ 2.0000,  5.0000,  5.0000,  5.0000],\n",
      "          [ 8.0000, 10.0000,  6.0000,  4.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  1.0000,  3.0000,  5.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  4.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000,  1.0000],\n",
      "          [ 4.0000,  5.0000,  5.0000,  5.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  1.0000,  1.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  3.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 1.0000,  0.0000,  0.0000,  2.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  1.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  3.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 4.0000,  9.0000,  9.0000,  8.0000],\n",
      "          [ 3.0000,  9.0000,  6.0000,  5.0000],\n",
      "          [ 1.0000,  4.0000,  0.0000,  2.0000],\n",
      "          [ 1.0000,  2.0000,  0.0000,  2.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  2.0000,  4.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  1.0000,  3.0000,  3.0000],\n",
      "          [ 0.0000,  0.0000,  1.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  1.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  2.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  1.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  3.0000,  2.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  1.0000,  0.0000],\n",
      "          [ 0.0000,  1.0000,  1.0000,  3.0000],\n",
      "          [ 0.0000,  4.0000,  5.0000,  5.0000],\n",
      "          [ 1.0000,  4.0000,  4.0000,  4.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 8.0000,  9.0000,  5.0000,  2.0000],\n",
      "          [ 8.0000,  4.0000,  4.0000,  0.0000],\n",
      "          [ 4.0000,  0.0000,  2.0000,  0.0000],\n",
      "          [ 5.0000,  1.0000,  3.0000,  1.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]]]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x_bit = 4    \n",
    "x = save_output.outputs[8][0]  # input of the 2nd conv layer\n",
    "x_alpha  = model.features[27].act_alpha\n",
    "x_delta = x_alpha/(2**x_bit-1)\n",
    "\n",
    "act_quant_fn = act_quantization(x_bit) # define the quantization function\n",
    "x_q = act_quant_fn(x, x_alpha)         # create the quantized value for x\n",
    "\n",
    "x_int = x_q/x_delta\n",
    "print(x_int) # you should see clean integer numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "designed-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### input floating number / weight quantized version\n",
    "\n",
    "conv_ref = torch.nn.Conv2d(in_channels = 8, out_channels=8, kernel_size = 3, padding=1, bias = False)\n",
    "conv_ref.weight = torch.nn.parameter.Parameter(weight_int)\n",
    "output_int = conv_ref(x_int)\n",
    "\n",
    "output_recovered = output_int*x_delta*w_delta\n",
    "relu = nn.ReLU(inplace=True)\n",
    "relu_output_recovered = relu(output_recovered)\n",
    "\n",
    "output_ref = save_output.outputs[9][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "157dffd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7059e-07, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "difference = abs( output_ref - relu_output_recovered )\n",
    "print(difference.mean())  ## It should be small, e.g.,2.3 in my trainned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-whole",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-significance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-witch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-barbados",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-serbia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "907cbb87cc825d52daa26d94a4f3471be4a7efbfbc56d778ed32b1cc3c9bdcfc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
