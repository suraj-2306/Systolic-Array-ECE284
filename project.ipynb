{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "radical-fifty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building model...\n",
      "VGG_quant(\n",
      "  (features): Sequential(\n",
      "    (0): QuantConv2d(\n",
      "      3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): QuantConv2d(\n",
      "      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): QuantConv2d(\n",
      "      64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): QuantConv2d(\n",
      "      128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): QuantConv2d(\n",
      "      128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): QuantConv2d(\n",
      "      256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (25): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): QuantConv2d(\n",
      "      8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (28): ReLU(inplace=True)\n",
      "    (29): QuantConv2d(\n",
      "      8, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (30): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (33): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (38): ReLU(inplace=True)\n",
      "    (39): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (40): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (41): ReLU(inplace=True)\n",
      "    (42): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (43): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "     \n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import *\n",
    "\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('=> Building model...')\n",
    "    \n",
    "    \n",
    "    \n",
    "batch_size = 128\n",
    "model_name = \"VGG16_project\"\n",
    "model = VGG16_project()\n",
    "\n",
    "print(model)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "            \n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "         \n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "        \n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))  \n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "#all_params = checkpoint['state_dict']\n",
    "#model.load_state_dict(all_params, strict=False)\n",
    "#criterion = nn.CrossEntropyLoss().cuda()\n",
    "#validate(testloader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2107ceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    adjust_list = [80, 90]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1   \n",
    "#             param_group['momentum'] = param_group['momentum']*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "junior-reminder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/391]\tTime 1.870 (1.870)\tData 0.323 (0.323)\tLoss 2.4934 (2.4934)\tPrec 7.031% (7.031%)\n",
      "Epoch: [0][100/391]\tTime 0.057 (0.064)\tData 0.006 (0.007)\tLoss 2.2796 (4.2597)\tPrec 14.844% (11.982%)\n",
      "Epoch: [0][200/391]\tTime 0.045 (0.056)\tData 0.003 (0.006)\tLoss 2.2888 (3.2490)\tPrec 10.938% (13.448%)\n",
      "Epoch: [0][300/391]\tTime 0.045 (0.052)\tData 0.003 (0.005)\tLoss 2.1486 (2.8945)\tPrec 16.406% (14.221%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.348 (0.348)\tLoss 2.0962 (2.0962)\tPrec 25.000% (25.000%)\n",
      " * Prec 19.110% \n",
      "best acc: 19.110000\n",
      "Epoch: [1][0/391]\tTime 0.458 (0.458)\tData 0.414 (0.414)\tLoss 2.2139 (2.2139)\tPrec 17.969% (17.969%)\n",
      "Epoch: [1][100/391]\tTime 0.046 (0.050)\tData 0.002 (0.008)\tLoss 2.1286 (2.1293)\tPrec 19.531% (17.907%)\n",
      "Epoch: [1][200/391]\tTime 0.048 (0.048)\tData 0.003 (0.005)\tLoss 2.0611 (2.1183)\tPrec 16.406% (17.833%)\n",
      "Epoch: [1][300/391]\tTime 0.044 (0.047)\tData 0.002 (0.004)\tLoss 2.0695 (2.0998)\tPrec 22.656% (18.462%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.401 (0.401)\tLoss 1.9406 (1.9406)\tPrec 28.125% (28.125%)\n",
      " * Prec 22.550% \n",
      "best acc: 22.550000\n",
      "Epoch: [2][0/391]\tTime 0.348 (0.348)\tData 0.312 (0.312)\tLoss 1.9394 (1.9394)\tPrec 24.219% (24.219%)\n",
      "Epoch: [2][100/391]\tTime 0.044 (0.050)\tData 0.005 (0.008)\tLoss 1.8990 (1.9674)\tPrec 28.125% (23.345%)\n",
      "Epoch: [2][200/391]\tTime 0.045 (0.048)\tData 0.002 (0.006)\tLoss 1.9986 (1.9473)\tPrec 25.781% (23.962%)\n",
      "Epoch: [2][300/391]\tTime 0.056 (0.047)\tData 0.013 (0.005)\tLoss 1.8542 (1.9351)\tPrec 32.812% (24.395%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.295 (0.295)\tLoss 1.8615 (1.8615)\tPrec 28.906% (28.906%)\n",
      " * Prec 27.820% \n",
      "best acc: 27.820000\n",
      "Epoch: [3][0/391]\tTime 0.404 (0.404)\tData 0.368 (0.368)\tLoss 1.9589 (1.9589)\tPrec 25.000% (25.000%)\n",
      "Epoch: [3][100/391]\tTime 0.047 (0.049)\tData 0.004 (0.007)\tLoss 1.8894 (1.8675)\tPrec 24.219% (27.684%)\n",
      "Epoch: [3][200/391]\tTime 0.045 (0.048)\tData 0.003 (0.005)\tLoss 2.0375 (1.8603)\tPrec 26.562% (27.826%)\n",
      "Epoch: [3][300/391]\tTime 0.045 (0.047)\tData 0.001 (0.005)\tLoss 1.9141 (1.8550)\tPrec 31.250% (27.969%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.351 (0.351)\tLoss 1.7488 (1.7488)\tPrec 28.125% (28.125%)\n",
      " * Prec 31.040% \n",
      "best acc: 31.040000\n",
      "Epoch: [4][0/391]\tTime 0.318 (0.318)\tData 0.282 (0.282)\tLoss 1.7597 (1.7597)\tPrec 23.438% (23.438%)\n",
      "Epoch: [4][100/391]\tTime 0.048 (0.049)\tData 0.004 (0.006)\tLoss 1.6382 (1.7885)\tPrec 39.844% (30.964%)\n",
      "Epoch: [4][200/391]\tTime 0.041 (0.047)\tData 0.002 (0.005)\tLoss 1.6363 (1.7811)\tPrec 32.031% (31.071%)\n",
      "Epoch: [4][300/391]\tTime 0.048 (0.047)\tData 0.013 (0.004)\tLoss 1.7394 (1.7624)\tPrec 42.188% (31.990%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.320 (0.320)\tLoss 1.7737 (1.7737)\tPrec 35.938% (35.938%)\n",
      " * Prec 33.280% \n",
      "best acc: 33.280000\n",
      "Epoch: [5][0/391]\tTime 0.416 (0.416)\tData 0.375 (0.375)\tLoss 1.6605 (1.6605)\tPrec 31.250% (31.250%)\n",
      "Epoch: [5][100/391]\tTime 0.044 (0.050)\tData 0.002 (0.007)\tLoss 1.7970 (1.6815)\tPrec 38.281% (35.210%)\n",
      "Epoch: [5][200/391]\tTime 0.046 (0.048)\tData 0.001 (0.005)\tLoss 1.6345 (1.6799)\tPrec 39.062% (35.316%)\n",
      "Epoch: [5][300/391]\tTime 0.044 (0.047)\tData 0.006 (0.005)\tLoss 1.6366 (1.6673)\tPrec 32.812% (35.706%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.305 (0.305)\tLoss 1.6097 (1.6097)\tPrec 41.406% (41.406%)\n",
      " * Prec 38.160% \n",
      "best acc: 38.160000\n",
      "Epoch: [6][0/391]\tTime 0.452 (0.452)\tData 0.410 (0.410)\tLoss 1.7503 (1.7503)\tPrec 33.594% (33.594%)\n",
      "Epoch: [6][100/391]\tTime 0.048 (0.050)\tData 0.001 (0.008)\tLoss 1.5793 (1.6092)\tPrec 35.938% (38.614%)\n",
      "Epoch: [6][200/391]\tTime 0.045 (0.050)\tData 0.002 (0.007)\tLoss 1.5186 (1.5877)\tPrec 39.844% (39.319%)\n",
      "Epoch: [6][300/391]\tTime 0.051 (0.049)\tData 0.002 (0.006)\tLoss 1.4278 (1.5778)\tPrec 42.969% (39.890%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.304 (0.304)\tLoss 1.5879 (1.5879)\tPrec 39.844% (39.844%)\n",
      " * Prec 40.990% \n",
      "best acc: 40.990000\n",
      "Epoch: [7][0/391]\tTime 0.477 (0.477)\tData 0.436 (0.436)\tLoss 1.5594 (1.5594)\tPrec 37.500% (37.500%)\n",
      "Epoch: [7][100/391]\tTime 0.045 (0.052)\tData 0.003 (0.010)\tLoss 1.5512 (1.5197)\tPrec 38.281% (42.466%)\n",
      "Epoch: [7][200/391]\tTime 0.050 (0.049)\tData 0.002 (0.007)\tLoss 1.4985 (1.5077)\tPrec 38.281% (43.085%)\n",
      "Epoch: [7][300/391]\tTime 0.045 (0.049)\tData 0.002 (0.007)\tLoss 1.3901 (1.4862)\tPrec 48.438% (43.986%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.355 (0.355)\tLoss 1.4178 (1.4178)\tPrec 42.969% (42.969%)\n",
      " * Prec 44.910% \n",
      "best acc: 44.910000\n",
      "Epoch: [8][0/391]\tTime 0.474 (0.474)\tData 0.433 (0.433)\tLoss 1.3085 (1.3085)\tPrec 50.781% (50.781%)\n",
      "Epoch: [8][100/391]\tTime 0.044 (0.050)\tData 0.001 (0.007)\tLoss 1.5065 (1.3975)\tPrec 39.844% (47.710%)\n",
      "Epoch: [8][200/391]\tTime 0.045 (0.048)\tData 0.003 (0.005)\tLoss 1.3965 (1.3970)\tPrec 48.438% (47.656%)\n",
      "Epoch: [8][300/391]\tTime 0.051 (0.047)\tData 0.003 (0.004)\tLoss 1.3256 (1.3880)\tPrec 50.781% (48.077%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.322 (0.322)\tLoss 1.3382 (1.3382)\tPrec 50.781% (50.781%)\n",
      " * Prec 45.680% \n",
      "best acc: 45.680000\n",
      "Epoch: [9][0/391]\tTime 0.464 (0.464)\tData 0.427 (0.427)\tLoss 1.2729 (1.2729)\tPrec 49.219% (49.219%)\n",
      "Epoch: [9][100/391]\tTime 0.048 (0.050)\tData 0.003 (0.007)\tLoss 1.4218 (1.3145)\tPrec 42.188% (50.882%)\n",
      "Epoch: [9][200/391]\tTime 0.053 (0.049)\tData 0.006 (0.006)\tLoss 1.4776 (1.3088)\tPrec 45.312% (51.403%)\n",
      "Epoch: [9][300/391]\tTime 0.045 (0.048)\tData 0.002 (0.005)\tLoss 1.3739 (1.3005)\tPrec 46.094% (51.757%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.320 (0.320)\tLoss 1.2970 (1.2970)\tPrec 49.219% (49.219%)\n",
      " * Prec 51.630% \n",
      "best acc: 51.630000\n",
      "Epoch: [10][0/391]\tTime 0.376 (0.376)\tData 0.340 (0.340)\tLoss 1.1196 (1.1196)\tPrec 60.938% (60.938%)\n",
      "Epoch: [10][100/391]\tTime 0.045 (0.051)\tData 0.003 (0.010)\tLoss 1.3183 (1.2120)\tPrec 48.438% (55.763%)\n",
      "Epoch: [10][200/391]\tTime 0.048 (0.050)\tData 0.012 (0.009)\tLoss 1.2375 (1.2058)\tPrec 55.469% (56.122%)\n",
      "Epoch: [10][300/391]\tTime 0.045 (0.049)\tData 0.001 (0.007)\tLoss 1.0458 (1.1835)\tPrec 60.938% (56.844%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.368 (0.368)\tLoss 1.3326 (1.3326)\tPrec 51.562% (51.562%)\n",
      " * Prec 51.860% \n",
      "best acc: 51.860000\n",
      "Epoch: [11][0/391]\tTime 0.372 (0.372)\tData 0.325 (0.325)\tLoss 1.3726 (1.3726)\tPrec 53.906% (53.906%)\n",
      "Epoch: [11][100/391]\tTime 0.053 (0.050)\tData 0.013 (0.009)\tLoss 1.0047 (1.1014)\tPrec 57.812% (60.466%)\n",
      "Epoch: [11][200/391]\tTime 0.045 (0.049)\tData 0.002 (0.007)\tLoss 1.0552 (1.0824)\tPrec 61.719% (60.953%)\n",
      "Epoch: [11][300/391]\tTime 0.047 (0.049)\tData 0.001 (0.006)\tLoss 1.1243 (1.0695)\tPrec 58.594% (61.503%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.329 (0.329)\tLoss 0.9262 (0.9262)\tPrec 62.500% (62.500%)\n",
      " * Prec 63.140% \n",
      "best acc: 63.140000\n",
      "Epoch: [12][0/391]\tTime 0.430 (0.430)\tData 0.388 (0.388)\tLoss 1.0940 (1.0940)\tPrec 60.938% (60.938%)\n",
      "Epoch: [12][100/391]\tTime 0.045 (0.051)\tData 0.001 (0.008)\tLoss 1.0573 (1.0013)\tPrec 62.500% (64.511%)\n",
      "Epoch: [12][200/391]\tTime 0.047 (0.050)\tData 0.003 (0.007)\tLoss 0.8543 (0.9887)\tPrec 74.219% (64.859%)\n",
      "Epoch: [12][300/391]\tTime 0.050 (0.048)\tData 0.002 (0.006)\tLoss 0.8802 (0.9901)\tPrec 71.094% (64.831%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.304 (0.304)\tLoss 1.1475 (1.1475)\tPrec 60.156% (60.156%)\n",
      " * Prec 54.800% \n",
      "best acc: 63.140000\n",
      "Epoch: [13][0/391]\tTime 0.329 (0.329)\tData 0.288 (0.288)\tLoss 1.1036 (1.1036)\tPrec 62.500% (62.500%)\n",
      "Epoch: [13][100/391]\tTime 0.052 (0.050)\tData 0.003 (0.008)\tLoss 1.0198 (0.9225)\tPrec 66.406% (67.474%)\n",
      "Epoch: [13][200/391]\tTime 0.045 (0.049)\tData 0.003 (0.006)\tLoss 0.9177 (0.9208)\tPrec 64.062% (67.444%)\n",
      "Epoch: [13][300/391]\tTime 0.046 (0.048)\tData 0.002 (0.005)\tLoss 0.8666 (0.9169)\tPrec 71.094% (67.543%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.296 (0.296)\tLoss 0.8269 (0.8269)\tPrec 73.438% (73.438%)\n",
      " * Prec 67.270% \n",
      "best acc: 67.270000\n",
      "Epoch: [14][0/391]\tTime 0.381 (0.381)\tData 0.341 (0.341)\tLoss 0.8006 (0.8006)\tPrec 72.656% (72.656%)\n",
      "Epoch: [14][100/391]\tTime 0.047 (0.051)\tData 0.011 (0.009)\tLoss 0.8814 (0.8562)\tPrec 66.406% (69.926%)\n",
      "Epoch: [14][200/391]\tTime 0.049 (0.048)\tData 0.004 (0.006)\tLoss 0.8621 (0.8555)\tPrec 71.875% (70.103%)\n",
      "Epoch: [14][300/391]\tTime 0.045 (0.047)\tData 0.005 (0.005)\tLoss 0.8621 (0.8531)\tPrec 73.438% (70.323%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.319 (0.319)\tLoss 0.8644 (0.8644)\tPrec 67.969% (67.969%)\n",
      " * Prec 69.520% \n",
      "best acc: 69.520000\n",
      "Epoch: [15][0/391]\tTime 0.358 (0.358)\tData 0.323 (0.323)\tLoss 0.6859 (0.6859)\tPrec 79.688% (79.688%)\n",
      "Epoch: [15][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.009)\tLoss 0.9542 (0.8054)\tPrec 67.969% (72.316%)\n",
      "Epoch: [15][200/391]\tTime 0.047 (0.048)\tData 0.001 (0.007)\tLoss 0.8126 (0.8104)\tPrec 73.438% (71.856%)\n",
      "Epoch: [15][300/391]\tTime 0.054 (0.048)\tData 0.022 (0.006)\tLoss 0.9175 (0.8114)\tPrec 70.312% (71.994%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.302 (0.302)\tLoss 0.7830 (0.7830)\tPrec 68.750% (68.750%)\n",
      " * Prec 70.800% \n",
      "best acc: 70.800000\n",
      "Epoch: [16][0/391]\tTime 0.448 (0.448)\tData 0.391 (0.391)\tLoss 0.5785 (0.5785)\tPrec 82.812% (82.812%)\n",
      "Epoch: [16][100/391]\tTime 0.044 (0.051)\tData 0.003 (0.008)\tLoss 0.6820 (0.7623)\tPrec 78.906% (74.018%)\n",
      "Epoch: [16][200/391]\tTime 0.045 (0.048)\tData 0.003 (0.005)\tLoss 0.6824 (0.7578)\tPrec 78.125% (74.122%)\n",
      "Epoch: [16][300/391]\tTime 0.044 (0.047)\tData 0.002 (0.004)\tLoss 0.8359 (0.7517)\tPrec 72.656% (74.317%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.337 (0.337)\tLoss 0.9035 (0.9035)\tPrec 71.094% (71.094%)\n",
      " * Prec 69.200% \n",
      "best acc: 70.800000\n",
      "Epoch: [17][0/391]\tTime 0.454 (0.454)\tData 0.418 (0.418)\tLoss 0.6370 (0.6370)\tPrec 78.125% (78.125%)\n",
      "Epoch: [17][100/391]\tTime 0.046 (0.049)\tData 0.002 (0.007)\tLoss 0.6880 (0.7082)\tPrec 79.688% (76.160%)\n",
      "Epoch: [17][200/391]\tTime 0.043 (0.047)\tData 0.002 (0.005)\tLoss 0.6386 (0.7115)\tPrec 72.656% (75.816%)\n",
      "Epoch: [17][300/391]\tTime 0.042 (0.047)\tData 0.007 (0.004)\tLoss 0.6181 (0.7092)\tPrec 79.688% (75.838%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.293 (0.293)\tLoss 0.7931 (0.7931)\tPrec 71.875% (71.875%)\n",
      " * Prec 73.890% \n",
      "best acc: 73.890000\n",
      "Epoch: [18][0/391]\tTime 0.478 (0.478)\tData 0.442 (0.442)\tLoss 0.7955 (0.7955)\tPrec 68.750% (68.750%)\n",
      "Epoch: [18][100/391]\tTime 0.041 (0.051)\tData 0.003 (0.008)\tLoss 0.7383 (0.6733)\tPrec 72.656% (77.127%)\n",
      "Epoch: [18][200/391]\tTime 0.054 (0.050)\tData 0.023 (0.007)\tLoss 0.7485 (0.6725)\tPrec 75.781% (77.161%)\n",
      "Epoch: [18][300/391]\tTime 0.050 (0.048)\tData 0.013 (0.006)\tLoss 0.7027 (0.6722)\tPrec 77.344% (77.144%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.312 (0.312)\tLoss 0.8274 (0.8274)\tPrec 73.438% (73.438%)\n",
      " * Prec 72.390% \n",
      "best acc: 73.890000\n",
      "Epoch: [19][0/391]\tTime 0.367 (0.367)\tData 0.321 (0.321)\tLoss 0.7934 (0.7934)\tPrec 75.781% (75.781%)\n",
      "Epoch: [19][100/391]\tTime 0.047 (0.049)\tData 0.002 (0.006)\tLoss 0.6728 (0.6510)\tPrec 77.344% (77.839%)\n",
      "Epoch: [19][200/391]\tTime 0.053 (0.048)\tData 0.003 (0.005)\tLoss 0.6178 (0.6402)\tPrec 78.125% (78.385%)\n",
      "Epoch: [19][300/391]\tTime 0.049 (0.048)\tData 0.002 (0.005)\tLoss 0.8093 (0.6367)\tPrec 73.438% (78.603%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.247 (0.247)\tLoss 0.6718 (0.6718)\tPrec 81.250% (81.250%)\n",
      " * Prec 76.210% \n",
      "best acc: 76.210000\n",
      "Epoch: [20][0/391]\tTime 0.412 (0.412)\tData 0.367 (0.367)\tLoss 0.5811 (0.5811)\tPrec 82.031% (82.031%)\n",
      "Epoch: [20][100/391]\tTime 0.056 (0.049)\tData 0.007 (0.007)\tLoss 0.6678 (0.6172)\tPrec 79.688% (79.332%)\n",
      "Epoch: [20][200/391]\tTime 0.052 (0.048)\tData 0.002 (0.006)\tLoss 0.6154 (0.6139)\tPrec 80.469% (79.361%)\n",
      "Epoch: [20][300/391]\tTime 0.045 (0.047)\tData 0.003 (0.005)\tLoss 0.5649 (0.6166)\tPrec 78.906% (79.171%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.321 (0.321)\tLoss 0.5484 (0.5484)\tPrec 82.031% (82.031%)\n",
      " * Prec 77.550% \n",
      "best acc: 77.550000\n",
      "Epoch: [21][0/391]\tTime 0.383 (0.383)\tData 0.329 (0.329)\tLoss 0.4938 (0.4938)\tPrec 82.031% (82.031%)\n",
      "Epoch: [21][100/391]\tTime 0.046 (0.049)\tData 0.005 (0.007)\tLoss 0.6216 (0.5744)\tPrec 77.344% (80.554%)\n",
      "Epoch: [21][200/391]\tTime 0.053 (0.047)\tData 0.006 (0.006)\tLoss 0.7633 (0.5748)\tPrec 73.438% (80.492%)\n",
      "Epoch: [21][300/391]\tTime 0.047 (0.047)\tData 0.003 (0.005)\tLoss 0.5274 (0.5746)\tPrec 84.375% (80.624%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.368 (0.368)\tLoss 0.6196 (0.6196)\tPrec 79.688% (79.688%)\n",
      " * Prec 77.720% \n",
      "best acc: 77.720000\n",
      "Epoch: [22][0/391]\tTime 0.377 (0.377)\tData 0.330 (0.330)\tLoss 0.6354 (0.6354)\tPrec 80.469% (80.469%)\n",
      "Epoch: [22][100/391]\tTime 0.045 (0.049)\tData 0.003 (0.007)\tLoss 0.6535 (0.5467)\tPrec 77.344% (81.675%)\n",
      "Epoch: [22][200/391]\tTime 0.045 (0.047)\tData 0.003 (0.005)\tLoss 0.5573 (0.5497)\tPrec 78.906% (81.573%)\n",
      "Epoch: [22][300/391]\tTime 0.046 (0.047)\tData 0.002 (0.004)\tLoss 0.5010 (0.5530)\tPrec 83.594% (81.439%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.295 (0.295)\tLoss 0.6406 (0.6406)\tPrec 75.781% (75.781%)\n",
      " * Prec 77.190% \n",
      "best acc: 77.720000\n",
      "Epoch: [23][0/391]\tTime 0.362 (0.362)\tData 0.321 (0.321)\tLoss 0.5082 (0.5082)\tPrec 84.375% (84.375%)\n",
      "Epoch: [23][100/391]\tTime 0.046 (0.049)\tData 0.002 (0.007)\tLoss 0.4786 (0.5259)\tPrec 82.812% (82.163%)\n",
      "Epoch: [23][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.005)\tLoss 0.5791 (0.5342)\tPrec 82.031% (82.023%)\n",
      "Epoch: [23][300/391]\tTime 0.044 (0.047)\tData 0.002 (0.004)\tLoss 0.8102 (0.5325)\tPrec 74.219% (82.023%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.245 (0.245)\tLoss 0.5503 (0.5503)\tPrec 81.250% (81.250%)\n",
      " * Prec 80.520% \n",
      "best acc: 80.520000\n",
      "Epoch: [24][0/391]\tTime 0.389 (0.389)\tData 0.353 (0.353)\tLoss 0.4034 (0.4034)\tPrec 85.938% (85.938%)\n",
      "Epoch: [24][100/391]\tTime 0.059 (0.051)\tData 0.023 (0.009)\tLoss 0.5792 (0.5086)\tPrec 80.469% (83.238%)\n",
      "Epoch: [24][200/391]\tTime 0.050 (0.048)\tData 0.003 (0.006)\tLoss 0.5178 (0.4985)\tPrec 83.594% (83.349%)\n",
      "Epoch: [24][300/391]\tTime 0.045 (0.048)\tData 0.003 (0.005)\tLoss 0.4447 (0.5037)\tPrec 89.844% (83.269%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.367 (0.367)\tLoss 0.5090 (0.5090)\tPrec 82.812% (82.812%)\n",
      " * Prec 80.490% \n",
      "best acc: 80.520000\n",
      "Epoch: [25][0/391]\tTime 0.336 (0.336)\tData 0.294 (0.294)\tLoss 0.4971 (0.4971)\tPrec 82.812% (82.812%)\n",
      "Epoch: [25][100/391]\tTime 0.047 (0.049)\tData 0.002 (0.005)\tLoss 0.5541 (0.4970)\tPrec 81.250% (83.493%)\n",
      "Epoch: [25][200/391]\tTime 0.056 (0.048)\tData 0.023 (0.005)\tLoss 0.5274 (0.4872)\tPrec 82.031% (83.710%)\n",
      "Epoch: [25][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.6331 (0.4890)\tPrec 78.125% (83.578%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.272 (0.272)\tLoss 0.5015 (0.5015)\tPrec 82.031% (82.031%)\n",
      " * Prec 80.230% \n",
      "best acc: 80.520000\n",
      "Epoch: [26][0/391]\tTime 0.322 (0.322)\tData 0.284 (0.284)\tLoss 0.4677 (0.4677)\tPrec 83.594% (83.594%)\n",
      "Epoch: [26][100/391]\tTime 0.049 (0.048)\tData 0.002 (0.005)\tLoss 0.4158 (0.4642)\tPrec 85.938% (84.452%)\n",
      "Epoch: [26][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.3580 (0.4637)\tPrec 88.281% (84.558%)\n",
      "Epoch: [26][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.4067 (0.4601)\tPrec 87.500% (84.635%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.283 (0.283)\tLoss 0.4034 (0.4034)\tPrec 86.719% (86.719%)\n",
      " * Prec 82.630% \n",
      "best acc: 82.630000\n",
      "Epoch: [27][0/391]\tTime 0.549 (0.549)\tData 0.510 (0.510)\tLoss 0.3969 (0.3969)\tPrec 85.938% (85.938%)\n",
      "Epoch: [27][100/391]\tTime 0.045 (0.050)\tData 0.005 (0.008)\tLoss 0.4066 (0.4289)\tPrec 87.500% (85.419%)\n",
      "Epoch: [27][200/391]\tTime 0.046 (0.049)\tData 0.001 (0.006)\tLoss 0.4253 (0.4433)\tPrec 83.594% (84.989%)\n",
      "Epoch: [27][300/391]\tTime 0.047 (0.048)\tData 0.002 (0.005)\tLoss 0.4704 (0.4455)\tPrec 84.375% (85.032%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.286 (0.286)\tLoss 0.4491 (0.4491)\tPrec 84.375% (84.375%)\n",
      " * Prec 82.130% \n",
      "best acc: 82.630000\n",
      "Epoch: [28][0/391]\tTime 0.360 (0.360)\tData 0.322 (0.322)\tLoss 0.5799 (0.5799)\tPrec 79.688% (79.688%)\n",
      "Epoch: [28][100/391]\tTime 0.045 (0.050)\tData 0.001 (0.007)\tLoss 0.3059 (0.4220)\tPrec 89.844% (85.984%)\n",
      "Epoch: [28][200/391]\tTime 0.045 (0.048)\tData 0.003 (0.005)\tLoss 0.3582 (0.4258)\tPrec 88.281% (85.731%)\n",
      "Epoch: [28][300/391]\tTime 0.046 (0.047)\tData 0.003 (0.005)\tLoss 0.3798 (0.4280)\tPrec 87.500% (85.771%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.336 (0.336)\tLoss 0.4529 (0.4529)\tPrec 84.375% (84.375%)\n",
      " * Prec 81.770% \n",
      "best acc: 82.630000\n",
      "Epoch: [29][0/391]\tTime 0.401 (0.401)\tData 0.363 (0.363)\tLoss 0.3671 (0.3671)\tPrec 86.719% (86.719%)\n",
      "Epoch: [29][100/391]\tTime 0.045 (0.051)\tData 0.003 (0.008)\tLoss 0.4260 (0.4139)\tPrec 81.250% (86.115%)\n",
      "Epoch: [29][200/391]\tTime 0.067 (0.049)\tData 0.002 (0.007)\tLoss 0.3898 (0.4121)\tPrec 87.500% (86.210%)\n",
      "Epoch: [29][300/391]\tTime 0.045 (0.048)\tData 0.003 (0.006)\tLoss 0.3993 (0.4120)\tPrec 88.281% (86.161%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.333 (0.333)\tLoss 0.5973 (0.5973)\tPrec 81.250% (81.250%)\n",
      " * Prec 81.400% \n",
      "best acc: 82.630000\n",
      "Epoch: [30][0/391]\tTime 0.389 (0.389)\tData 0.353 (0.353)\tLoss 0.3790 (0.3790)\tPrec 89.844% (89.844%)\n",
      "Epoch: [30][100/391]\tTime 0.045 (0.050)\tData 0.003 (0.007)\tLoss 0.4002 (0.3982)\tPrec 83.594% (86.912%)\n",
      "Epoch: [30][200/391]\tTime 0.045 (0.048)\tData 0.003 (0.005)\tLoss 0.3741 (0.3939)\tPrec 85.938% (86.878%)\n",
      "Epoch: [30][300/391]\tTime 0.050 (0.047)\tData 0.002 (0.005)\tLoss 0.3693 (0.3992)\tPrec 88.281% (86.664%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.348 (0.348)\tLoss 0.3584 (0.3584)\tPrec 88.281% (88.281%)\n",
      " * Prec 82.800% \n",
      "best acc: 82.800000\n",
      "Epoch: [31][0/391]\tTime 0.388 (0.388)\tData 0.349 (0.349)\tLoss 0.4305 (0.4305)\tPrec 84.375% (84.375%)\n",
      "Epoch: [31][100/391]\tTime 0.044 (0.049)\tData 0.002 (0.007)\tLoss 0.3436 (0.4019)\tPrec 88.281% (86.711%)\n",
      "Epoch: [31][200/391]\tTime 0.063 (0.047)\tData 0.020 (0.005)\tLoss 0.3116 (0.3997)\tPrec 91.406% (86.843%)\n",
      "Epoch: [31][300/391]\tTime 0.042 (0.047)\tData 0.001 (0.005)\tLoss 0.3942 (0.3950)\tPrec 88.281% (86.950%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.317 (0.317)\tLoss 0.4233 (0.4233)\tPrec 85.938% (85.938%)\n",
      " * Prec 83.160% \n",
      "best acc: 83.160000\n",
      "Epoch: [32][0/391]\tTime 0.402 (0.402)\tData 0.367 (0.367)\tLoss 0.3398 (0.3398)\tPrec 87.500% (87.500%)\n",
      "Epoch: [32][100/391]\tTime 0.047 (0.049)\tData 0.011 (0.006)\tLoss 0.3533 (0.3704)\tPrec 87.500% (87.717%)\n",
      "Epoch: [32][200/391]\tTime 0.045 (0.047)\tData 0.003 (0.004)\tLoss 0.2758 (0.3719)\tPrec 91.406% (87.570%)\n",
      "Epoch: [32][300/391]\tTime 0.044 (0.046)\tData 0.003 (0.004)\tLoss 0.4456 (0.3776)\tPrec 84.375% (87.401%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.280 (0.280)\tLoss 0.5866 (0.5866)\tPrec 80.469% (80.469%)\n",
      " * Prec 75.880% \n",
      "best acc: 83.160000\n",
      "Epoch: [33][0/391]\tTime 0.351 (0.351)\tData 0.315 (0.315)\tLoss 0.6040 (0.6040)\tPrec 81.250% (81.250%)\n",
      "Epoch: [33][100/391]\tTime 0.050 (0.048)\tData 0.002 (0.006)\tLoss 0.4116 (0.3483)\tPrec 85.938% (87.956%)\n",
      "Epoch: [33][200/391]\tTime 0.044 (0.047)\tData 0.002 (0.005)\tLoss 0.3753 (0.3551)\tPrec 90.625% (87.982%)\n",
      "Epoch: [33][300/391]\tTime 0.045 (0.046)\tData 0.003 (0.004)\tLoss 0.4491 (0.3632)\tPrec 84.375% (87.689%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.315 (0.315)\tLoss 0.3264 (0.3264)\tPrec 88.281% (88.281%)\n",
      " * Prec 83.950% \n",
      "best acc: 83.950000\n",
      "Epoch: [34][0/391]\tTime 0.351 (0.351)\tData 0.307 (0.307)\tLoss 0.2674 (0.2674)\tPrec 90.625% (90.625%)\n",
      "Epoch: [34][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.007)\tLoss 0.3817 (0.3473)\tPrec 90.625% (88.676%)\n",
      "Epoch: [34][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.005)\tLoss 0.3235 (0.3508)\tPrec 92.188% (88.452%)\n",
      "Epoch: [34][300/391]\tTime 0.046 (0.047)\tData 0.002 (0.004)\tLoss 0.2278 (0.3516)\tPrec 89.844% (88.307%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.355 (0.355)\tLoss 0.4034 (0.4034)\tPrec 87.500% (87.500%)\n",
      " * Prec 84.880% \n",
      "best acc: 84.880000\n",
      "Epoch: [35][0/391]\tTime 0.361 (0.361)\tData 0.321 (0.321)\tLoss 0.2760 (0.2760)\tPrec 90.625% (90.625%)\n",
      "Epoch: [35][100/391]\tTime 0.042 (0.049)\tData 0.002 (0.007)\tLoss 0.3230 (0.3202)\tPrec 89.844% (89.364%)\n",
      "Epoch: [35][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.005)\tLoss 0.2102 (0.3266)\tPrec 94.531% (88.989%)\n",
      "Epoch: [35][300/391]\tTime 0.047 (0.047)\tData 0.002 (0.004)\tLoss 0.3003 (0.3363)\tPrec 88.281% (88.668%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.342 (0.342)\tLoss 0.3183 (0.3183)\tPrec 89.062% (89.062%)\n",
      " * Prec 85.120% \n",
      "best acc: 85.120000\n",
      "Epoch: [36][0/391]\tTime 0.397 (0.397)\tData 0.359 (0.359)\tLoss 0.3522 (0.3522)\tPrec 91.406% (91.406%)\n",
      "Epoch: [36][100/391]\tTime 0.043 (0.049)\tData 0.002 (0.007)\tLoss 0.2050 (0.3198)\tPrec 94.531% (89.442%)\n",
      "Epoch: [36][200/391]\tTime 0.045 (0.048)\tData 0.008 (0.006)\tLoss 0.4486 (0.3273)\tPrec 84.375% (89.082%)\n",
      "Epoch: [36][300/391]\tTime 0.051 (0.048)\tData 0.002 (0.005)\tLoss 0.2271 (0.3314)\tPrec 90.625% (88.912%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.283 (0.283)\tLoss 0.3331 (0.3331)\tPrec 87.500% (87.500%)\n",
      " * Prec 84.760% \n",
      "best acc: 85.120000\n",
      "Epoch: [37][0/391]\tTime 0.328 (0.328)\tData 0.285 (0.285)\tLoss 0.2464 (0.2464)\tPrec 91.406% (91.406%)\n",
      "Epoch: [37][100/391]\tTime 0.045 (0.048)\tData 0.009 (0.005)\tLoss 0.3397 (0.3068)\tPrec 90.625% (89.588%)\n",
      "Epoch: [37][200/391]\tTime 0.047 (0.046)\tData 0.002 (0.003)\tLoss 0.2337 (0.3179)\tPrec 92.188% (89.253%)\n",
      "Epoch: [37][300/391]\tTime 0.045 (0.046)\tData 0.003 (0.003)\tLoss 0.3012 (0.3169)\tPrec 88.281% (89.345%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.272 (0.272)\tLoss 0.3302 (0.3302)\tPrec 89.844% (89.844%)\n",
      " * Prec 85.770% \n",
      "best acc: 85.770000\n",
      "Epoch: [38][0/391]\tTime 0.331 (0.331)\tData 0.298 (0.298)\tLoss 0.3246 (0.3246)\tPrec 87.500% (87.500%)\n",
      "Epoch: [38][100/391]\tTime 0.049 (0.048)\tData 0.002 (0.005)\tLoss 0.2329 (0.2911)\tPrec 89.844% (90.292%)\n",
      "Epoch: [38][200/391]\tTime 0.045 (0.047)\tData 0.004 (0.004)\tLoss 0.2665 (0.2995)\tPrec 91.406% (90.120%)\n",
      "Epoch: [38][300/391]\tTime 0.044 (0.046)\tData 0.003 (0.003)\tLoss 0.3452 (0.3030)\tPrec 85.938% (90.015%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.415 (0.415)\tLoss 0.3494 (0.3494)\tPrec 89.844% (89.844%)\n",
      " * Prec 85.420% \n",
      "best acc: 85.770000\n",
      "Epoch: [39][0/391]\tTime 0.361 (0.361)\tData 0.321 (0.321)\tLoss 0.3719 (0.3719)\tPrec 85.156% (85.156%)\n",
      "Epoch: [39][100/391]\tTime 0.049 (0.048)\tData 0.002 (0.006)\tLoss 0.3187 (0.2915)\tPrec 87.500% (90.099%)\n",
      "Epoch: [39][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.3617 (0.3010)\tPrec 88.281% (89.848%)\n",
      "Epoch: [39][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.004)\tLoss 0.3816 (0.3036)\tPrec 86.719% (89.875%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.215 (0.215)\tLoss 0.3221 (0.3221)\tPrec 86.719% (86.719%)\n",
      " * Prec 85.820% \n",
      "best acc: 85.820000\n",
      "Epoch: [40][0/391]\tTime 0.361 (0.361)\tData 0.300 (0.300)\tLoss 0.2802 (0.2802)\tPrec 88.281% (88.281%)\n",
      "Epoch: [40][100/391]\tTime 0.045 (0.050)\tData 0.003 (0.006)\tLoss 0.1766 (0.2768)\tPrec 94.531% (90.617%)\n",
      "Epoch: [40][200/391]\tTime 0.047 (0.048)\tData 0.002 (0.005)\tLoss 0.2543 (0.2875)\tPrec 91.406% (90.306%)\n",
      "Epoch: [40][300/391]\tTime 0.046 (0.047)\tData 0.002 (0.004)\tLoss 0.3185 (0.2960)\tPrec 89.844% (90.018%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.298 (0.298)\tLoss 0.3119 (0.3119)\tPrec 89.844% (89.844%)\n",
      " * Prec 84.880% \n",
      "best acc: 85.820000\n",
      "Epoch: [41][0/391]\tTime 0.427 (0.427)\tData 0.387 (0.387)\tLoss 0.2710 (0.2710)\tPrec 92.188% (92.188%)\n",
      "Epoch: [41][100/391]\tTime 0.045 (0.051)\tData 0.002 (0.009)\tLoss 0.3820 (0.2761)\tPrec 89.062% (90.803%)\n",
      "Epoch: [41][200/391]\tTime 0.063 (0.049)\tData 0.002 (0.006)\tLoss 0.2158 (0.2857)\tPrec 92.188% (90.407%)\n",
      "Epoch: [41][300/391]\tTime 0.045 (0.048)\tData 0.002 (0.005)\tLoss 0.3268 (0.2875)\tPrec 88.281% (90.303%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.277 (0.277)\tLoss 0.3317 (0.3317)\tPrec 91.406% (91.406%)\n",
      " * Prec 84.850% \n",
      "best acc: 85.820000\n",
      "Epoch: [42][0/391]\tTime 0.476 (0.476)\tData 0.440 (0.440)\tLoss 0.2297 (0.2297)\tPrec 89.844% (89.844%)\n",
      "Epoch: [42][100/391]\tTime 0.047 (0.050)\tData 0.003 (0.008)\tLoss 0.3280 (0.2732)\tPrec 89.062% (90.934%)\n",
      "Epoch: [42][200/391]\tTime 0.045 (0.048)\tData 0.002 (0.006)\tLoss 0.2870 (0.2766)\tPrec 87.500% (90.858%)\n",
      "Epoch: [42][300/391]\tTime 0.041 (0.047)\tData 0.002 (0.005)\tLoss 0.3821 (0.2788)\tPrec 86.719% (90.687%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.286 (0.286)\tLoss 0.3330 (0.3330)\tPrec 86.719% (86.719%)\n",
      " * Prec 84.300% \n",
      "best acc: 85.820000\n",
      "Epoch: [43][0/391]\tTime 0.304 (0.304)\tData 0.268 (0.268)\tLoss 0.2535 (0.2535)\tPrec 91.406% (91.406%)\n",
      "Epoch: [43][100/391]\tTime 0.045 (0.048)\tData 0.003 (0.006)\tLoss 0.2679 (0.2571)\tPrec 92.969% (91.329%)\n",
      "Epoch: [43][200/391]\tTime 0.043 (0.047)\tData 0.001 (0.005)\tLoss 0.2009 (0.2673)\tPrec 92.188% (91.088%)\n",
      "Epoch: [43][300/391]\tTime 0.046 (0.047)\tData 0.003 (0.004)\tLoss 0.4319 (0.2732)\tPrec 83.594% (90.866%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.280 (0.280)\tLoss 0.2618 (0.2618)\tPrec 88.281% (88.281%)\n",
      " * Prec 85.470% \n",
      "best acc: 85.820000\n",
      "Epoch: [44][0/391]\tTime 0.403 (0.403)\tData 0.367 (0.367)\tLoss 0.2570 (0.2570)\tPrec 92.188% (92.188%)\n",
      "Epoch: [44][100/391]\tTime 0.045 (0.049)\tData 0.003 (0.007)\tLoss 0.3030 (0.2730)\tPrec 92.188% (90.934%)\n",
      "Epoch: [44][200/391]\tTime 0.068 (0.049)\tData 0.032 (0.007)\tLoss 0.2643 (0.2661)\tPrec 92.188% (91.119%)\n",
      "Epoch: [44][300/391]\tTime 0.045 (0.048)\tData 0.003 (0.006)\tLoss 0.2703 (0.2683)\tPrec 88.281% (90.965%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.360 (0.360)\tLoss 0.2821 (0.2821)\tPrec 92.188% (92.188%)\n",
      " * Prec 87.130% \n",
      "best acc: 87.130000\n",
      "Epoch: [45][0/391]\tTime 0.350 (0.350)\tData 0.308 (0.308)\tLoss 0.1913 (0.1913)\tPrec 94.531% (94.531%)\n",
      "Epoch: [45][100/391]\tTime 0.072 (0.050)\tData 0.003 (0.007)\tLoss 0.2714 (0.2400)\tPrec 89.844% (92.133%)\n",
      "Epoch: [45][200/391]\tTime 0.044 (0.048)\tData 0.003 (0.005)\tLoss 0.2400 (0.2508)\tPrec 92.188% (91.818%)\n",
      "Epoch: [45][300/391]\tTime 0.052 (0.048)\tData 0.003 (0.005)\tLoss 0.2329 (0.2538)\tPrec 89.844% (91.648%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.378 (0.378)\tLoss 0.3316 (0.3316)\tPrec 89.062% (89.062%)\n",
      " * Prec 85.330% \n",
      "best acc: 87.130000\n",
      "Epoch: [46][0/391]\tTime 0.472 (0.472)\tData 0.434 (0.434)\tLoss 0.2248 (0.2248)\tPrec 94.531% (94.531%)\n",
      "Epoch: [46][100/391]\tTime 0.045 (0.055)\tData 0.001 (0.016)\tLoss 0.1933 (0.2418)\tPrec 93.750% (91.855%)\n",
      "Epoch: [46][200/391]\tTime 0.049 (0.053)\tData 0.017 (0.013)\tLoss 0.1687 (0.2457)\tPrec 94.531% (91.799%)\n",
      "Epoch: [46][300/391]\tTime 0.053 (0.051)\tData 0.001 (0.010)\tLoss 0.3397 (0.2477)\tPrec 85.938% (91.754%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.404 (0.404)\tLoss 0.2907 (0.2907)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.450% \n",
      "best acc: 87.130000\n",
      "Epoch: [47][0/391]\tTime 0.362 (0.362)\tData 0.324 (0.324)\tLoss 0.1936 (0.1936)\tPrec 92.188% (92.188%)\n",
      "Epoch: [47][100/391]\tTime 0.044 (0.050)\tData 0.002 (0.006)\tLoss 0.3070 (0.2403)\tPrec 87.500% (91.940%)\n",
      "Epoch: [47][200/391]\tTime 0.046 (0.047)\tData 0.012 (0.004)\tLoss 0.2028 (0.2410)\tPrec 93.750% (92.059%)\n",
      "Epoch: [47][300/391]\tTime 0.047 (0.047)\tData 0.010 (0.005)\tLoss 0.2639 (0.2482)\tPrec 92.188% (91.775%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.313 (0.313)\tLoss 0.3317 (0.3317)\tPrec 89.844% (89.844%)\n",
      " * Prec 87.260% \n",
      "best acc: 87.260000\n",
      "Epoch: [48][0/391]\tTime 0.375 (0.375)\tData 0.334 (0.334)\tLoss 0.3387 (0.3387)\tPrec 86.719% (86.719%)\n",
      "Epoch: [48][100/391]\tTime 0.069 (0.051)\tData 0.014 (0.009)\tLoss 0.1490 (0.2316)\tPrec 95.312% (92.141%)\n",
      "Epoch: [48][200/391]\tTime 0.045 (0.050)\tData 0.003 (0.008)\tLoss 0.1562 (0.2343)\tPrec 94.531% (92.083%)\n",
      "Epoch: [48][300/391]\tTime 0.053 (0.049)\tData 0.019 (0.007)\tLoss 0.3133 (0.2352)\tPrec 90.625% (92.146%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.307 (0.307)\tLoss 0.3871 (0.3871)\tPrec 84.375% (84.375%)\n",
      " * Prec 86.080% \n",
      "best acc: 87.260000\n",
      "Epoch: [49][0/391]\tTime 0.484 (0.484)\tData 0.440 (0.440)\tLoss 0.2554 (0.2554)\tPrec 91.406% (91.406%)\n",
      "Epoch: [49][100/391]\tTime 0.058 (0.053)\tData 0.026 (0.010)\tLoss 0.2232 (0.2195)\tPrec 92.969% (92.559%)\n",
      "Epoch: [49][200/391]\tTime 0.045 (0.049)\tData 0.002 (0.007)\tLoss 0.3448 (0.2247)\tPrec 86.719% (92.269%)\n",
      "Epoch: [49][300/391]\tTime 0.045 (0.048)\tData 0.005 (0.005)\tLoss 0.1289 (0.2282)\tPrec 95.312% (92.143%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.309 (0.309)\tLoss 0.3897 (0.3897)\tPrec 83.594% (83.594%)\n",
      " * Prec 85.460% \n",
      "best acc: 87.260000\n",
      "Epoch: [50][0/391]\tTime 0.413 (0.413)\tData 0.367 (0.367)\tLoss 0.2331 (0.2331)\tPrec 94.531% (94.531%)\n",
      "Epoch: [50][100/391]\tTime 0.048 (0.050)\tData 0.006 (0.008)\tLoss 0.1631 (0.2376)\tPrec 95.312% (92.087%)\n",
      "Epoch: [50][200/391]\tTime 0.047 (0.048)\tData 0.011 (0.007)\tLoss 0.2289 (0.2334)\tPrec 92.188% (92.180%)\n",
      "Epoch: [50][300/391]\tTime 0.045 (0.048)\tData 0.008 (0.007)\tLoss 0.2408 (0.2306)\tPrec 90.625% (92.247%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.314 (0.314)\tLoss 0.3396 (0.3396)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.880% \n",
      "best acc: 87.260000\n",
      "Epoch: [51][0/391]\tTime 0.341 (0.341)\tData 0.294 (0.294)\tLoss 0.2134 (0.2134)\tPrec 91.406% (91.406%)\n",
      "Epoch: [51][100/391]\tTime 0.047 (0.049)\tData 0.002 (0.007)\tLoss 0.3529 (0.2220)\tPrec 86.719% (92.559%)\n",
      "Epoch: [51][200/391]\tTime 0.042 (0.048)\tData 0.001 (0.005)\tLoss 0.2609 (0.2258)\tPrec 90.625% (92.382%)\n",
      "Epoch: [51][300/391]\tTime 0.041 (0.047)\tData 0.004 (0.004)\tLoss 0.1724 (0.2245)\tPrec 93.750% (92.450%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.304 (0.304)\tLoss 0.3045 (0.3045)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.210% \n",
      "best acc: 87.260000\n",
      "Epoch: [52][0/391]\tTime 0.371 (0.371)\tData 0.330 (0.330)\tLoss 0.2567 (0.2567)\tPrec 92.188% (92.188%)\n",
      "Epoch: [52][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.006)\tLoss 0.3608 (0.2184)\tPrec 85.938% (92.845%)\n",
      "Epoch: [52][200/391]\tTime 0.046 (0.047)\tData 0.003 (0.004)\tLoss 0.1509 (0.2187)\tPrec 94.531% (92.790%)\n",
      "Epoch: [52][300/391]\tTime 0.045 (0.047)\tData 0.003 (0.004)\tLoss 0.1072 (0.2175)\tPrec 96.094% (92.774%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.323 (0.323)\tLoss 0.2680 (0.2680)\tPrec 92.188% (92.188%)\n",
      " * Prec 86.840% \n",
      "best acc: 87.260000\n",
      "Epoch: [53][0/391]\tTime 0.470 (0.470)\tData 0.431 (0.431)\tLoss 0.2185 (0.2185)\tPrec 94.531% (94.531%)\n",
      "Epoch: [53][100/391]\tTime 0.045 (0.049)\tData 0.005 (0.008)\tLoss 0.1807 (0.2014)\tPrec 92.188% (93.123%)\n",
      "Epoch: [53][200/391]\tTime 0.042 (0.047)\tData 0.002 (0.006)\tLoss 0.2629 (0.2108)\tPrec 93.750% (92.786%)\n",
      "Epoch: [53][300/391]\tTime 0.046 (0.047)\tData 0.002 (0.005)\tLoss 0.1492 (0.2108)\tPrec 96.875% (92.829%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.302 (0.302)\tLoss 0.2919 (0.2919)\tPrec 90.625% (90.625%)\n",
      " * Prec 86.240% \n",
      "best acc: 87.260000\n",
      "Epoch: [54][0/391]\tTime 0.408 (0.408)\tData 0.366 (0.366)\tLoss 0.1287 (0.1287)\tPrec 94.531% (94.531%)\n",
      "Epoch: [54][100/391]\tTime 0.042 (0.049)\tData 0.002 (0.007)\tLoss 0.1960 (0.1913)\tPrec 92.188% (93.533%)\n",
      "Epoch: [54][200/391]\tTime 0.054 (0.047)\tData 0.003 (0.005)\tLoss 0.2423 (0.1972)\tPrec 92.188% (93.229%)\n",
      "Epoch: [54][300/391]\tTime 0.048 (0.047)\tData 0.002 (0.004)\tLoss 0.2555 (0.2028)\tPrec 89.062% (93.132%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.250 (0.250)\tLoss 0.3290 (0.3290)\tPrec 87.500% (87.500%)\n",
      " * Prec 86.800% \n",
      "best acc: 87.260000\n",
      "Epoch: [55][0/391]\tTime 0.397 (0.397)\tData 0.356 (0.356)\tLoss 0.1577 (0.1577)\tPrec 94.531% (94.531%)\n",
      "Epoch: [55][100/391]\tTime 0.051 (0.049)\tData 0.002 (0.007)\tLoss 0.0808 (0.1865)\tPrec 97.656% (93.843%)\n",
      "Epoch: [55][200/391]\tTime 0.045 (0.047)\tData 0.004 (0.005)\tLoss 0.2413 (0.1941)\tPrec 95.312% (93.618%)\n",
      "Epoch: [55][300/391]\tTime 0.041 (0.047)\tData 0.003 (0.004)\tLoss 0.3323 (0.1995)\tPrec 89.844% (93.374%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.269 (0.269)\tLoss 0.3046 (0.3046)\tPrec 89.844% (89.844%)\n",
      " * Prec 86.760% \n",
      "best acc: 87.260000\n",
      "Epoch: [56][0/391]\tTime 0.325 (0.325)\tData 0.277 (0.277)\tLoss 0.2068 (0.2068)\tPrec 93.750% (93.750%)\n",
      "Epoch: [56][100/391]\tTime 0.039 (0.048)\tData 0.003 (0.005)\tLoss 0.1629 (0.1905)\tPrec 93.750% (93.464%)\n",
      "Epoch: [56][200/391]\tTime 0.044 (0.047)\tData 0.002 (0.004)\tLoss 0.2311 (0.1953)\tPrec 94.531% (93.354%)\n",
      "Epoch: [56][300/391]\tTime 0.045 (0.046)\tData 0.002 (0.004)\tLoss 0.1369 (0.1977)\tPrec 94.531% (93.257%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.335 (0.335)\tLoss 0.2121 (0.2121)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.930% \n",
      "best acc: 87.260000\n",
      "Epoch: [57][0/391]\tTime 0.364 (0.364)\tData 0.322 (0.322)\tLoss 0.1762 (0.1762)\tPrec 94.531% (94.531%)\n",
      "Epoch: [57][100/391]\tTime 0.045 (0.050)\tData 0.005 (0.006)\tLoss 0.2626 (0.1805)\tPrec 92.969% (93.827%)\n",
      "Epoch: [57][200/391]\tTime 0.044 (0.048)\tData 0.003 (0.005)\tLoss 0.1267 (0.1902)\tPrec 96.875% (93.614%)\n",
      "Epoch: [57][300/391]\tTime 0.046 (0.047)\tData 0.002 (0.004)\tLoss 0.1864 (0.1961)\tPrec 94.531% (93.457%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.334 (0.334)\tLoss 0.2712 (0.2712)\tPrec 92.188% (92.188%)\n",
      " * Prec 87.140% \n",
      "best acc: 87.260000\n",
      "Epoch: [58][0/391]\tTime 0.360 (0.360)\tData 0.318 (0.318)\tLoss 0.3120 (0.3120)\tPrec 91.406% (91.406%)\n",
      "Epoch: [58][100/391]\tTime 0.038 (0.049)\tData 0.003 (0.005)\tLoss 0.1253 (0.1743)\tPrec 94.531% (94.183%)\n",
      "Epoch: [58][200/391]\tTime 0.052 (0.048)\tData 0.018 (0.005)\tLoss 0.2025 (0.1809)\tPrec 92.188% (93.886%)\n",
      "Epoch: [58][300/391]\tTime 0.047 (0.049)\tData 0.003 (0.006)\tLoss 0.1314 (0.1862)\tPrec 94.531% (93.729%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.294 (0.294)\tLoss 0.3151 (0.3151)\tPrec 90.625% (90.625%)\n",
      " * Prec 87.230% \n",
      "best acc: 87.260000\n",
      "Epoch: [59][0/391]\tTime 0.410 (0.410)\tData 0.369 (0.369)\tLoss 0.2141 (0.2141)\tPrec 92.969% (92.969%)\n",
      "Epoch: [59][100/391]\tTime 0.045 (0.050)\tData 0.005 (0.008)\tLoss 0.1192 (0.1867)\tPrec 96.875% (93.696%)\n",
      "Epoch: [59][200/391]\tTime 0.041 (0.049)\tData 0.005 (0.006)\tLoss 0.1686 (0.1837)\tPrec 93.750% (93.816%)\n",
      "Epoch: [59][300/391]\tTime 0.044 (0.049)\tData 0.011 (0.007)\tLoss 0.2450 (0.1859)\tPrec 92.188% (93.760%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.260 (0.260)\tLoss 0.4178 (0.4178)\tPrec 89.062% (89.062%)\n",
      " * Prec 86.360% \n",
      "best acc: 87.260000\n",
      "Epoch: [60][0/391]\tTime 0.427 (0.427)\tData 0.386 (0.386)\tLoss 0.0950 (0.0950)\tPrec 96.875% (96.875%)\n",
      "Epoch: [60][100/391]\tTime 0.055 (0.052)\tData 0.023 (0.010)\tLoss 0.3263 (0.1603)\tPrec 91.406% (94.500%)\n",
      "Epoch: [60][200/391]\tTime 0.045 (0.050)\tData 0.000 (0.008)\tLoss 0.2382 (0.1760)\tPrec 92.188% (93.940%)\n",
      "Epoch: [60][300/391]\tTime 0.047 (0.049)\tData 0.004 (0.007)\tLoss 0.2065 (0.1812)\tPrec 92.969% (93.812%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.279 (0.279)\tLoss 0.1873 (0.1873)\tPrec 92.969% (92.969%)\n",
      " * Prec 87.620% \n",
      "best acc: 87.620000\n",
      "Epoch: [61][0/391]\tTime 0.372 (0.372)\tData 0.336 (0.336)\tLoss 0.1946 (0.1946)\tPrec 94.531% (94.531%)\n",
      "Epoch: [61][100/391]\tTime 0.045 (0.049)\tData 0.003 (0.007)\tLoss 0.1722 (0.1752)\tPrec 92.188% (94.075%)\n",
      "Epoch: [61][200/391]\tTime 0.042 (0.047)\tData 0.001 (0.005)\tLoss 0.1290 (0.1785)\tPrec 95.312% (93.913%)\n",
      "Epoch: [61][300/391]\tTime 0.045 (0.047)\tData 0.001 (0.004)\tLoss 0.2479 (0.1802)\tPrec 91.406% (93.880%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.394 (0.394)\tLoss 0.3065 (0.3065)\tPrec 89.062% (89.062%)\n",
      " * Prec 87.610% \n",
      "best acc: 87.620000\n",
      "Epoch: [62][0/391]\tTime 0.332 (0.332)\tData 0.291 (0.291)\tLoss 0.2026 (0.2026)\tPrec 94.531% (94.531%)\n",
      "Epoch: [62][100/391]\tTime 0.046 (0.049)\tData 0.002 (0.005)\tLoss 0.1460 (0.1620)\tPrec 95.312% (94.686%)\n",
      "Epoch: [62][200/391]\tTime 0.045 (0.047)\tData 0.005 (0.004)\tLoss 0.2697 (0.1713)\tPrec 90.625% (94.244%)\n",
      "Epoch: [62][300/391]\tTime 0.045 (0.046)\tData 0.001 (0.004)\tLoss 0.1450 (0.1760)\tPrec 95.312% (94.106%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.268 (0.268)\tLoss 0.1391 (0.1391)\tPrec 96.094% (96.094%)\n",
      " * Prec 87.030% \n",
      "best acc: 87.620000\n",
      "Epoch: [63][0/391]\tTime 0.342 (0.342)\tData 0.300 (0.300)\tLoss 0.0919 (0.0919)\tPrec 97.656% (97.656%)\n",
      "Epoch: [63][100/391]\tTime 0.045 (0.049)\tData 0.003 (0.006)\tLoss 0.1271 (0.1723)\tPrec 95.312% (94.036%)\n",
      "Epoch: [63][200/391]\tTime 0.058 (0.047)\tData 0.001 (0.005)\tLoss 0.2170 (0.1726)\tPrec 92.188% (94.213%)\n",
      "Epoch: [63][300/391]\tTime 0.045 (0.047)\tData 0.007 (0.005)\tLoss 0.2132 (0.1743)\tPrec 92.188% (94.217%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.359 (0.359)\tLoss 0.2784 (0.2784)\tPrec 90.625% (90.625%)\n",
      " * Prec 87.090% \n",
      "best acc: 87.620000\n",
      "Epoch: [64][0/391]\tTime 0.418 (0.418)\tData 0.378 (0.378)\tLoss 0.1190 (0.1190)\tPrec 96.875% (96.875%)\n",
      "Epoch: [64][100/391]\tTime 0.044 (0.053)\tData 0.001 (0.010)\tLoss 0.0884 (0.1562)\tPrec 96.875% (94.810%)\n",
      "Epoch: [64][200/391]\tTime 0.044 (0.050)\tData 0.003 (0.008)\tLoss 0.1550 (0.1623)\tPrec 94.531% (94.698%)\n",
      "Epoch: [64][300/391]\tTime 0.050 (0.049)\tData 0.014 (0.007)\tLoss 0.2359 (0.1639)\tPrec 90.625% (94.617%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.345 (0.345)\tLoss 0.3142 (0.3142)\tPrec 91.406% (91.406%)\n",
      " * Prec 84.950% \n",
      "best acc: 87.620000\n",
      "Epoch: [65][0/391]\tTime 0.337 (0.337)\tData 0.296 (0.296)\tLoss 0.1820 (0.1820)\tPrec 92.188% (92.188%)\n",
      "Epoch: [65][100/391]\tTime 0.045 (0.050)\tData 0.006 (0.010)\tLoss 0.1048 (0.1520)\tPrec 94.531% (94.748%)\n",
      "Epoch: [65][200/391]\tTime 0.046 (0.048)\tData 0.001 (0.007)\tLoss 0.2390 (0.1569)\tPrec 92.188% (94.574%)\n",
      "Epoch: [65][300/391]\tTime 0.045 (0.047)\tData 0.003 (0.007)\tLoss 0.1216 (0.1630)\tPrec 96.875% (94.459%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.371 (0.371)\tLoss 0.2355 (0.2355)\tPrec 92.969% (92.969%)\n",
      " * Prec 86.760% \n",
      "best acc: 87.620000\n",
      "Epoch: [66][0/391]\tTime 0.356 (0.356)\tData 0.315 (0.315)\tLoss 0.1315 (0.1315)\tPrec 96.094% (96.094%)\n",
      "Epoch: [66][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.006)\tLoss 0.3170 (0.1604)\tPrec 89.844% (94.647%)\n",
      "Epoch: [66][200/391]\tTime 0.045 (0.047)\tData 0.006 (0.004)\tLoss 0.2226 (0.1658)\tPrec 96.094% (94.430%)\n",
      "Epoch: [66][300/391]\tTime 0.041 (0.047)\tData 0.002 (0.004)\tLoss 0.1575 (0.1712)\tPrec 94.531% (94.285%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.319 (0.319)\tLoss 0.3487 (0.3487)\tPrec 89.062% (89.062%)\n",
      " * Prec 87.620% \n",
      "best acc: 87.620000\n",
      "Epoch: [67][0/391]\tTime 0.361 (0.361)\tData 0.326 (0.326)\tLoss 0.1967 (0.1967)\tPrec 95.312% (95.312%)\n",
      "Epoch: [67][100/391]\tTime 0.048 (0.049)\tData 0.002 (0.006)\tLoss 0.1675 (0.1539)\tPrec 93.750% (94.686%)\n",
      "Epoch: [67][200/391]\tTime 0.045 (0.047)\tData 0.003 (0.004)\tLoss 0.1474 (0.1523)\tPrec 95.312% (94.726%)\n",
      "Epoch: [67][300/391]\tTime 0.050 (0.046)\tData 0.003 (0.004)\tLoss 0.1212 (0.1574)\tPrec 96.094% (94.607%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.296 (0.296)\tLoss 0.3966 (0.3966)\tPrec 88.281% (88.281%)\n",
      " * Prec 86.390% \n",
      "best acc: 87.620000\n",
      "Epoch: [68][0/391]\tTime 0.349 (0.349)\tData 0.305 (0.305)\tLoss 0.1123 (0.1123)\tPrec 95.312% (95.312%)\n",
      "Epoch: [68][100/391]\tTime 0.044 (0.051)\tData 0.001 (0.008)\tLoss 0.3311 (0.1425)\tPrec 90.625% (95.096%)\n",
      "Epoch: [68][200/391]\tTime 0.045 (0.048)\tData 0.002 (0.005)\tLoss 0.1536 (0.1465)\tPrec 96.094% (94.928%)\n",
      "Epoch: [68][300/391]\tTime 0.045 (0.047)\tData 0.003 (0.005)\tLoss 0.0973 (0.1517)\tPrec 96.875% (94.791%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.301 (0.301)\tLoss 0.4092 (0.4092)\tPrec 89.844% (89.844%)\n",
      " * Prec 87.480% \n",
      "best acc: 87.620000\n",
      "Epoch: [69][0/391]\tTime 0.365 (0.365)\tData 0.323 (0.323)\tLoss 0.1109 (0.1109)\tPrec 96.094% (96.094%)\n",
      "Epoch: [69][100/391]\tTime 0.040 (0.048)\tData 0.002 (0.006)\tLoss 0.1571 (0.1386)\tPrec 92.969% (95.297%)\n",
      "Epoch: [69][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.004)\tLoss 0.2005 (0.1493)\tPrec 92.188% (94.932%)\n",
      "Epoch: [69][300/391]\tTime 0.051 (0.046)\tData 0.002 (0.003)\tLoss 0.1308 (0.1570)\tPrec 94.531% (94.703%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.369 (0.369)\tLoss 0.3154 (0.3154)\tPrec 90.625% (90.625%)\n",
      " * Prec 85.960% \n",
      "best acc: 87.620000\n",
      "Epoch: [70][0/391]\tTime 0.332 (0.332)\tData 0.289 (0.289)\tLoss 0.1209 (0.1209)\tPrec 95.312% (95.312%)\n",
      "Epoch: [70][100/391]\tTime 0.045 (0.048)\tData 0.002 (0.005)\tLoss 0.1495 (0.1468)\tPrec 95.312% (94.941%)\n",
      "Epoch: [70][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.004)\tLoss 0.0838 (0.1495)\tPrec 96.875% (94.846%)\n",
      "Epoch: [70][300/391]\tTime 0.039 (0.046)\tData 0.002 (0.003)\tLoss 0.1333 (0.1505)\tPrec 96.094% (94.876%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.309 (0.309)\tLoss 0.3033 (0.3033)\tPrec 89.844% (89.844%)\n",
      " * Prec 88.120% \n",
      "best acc: 88.120000\n",
      "Epoch: [71][0/391]\tTime 0.356 (0.356)\tData 0.319 (0.319)\tLoss 0.1179 (0.1179)\tPrec 96.875% (96.875%)\n",
      "Epoch: [71][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.006)\tLoss 0.0835 (0.1339)\tPrec 96.875% (95.560%)\n",
      "Epoch: [71][200/391]\tTime 0.039 (0.047)\tData 0.002 (0.004)\tLoss 0.1587 (0.1428)\tPrec 94.531% (95.309%)\n",
      "Epoch: [71][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.1793 (0.1457)\tPrec 93.750% (95.170%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.284 (0.284)\tLoss 0.2778 (0.2778)\tPrec 89.062% (89.062%)\n",
      " * Prec 87.330% \n",
      "best acc: 88.120000\n",
      "Epoch: [72][0/391]\tTime 0.323 (0.323)\tData 0.279 (0.279)\tLoss 0.0533 (0.0533)\tPrec 98.438% (98.438%)\n",
      "Epoch: [72][100/391]\tTime 0.050 (0.049)\tData 0.011 (0.007)\tLoss 0.1458 (0.1405)\tPrec 93.750% (95.142%)\n",
      "Epoch: [72][200/391]\tTime 0.048 (0.048)\tData 0.001 (0.006)\tLoss 0.1556 (0.1422)\tPrec 96.094% (95.246%)\n",
      "Epoch: [72][300/391]\tTime 0.048 (0.048)\tData 0.006 (0.005)\tLoss 0.1981 (0.1438)\tPrec 94.531% (95.214%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.277 (0.277)\tLoss 0.3223 (0.3223)\tPrec 90.625% (90.625%)\n",
      " * Prec 88.400% \n",
      "best acc: 88.400000\n",
      "Epoch: [73][0/391]\tTime 0.429 (0.429)\tData 0.379 (0.379)\tLoss 0.1596 (0.1596)\tPrec 95.312% (95.312%)\n",
      "Epoch: [73][100/391]\tTime 0.045 (0.050)\tData 0.003 (0.007)\tLoss 0.0896 (0.1365)\tPrec 97.656% (95.506%)\n",
      "Epoch: [73][200/391]\tTime 0.059 (0.049)\tData 0.017 (0.007)\tLoss 0.1412 (0.1404)\tPrec 95.312% (95.324%)\n",
      "Epoch: [73][300/391]\tTime 0.049 (0.048)\tData 0.017 (0.006)\tLoss 0.2077 (0.1409)\tPrec 92.969% (95.341%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.344 (0.344)\tLoss 0.2690 (0.2690)\tPrec 90.625% (90.625%)\n",
      " * Prec 88.530% \n",
      "best acc: 88.530000\n",
      "Epoch: [74][0/391]\tTime 0.433 (0.433)\tData 0.385 (0.385)\tLoss 0.1570 (0.1570)\tPrec 91.406% (91.406%)\n",
      "Epoch: [74][100/391]\tTime 0.045 (0.052)\tData 0.013 (0.012)\tLoss 0.0897 (0.1352)\tPrec 97.656% (95.359%)\n",
      "Epoch: [74][200/391]\tTime 0.046 (0.049)\tData 0.003 (0.008)\tLoss 0.0998 (0.1432)\tPrec 98.438% (95.254%)\n",
      "Epoch: [74][300/391]\tTime 0.049 (0.048)\tData 0.006 (0.007)\tLoss 0.1027 (0.1467)\tPrec 94.531% (95.107%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.329 (0.329)\tLoss 0.2809 (0.2809)\tPrec 92.188% (92.188%)\n",
      " * Prec 88.180% \n",
      "best acc: 88.530000\n",
      "Epoch: [75][0/391]\tTime 0.337 (0.337)\tData 0.296 (0.296)\tLoss 0.0969 (0.0969)\tPrec 96.094% (96.094%)\n",
      "Epoch: [75][100/391]\tTime 0.048 (0.048)\tData 0.002 (0.005)\tLoss 0.1434 (0.1351)\tPrec 95.312% (95.591%)\n",
      "Epoch: [75][200/391]\tTime 0.048 (0.047)\tData 0.002 (0.004)\tLoss 0.2416 (0.1434)\tPrec 94.531% (95.254%)\n",
      "Epoch: [75][300/391]\tTime 0.042 (0.047)\tData 0.003 (0.005)\tLoss 0.1848 (0.1424)\tPrec 93.750% (95.248%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.310 (0.310)\tLoss 0.2687 (0.2687)\tPrec 92.188% (92.188%)\n",
      " * Prec 88.420% \n",
      "best acc: 88.530000\n",
      "Epoch: [76][0/391]\tTime 0.358 (0.358)\tData 0.316 (0.316)\tLoss 0.1792 (0.1792)\tPrec 93.750% (93.750%)\n",
      "Epoch: [76][100/391]\tTime 0.045 (0.049)\tData 0.003 (0.006)\tLoss 0.1213 (0.1272)\tPrec 96.094% (95.784%)\n",
      "Epoch: [76][200/391]\tTime 0.048 (0.047)\tData 0.003 (0.005)\tLoss 0.2852 (0.1319)\tPrec 91.406% (95.550%)\n",
      "Epoch: [76][300/391]\tTime 0.045 (0.047)\tData 0.003 (0.004)\tLoss 0.2588 (0.1364)\tPrec 90.625% (95.396%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.278 (0.278)\tLoss 0.2775 (0.2775)\tPrec 91.406% (91.406%)\n",
      " * Prec 87.670% \n",
      "best acc: 88.530000\n",
      "Epoch: [77][0/391]\tTime 0.366 (0.366)\tData 0.319 (0.319)\tLoss 0.1646 (0.1646)\tPrec 92.969% (92.969%)\n",
      "Epoch: [77][100/391]\tTime 0.044 (0.049)\tData 0.002 (0.005)\tLoss 0.2362 (0.1303)\tPrec 91.406% (95.552%)\n",
      "Epoch: [77][200/391]\tTime 0.045 (0.048)\tData 0.003 (0.005)\tLoss 0.1058 (0.1355)\tPrec 95.312% (95.425%)\n",
      "Epoch: [77][300/391]\tTime 0.051 (0.047)\tData 0.004 (0.004)\tLoss 0.1465 (0.1341)\tPrec 94.531% (95.362%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.325 (0.325)\tLoss 0.3845 (0.3845)\tPrec 88.281% (88.281%)\n",
      " * Prec 85.920% \n",
      "best acc: 88.530000\n",
      "Epoch: [78][0/391]\tTime 0.372 (0.372)\tData 0.330 (0.330)\tLoss 0.1176 (0.1176)\tPrec 96.875% (96.875%)\n",
      "Epoch: [78][100/391]\tTime 0.044 (0.049)\tData 0.003 (0.006)\tLoss 0.1765 (0.1234)\tPrec 93.750% (96.032%)\n",
      "Epoch: [78][200/391]\tTime 0.045 (0.048)\tData 0.005 (0.006)\tLoss 0.0714 (0.1297)\tPrec 96.875% (95.682%)\n",
      "Epoch: [78][300/391]\tTime 0.045 (0.047)\tData 0.003 (0.005)\tLoss 0.1634 (0.1333)\tPrec 94.531% (95.531%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.387 (0.387)\tLoss 0.2587 (0.2587)\tPrec 88.281% (88.281%)\n",
      " * Prec 85.870% \n",
      "best acc: 88.530000\n",
      "Epoch: [79][0/391]\tTime 0.407 (0.407)\tData 0.366 (0.366)\tLoss 0.1270 (0.1270)\tPrec 96.094% (96.094%)\n",
      "Epoch: [79][100/391]\tTime 0.045 (0.049)\tData 0.002 (0.007)\tLoss 0.1178 (0.1225)\tPrec 96.875% (95.692%)\n",
      "Epoch: [79][200/391]\tTime 0.044 (0.048)\tData 0.001 (0.005)\tLoss 0.1571 (0.1252)\tPrec 94.531% (95.678%)\n",
      "Epoch: [79][300/391]\tTime 0.046 (0.047)\tData 0.003 (0.005)\tLoss 0.2029 (0.1296)\tPrec 95.312% (95.676%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.325 (0.325)\tLoss 0.2543 (0.2543)\tPrec 93.750% (93.750%)\n",
      " * Prec 87.390% \n",
      "best acc: 88.530000\n",
      "Epoch: [80][0/391]\tTime 0.363 (0.363)\tData 0.326 (0.326)\tLoss 0.1154 (0.1154)\tPrec 96.094% (96.094%)\n",
      "Epoch: [80][100/391]\tTime 0.039 (0.049)\tData 0.002 (0.006)\tLoss 0.0714 (0.0911)\tPrec 96.875% (97.037%)\n",
      "Epoch: [80][200/391]\tTime 0.045 (0.048)\tData 0.002 (0.005)\tLoss 0.0197 (0.0786)\tPrec 100.000% (97.423%)\n",
      "Epoch: [80][300/391]\tTime 0.054 (0.047)\tData 0.003 (0.005)\tLoss 0.0265 (0.0737)\tPrec 99.219% (97.563%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.365 (0.365)\tLoss 0.1003 (0.1003)\tPrec 95.312% (95.312%)\n",
      " * Prec 90.650% \n",
      "best acc: 90.650000\n",
      "Epoch: [81][0/391]\tTime 0.429 (0.429)\tData 0.395 (0.395)\tLoss 0.0251 (0.0251)\tPrec 100.000% (100.000%)\n",
      "Epoch: [81][100/391]\tTime 0.045 (0.050)\tData 0.005 (0.009)\tLoss 0.0790 (0.0526)\tPrec 96.094% (98.291%)\n",
      "Epoch: [81][200/391]\tTime 0.045 (0.048)\tData 0.003 (0.007)\tLoss 0.0968 (0.0502)\tPrec 96.875% (98.356%)\n",
      "Epoch: [81][300/391]\tTime 0.040 (0.047)\tData 0.000 (0.006)\tLoss 0.0222 (0.0514)\tPrec 100.000% (98.300%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.297 (0.297)\tLoss 0.1276 (0.1276)\tPrec 95.312% (95.312%)\n",
      " * Prec 90.540% \n",
      "best acc: 90.650000\n",
      "Epoch: [82][0/391]\tTime 0.427 (0.427)\tData 0.388 (0.388)\tLoss 0.0509 (0.0509)\tPrec 99.219% (99.219%)\n",
      "Epoch: [82][100/391]\tTime 0.039 (0.050)\tData 0.002 (0.008)\tLoss 0.0228 (0.0451)\tPrec 99.219% (98.615%)\n",
      "Epoch: [82][200/391]\tTime 0.045 (0.048)\tData 0.003 (0.006)\tLoss 0.0241 (0.0435)\tPrec 99.219% (98.566%)\n",
      "Epoch: [82][300/391]\tTime 0.045 (0.047)\tData 0.003 (0.005)\tLoss 0.0552 (0.0432)\tPrec 97.656% (98.593%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.304 (0.304)\tLoss 0.1310 (0.1310)\tPrec 95.312% (95.312%)\n",
      " * Prec 91.150% \n",
      "best acc: 91.150000\n",
      "Epoch: [83][0/391]\tTime 0.491 (0.491)\tData 0.451 (0.451)\tLoss 0.0554 (0.0554)\tPrec 97.656% (97.656%)\n",
      "Epoch: [83][100/391]\tTime 0.042 (0.051)\tData 0.013 (0.008)\tLoss 0.0523 (0.0421)\tPrec 98.438% (98.677%)\n",
      "Epoch: [83][200/391]\tTime 0.054 (0.048)\tData 0.018 (0.006)\tLoss 0.0226 (0.0405)\tPrec 99.219% (98.733%)\n",
      "Epoch: [83][300/391]\tTime 0.044 (0.047)\tData 0.003 (0.005)\tLoss 0.0063 (0.0397)\tPrec 100.000% (98.723%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.336 (0.336)\tLoss 0.1709 (0.1709)\tPrec 95.312% (95.312%)\n",
      " * Prec 91.000% \n",
      "best acc: 91.150000\n",
      "Epoch: [84][0/391]\tTime 0.409 (0.409)\tData 0.370 (0.370)\tLoss 0.0227 (0.0227)\tPrec 98.438% (98.438%)\n",
      "Epoch: [84][100/391]\tTime 0.049 (0.050)\tData 0.013 (0.007)\tLoss 0.0163 (0.0360)\tPrec 99.219% (98.762%)\n",
      "Epoch: [84][200/391]\tTime 0.045 (0.048)\tData 0.003 (0.005)\tLoss 0.0082 (0.0355)\tPrec 100.000% (98.791%)\n",
      "Epoch: [84][300/391]\tTime 0.044 (0.048)\tData 0.005 (0.005)\tLoss 0.0607 (0.0350)\tPrec 97.656% (98.801%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.341 (0.341)\tLoss 0.0896 (0.0896)\tPrec 98.438% (98.438%)\n",
      " * Prec 90.750% \n",
      "best acc: 91.150000\n",
      "Epoch: [85][0/391]\tTime 0.370 (0.370)\tData 0.327 (0.327)\tLoss 0.0055 (0.0055)\tPrec 100.000% (100.000%)\n",
      "Epoch: [85][100/391]\tTime 0.045 (0.048)\tData 0.002 (0.006)\tLoss 0.0227 (0.0330)\tPrec 99.219% (98.956%)\n",
      "Epoch: [85][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.0043 (0.0335)\tPrec 100.000% (98.912%)\n",
      "Epoch: [85][300/391]\tTime 0.046 (0.046)\tData 0.015 (0.004)\tLoss 0.0199 (0.0339)\tPrec 99.219% (98.928%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.276 (0.276)\tLoss 0.0897 (0.0897)\tPrec 95.312% (95.312%)\n",
      " * Prec 90.960% \n",
      "best acc: 91.150000\n",
      "Epoch: [86][0/391]\tTime 0.340 (0.340)\tData 0.298 (0.298)\tLoss 0.0247 (0.0247)\tPrec 99.219% (99.219%)\n",
      "Epoch: [86][100/391]\tTime 0.042 (0.048)\tData 0.003 (0.006)\tLoss 0.0569 (0.0335)\tPrec 98.438% (98.871%)\n",
      "Epoch: [86][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.005)\tLoss 0.0077 (0.0341)\tPrec 100.000% (98.912%)\n",
      "Epoch: [86][300/391]\tTime 0.045 (0.046)\tData 0.001 (0.004)\tLoss 0.0219 (0.0339)\tPrec 99.219% (98.900%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.314 (0.314)\tLoss 0.1698 (0.1698)\tPrec 95.312% (95.312%)\n",
      " * Prec 91.030% \n",
      "best acc: 91.150000\n",
      "Epoch: [87][0/391]\tTime 0.348 (0.348)\tData 0.310 (0.310)\tLoss 0.0345 (0.0345)\tPrec 99.219% (99.219%)\n",
      "Epoch: [87][100/391]\tTime 0.054 (0.049)\tData 0.020 (0.007)\tLoss 0.0356 (0.0323)\tPrec 98.438% (98.940%)\n",
      "Epoch: [87][200/391]\tTime 0.046 (0.048)\tData 0.003 (0.006)\tLoss 0.0626 (0.0311)\tPrec 96.875% (98.986%)\n",
      "Epoch: [87][300/391]\tTime 0.047 (0.048)\tData 0.003 (0.005)\tLoss 0.0075 (0.0317)\tPrec 100.000% (98.967%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.341 (0.341)\tLoss 0.1097 (0.1097)\tPrec 96.875% (96.875%)\n",
      " * Prec 91.280% \n",
      "best acc: 91.280000\n",
      "Epoch: [88][0/391]\tTime 0.362 (0.362)\tData 0.326 (0.326)\tLoss 0.0116 (0.0116)\tPrec 100.000% (100.000%)\n",
      "Epoch: [88][100/391]\tTime 0.044 (0.049)\tData 0.004 (0.006)\tLoss 0.0554 (0.0300)\tPrec 98.438% (99.134%)\n",
      "Epoch: [88][200/391]\tTime 0.046 (0.047)\tData 0.003 (0.005)\tLoss 0.0075 (0.0276)\tPrec 100.000% (99.137%)\n",
      "Epoch: [88][300/391]\tTime 0.039 (0.047)\tData 0.002 (0.004)\tLoss 0.0156 (0.0286)\tPrec 99.219% (99.092%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.329 (0.329)\tLoss 0.1120 (0.1120)\tPrec 96.094% (96.094%)\n",
      " * Prec 90.880% \n",
      "best acc: 91.280000\n",
      "Epoch: [89][0/391]\tTime 0.334 (0.334)\tData 0.293 (0.293)\tLoss 0.0680 (0.0680)\tPrec 98.438% (98.438%)\n",
      "Epoch: [89][100/391]\tTime 0.044 (0.049)\tData 0.001 (0.007)\tLoss 0.0355 (0.0319)\tPrec 99.219% (98.979%)\n",
      "Epoch: [89][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.005)\tLoss 0.0426 (0.0291)\tPrec 98.438% (99.079%)\n",
      "Epoch: [89][300/391]\tTime 0.047 (0.047)\tData 0.003 (0.004)\tLoss 0.0351 (0.0291)\tPrec 97.656% (99.071%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.268 (0.268)\tLoss 0.1636 (0.1636)\tPrec 94.531% (94.531%)\n",
      " * Prec 91.230% \n",
      "best acc: 91.280000\n",
      "Epoch: [90][0/391]\tTime 0.413 (0.413)\tData 0.371 (0.371)\tLoss 0.0217 (0.0217)\tPrec 99.219% (99.219%)\n",
      "Epoch: [90][100/391]\tTime 0.044 (0.050)\tData 0.001 (0.007)\tLoss 0.0149 (0.0265)\tPrec 99.219% (99.118%)\n",
      "Epoch: [90][200/391]\tTime 0.045 (0.048)\tData 0.005 (0.005)\tLoss 0.0140 (0.0252)\tPrec 100.000% (99.199%)\n",
      "Epoch: [90][300/391]\tTime 0.049 (0.048)\tData 0.002 (0.005)\tLoss 0.0271 (0.0247)\tPrec 99.219% (99.208%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.301 (0.301)\tLoss 0.1534 (0.1534)\tPrec 95.312% (95.312%)\n",
      " * Prec 91.270% \n",
      "best acc: 91.280000\n",
      "Epoch: [91][0/391]\tTime 0.396 (0.396)\tData 0.352 (0.352)\tLoss 0.1251 (0.1251)\tPrec 97.656% (97.656%)\n",
      "Epoch: [91][100/391]\tTime 0.046 (0.050)\tData 0.000 (0.009)\tLoss 0.0565 (0.0261)\tPrec 99.219% (99.172%)\n",
      "Epoch: [91][200/391]\tTime 0.046 (0.049)\tData 0.001 (0.006)\tLoss 0.0185 (0.0246)\tPrec 99.219% (99.223%)\n",
      "Epoch: [91][300/391]\tTime 0.051 (0.047)\tData 0.002 (0.005)\tLoss 0.0130 (0.0237)\tPrec 99.219% (99.250%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.269 (0.269)\tLoss 0.1661 (0.1661)\tPrec 95.312% (95.312%)\n",
      " * Prec 91.160% \n",
      "best acc: 91.280000\n",
      "Epoch: [92][0/391]\tTime 0.372 (0.372)\tData 0.336 (0.336)\tLoss 0.0322 (0.0322)\tPrec 98.438% (98.438%)\n",
      "Epoch: [92][100/391]\tTime 0.051 (0.049)\tData 0.002 (0.007)\tLoss 0.0159 (0.0256)\tPrec 99.219% (99.149%)\n",
      "Epoch: [92][200/391]\tTime 0.045 (0.048)\tData 0.003 (0.006)\tLoss 0.0058 (0.0223)\tPrec 100.000% (99.227%)\n",
      "Epoch: [92][300/391]\tTime 0.045 (0.048)\tData 0.003 (0.005)\tLoss 0.0273 (0.0227)\tPrec 98.438% (99.240%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.304 (0.304)\tLoss 0.1265 (0.1265)\tPrec 96.875% (96.875%)\n",
      " * Prec 91.220% \n",
      "best acc: 91.280000\n",
      "Epoch: [93][0/391]\tTime 0.472 (0.472)\tData 0.429 (0.429)\tLoss 0.0158 (0.0158)\tPrec 99.219% (99.219%)\n",
      "Epoch: [93][100/391]\tTime 0.061 (0.051)\tData 0.029 (0.010)\tLoss 0.0209 (0.0254)\tPrec 99.219% (99.296%)\n",
      "Epoch: [93][200/391]\tTime 0.048 (0.049)\tData 0.003 (0.007)\tLoss 0.0326 (0.0236)\tPrec 98.438% (99.277%)\n",
      "Epoch: [93][300/391]\tTime 0.045 (0.048)\tData 0.001 (0.006)\tLoss 0.0059 (0.0229)\tPrec 100.000% (99.289%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.382 (0.382)\tLoss 0.1522 (0.1522)\tPrec 96.094% (96.094%)\n",
      " * Prec 91.490% \n",
      "best acc: 91.490000\n",
      "Epoch: [94][0/391]\tTime 0.444 (0.444)\tData 0.402 (0.402)\tLoss 0.0852 (0.0852)\tPrec 97.656% (97.656%)\n",
      "Epoch: [94][100/391]\tTime 0.045 (0.050)\tData 0.003 (0.008)\tLoss 0.0238 (0.0217)\tPrec 99.219% (99.312%)\n",
      "Epoch: [94][200/391]\tTime 0.045 (0.048)\tData 0.002 (0.006)\tLoss 0.0128 (0.0219)\tPrec 100.000% (99.300%)\n",
      "Epoch: [94][300/391]\tTime 0.041 (0.047)\tData 0.005 (0.005)\tLoss 0.0084 (0.0222)\tPrec 100.000% (99.312%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.336 (0.336)\tLoss 0.1603 (0.1603)\tPrec 96.094% (96.094%)\n",
      " * Prec 91.320% \n",
      "best acc: 91.490000\n",
      "Epoch: [95][0/391]\tTime 0.455 (0.455)\tData 0.414 (0.414)\tLoss 0.0060 (0.0060)\tPrec 100.000% (100.000%)\n",
      "Epoch: [95][100/391]\tTime 0.044 (0.052)\tData 0.001 (0.010)\tLoss 0.0043 (0.0252)\tPrec 100.000% (99.141%)\n",
      "Epoch: [95][200/391]\tTime 0.050 (0.050)\tData 0.002 (0.008)\tLoss 0.0080 (0.0229)\tPrec 100.000% (99.234%)\n",
      "Epoch: [95][300/391]\tTime 0.044 (0.049)\tData 0.003 (0.007)\tLoss 0.0387 (0.0216)\tPrec 98.438% (99.297%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.363 (0.363)\tLoss 0.1549 (0.1549)\tPrec 95.312% (95.312%)\n",
      " * Prec 91.250% \n",
      "best acc: 91.490000\n",
      "Epoch: [96][0/391]\tTime 0.390 (0.390)\tData 0.355 (0.355)\tLoss 0.0136 (0.0136)\tPrec 99.219% (99.219%)\n",
      "Epoch: [96][100/391]\tTime 0.045 (0.050)\tData 0.003 (0.009)\tLoss 0.0098 (0.0195)\tPrec 100.000% (99.381%)\n",
      "Epoch: [96][200/391]\tTime 0.054 (0.048)\tData 0.022 (0.006)\tLoss 0.0043 (0.0206)\tPrec 100.000% (99.339%)\n",
      "Epoch: [96][300/391]\tTime 0.050 (0.047)\tData 0.002 (0.005)\tLoss 0.1428 (0.0208)\tPrec 95.312% (99.315%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.285 (0.285)\tLoss 0.1066 (0.1066)\tPrec 96.094% (96.094%)\n",
      " * Prec 91.160% \n",
      "best acc: 91.490000\n",
      "Epoch: [97][0/391]\tTime 0.314 (0.314)\tData 0.277 (0.277)\tLoss 0.0090 (0.0090)\tPrec 99.219% (99.219%)\n",
      "Epoch: [97][100/391]\tTime 0.045 (0.049)\tData 0.003 (0.006)\tLoss 0.0462 (0.0214)\tPrec 97.656% (99.296%)\n",
      "Epoch: [97][200/391]\tTime 0.045 (0.049)\tData 0.003 (0.005)\tLoss 0.0091 (0.0228)\tPrec 100.000% (99.250%)\n",
      "Epoch: [97][300/391]\tTime 0.047 (0.048)\tData 0.003 (0.005)\tLoss 0.0110 (0.0224)\tPrec 100.000% (99.265%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.329 (0.329)\tLoss 0.1661 (0.1661)\tPrec 95.312% (95.312%)\n",
      " * Prec 91.250% \n",
      "best acc: 91.490000\n",
      "Epoch: [98][0/391]\tTime 0.376 (0.376)\tData 0.335 (0.335)\tLoss 0.0272 (0.0272)\tPrec 97.656% (97.656%)\n",
      "Epoch: [98][100/391]\tTime 0.048 (0.049)\tData 0.003 (0.007)\tLoss 0.0651 (0.0203)\tPrec 99.219% (99.366%)\n",
      "Epoch: [98][200/391]\tTime 0.052 (0.048)\tData 0.002 (0.005)\tLoss 0.0054 (0.0206)\tPrec 100.000% (99.370%)\n",
      "Epoch: [98][300/391]\tTime 0.046 (0.047)\tData 0.003 (0.004)\tLoss 0.0187 (0.0205)\tPrec 99.219% (99.382%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.287 (0.287)\tLoss 0.1610 (0.1610)\tPrec 96.094% (96.094%)\n",
      " * Prec 91.150% \n",
      "best acc: 91.490000\n",
      "Epoch: [99][0/391]\tTime 0.406 (0.406)\tData 0.365 (0.365)\tLoss 0.0768 (0.0768)\tPrec 97.656% (97.656%)\n",
      "Epoch: [99][100/391]\tTime 0.041 (0.049)\tData 0.003 (0.007)\tLoss 0.0017 (0.0210)\tPrec 100.000% (99.366%)\n",
      "Epoch: [99][200/391]\tTime 0.047 (0.047)\tData 0.003 (0.005)\tLoss 0.0041 (0.0203)\tPrec 100.000% (99.390%)\n",
      "Epoch: [99][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.004)\tLoss 0.0061 (0.0205)\tPrec 100.000% (99.349%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.280 (0.280)\tLoss 0.1427 (0.1427)\tPrec 95.312% (95.312%)\n",
      " * Prec 91.530% \n",
      "best acc: 91.530000\n"
     ]
    }
   ],
   "source": [
    "# This cell won't be given, but students will complete the training\n",
    "\n",
    "lr = 4e-2\n",
    "weight_decay = 1e-4\n",
    "epochs = 100\n",
    "best_prec = 0\n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "#cudnn.benchmark = True\n",
    "\n",
    "if not os.path.exists('result'):\n",
    "    os.makedirs('result')\n",
    "fdir = 'result/'+str(model_name)\n",
    "if not os.path.exists(fdir):\n",
    "    os.makedirs(fdir)\n",
    "        \n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    train(trainloader, model, criterion, optimizer, epoch)\n",
    "    \n",
    "    # evaluate on test set\n",
    "    print(\"Validation starts\")\n",
    "    prec = validate(testloader, model, criterion)\n",
    "\n",
    "    # remember best precision and save checkpoint\n",
    "    is_best = prec > best_prec\n",
    "    best_prec = max(prec,best_prec)\n",
    "    print('best acc: {:1f}'.format(best_prec))\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec': best_prec,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, fdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-harris",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HW\n",
    "\n",
    "#  1. Train with 4 bits for both weight and activation to achieve >90% accuracy\n",
    "#  2. Find x_int and w_int for the 2nd convolution layer\n",
    "#  3. Check the recovered psum has similar value to the un-quantized original psum\n",
    "#     (such as example 1 in W3S2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "entertaining-queensland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 9153/10000 (92%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = \"result/VGG16_project/model_best.pth.tar\"\n",
    "model_name = \"VGG16_project\"\n",
    "model = VGG16_project()\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\") \n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceramic-nigeria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 -th layer prehooked\n",
      "7 -th layer prehooked\n",
      "12 -th layer prehooked\n",
      "16 -th layer prehooked\n",
      "21 -th layer prehooked\n",
      "25 -th layer prehooked\n",
      "29 -th layer prehooked\n",
      "34 -th layer prehooked\n",
      "38 -th layer prehooked\n",
      "41 -th layer prehooked\n",
      "46 -th layer prehooked\n",
      "50 -th layer prehooked\n",
      "54 -th layer prehooked\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "    def __call__(self, module, module_in):\n",
    "        self.outputs.append(module_in)\n",
    "    def clear(self):\n",
    "        self.outputs = []  \n",
    "        \n",
    "######### Save inputs from selected layer ##########\n",
    "save_output = SaveOutput()\n",
    "i = 0\n",
    "\n",
    "for layer in model.modules():\n",
    "    i = i+1\n",
    "    if isinstance(layer, QuantConv2d):\n",
    "        print(i,\"-th layer prehooked\")\n",
    "        layer.register_forward_pre_hook(save_output)             \n",
    "####################################################\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "images = images.to(device)\n",
    "out = model(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "spoken-worst",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.0000,  1.0000,  0.0000],\n",
      "          [-1.0000, -0.0000,  1.0000],\n",
      "          [-1.0000, -1.0000,  0.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000,  0.0000],\n",
      "          [-2.0000, -2.0000,  1.0000],\n",
      "          [-1.0000, -1.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -3.0000],\n",
      "          [-1.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -2.0000, -2.0000],\n",
      "          [-1.0000, -2.0000, -2.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -2.0000],\n",
      "          [-1.0000, -2.0000, -2.0000],\n",
      "          [-2.0000, -2.0000, -2.0000]],\n",
      "\n",
      "         [[-1.0000, -2.0000, -2.0000],\n",
      "          [-2.0000, -4.0000, -3.0000],\n",
      "          [-1.0000, -2.0000, -1.0000]],\n",
      "\n",
      "         [[-1.0000, -0.0000, -0.0000],\n",
      "          [-2.0000, -1.0000, -0.0000],\n",
      "          [-2.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[-2.0000, -3.0000, -1.0000],\n",
      "          [-4.0000, -5.0000, -2.0000],\n",
      "          [-2.0000, -3.0000, -2.0000]]],\n",
      "\n",
      "\n",
      "        [[[-5.0000, -2.0000, -4.0000],\n",
      "          [ 1.0000, -1.0000, -0.0000],\n",
      "          [-0.0000, -1.0000, -0.0000]],\n",
      "\n",
      "         [[ 7.0000,  7.0000,  1.0000],\n",
      "          [ 2.0000,  2.0000, -2.0000],\n",
      "          [-1.0000, -0.0000, -3.0000]],\n",
      "\n",
      "         [[ 0.0000, -2.0000, -3.0000],\n",
      "          [ 0.0000, -1.0000, -2.0000],\n",
      "          [-0.0000, -1.0000, -3.0000]],\n",
      "\n",
      "         [[-7.0000, -2.0000,  0.0000],\n",
      "          [-0.0000,  3.0000,  6.0000],\n",
      "          [-0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[ 2.0000, -2.0000, -0.0000],\n",
      "          [ 6.0000,  7.0000,  4.0000],\n",
      "          [ 2.0000,  1.0000, -2.0000]],\n",
      "\n",
      "         [[ 0.0000,  1.0000, -2.0000],\n",
      "          [-1.0000,  2.0000,  3.0000],\n",
      "          [-6.0000, -6.0000, -3.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -0.0000],\n",
      "          [-1.0000, -0.0000,  0.0000],\n",
      "          [-2.0000, -1.0000, -0.0000]],\n",
      "\n",
      "         [[-1.0000,  4.0000, -2.0000],\n",
      "          [-2.0000,  5.0000,  6.0000],\n",
      "          [-3.0000, -3.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0000, -0.0000],\n",
      "          [-2.0000, -2.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -2.0000, -2.0000],\n",
      "          [-1.0000, -2.0000, -2.0000]],\n",
      "\n",
      "         [[-0.0000, -2.0000, -2.0000],\n",
      "          [-0.0000, -2.0000, -3.0000],\n",
      "          [-0.0000, -2.0000, -3.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -1.0000],\n",
      "          [-4.0000, -4.0000, -2.0000],\n",
      "          [-3.0000, -3.0000, -3.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -1.0000],\n",
      "          [-2.0000, -4.0000, -3.0000],\n",
      "          [-1.0000, -2.0000, -2.0000]],\n",
      "\n",
      "         [[-0.0000, -1.0000, -1.0000],\n",
      "          [-3.0000, -3.0000, -2.0000],\n",
      "          [-2.0000, -2.0000, -2.0000]],\n",
      "\n",
      "         [[-2.0000, -1.0000, -1.0000],\n",
      "          [-3.0000, -3.0000, -2.0000],\n",
      "          [-3.0000, -3.0000, -2.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -1.0000],\n",
      "          [-4.0000, -2.0000, -1.0000],\n",
      "          [-3.0000, -3.0000, -2.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  2.0000,  2.0000],\n",
      "          [ 2.0000,  3.0000,  5.0000],\n",
      "          [ 2.0000,  2.0000,  6.0000]],\n",
      "\n",
      "         [[ 0.0000,  1.0000,  4.0000],\n",
      "          [ 5.0000,  4.0000,  7.0000],\n",
      "          [ 0.0000,  4.0000,  7.0000]],\n",
      "\n",
      "         [[-1.0000,  0.0000,  0.0000],\n",
      "          [-1.0000,  0.0000, -1.0000],\n",
      "          [-0.0000, -1.0000, -0.0000]],\n",
      "\n",
      "         [[-7.0000, -7.0000, -3.0000],\n",
      "          [-6.0000, -4.0000, -5.0000],\n",
      "          [ 2.0000,  1.0000,  2.0000]],\n",
      "\n",
      "         [[ 5.0000,  6.0000,  3.0000],\n",
      "          [-1.0000, -1.0000, -1.0000],\n",
      "          [ 1.0000,  0.0000,  7.0000]],\n",
      "\n",
      "         [[-3.0000, -2.0000, -1.0000],\n",
      "          [-5.0000, -2.0000, -4.0000],\n",
      "          [-4.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[ 1.0000, -1.0000, -1.0000],\n",
      "          [-0.0000, -1.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  1.0000]],\n",
      "\n",
      "         [[-2.0000,  1.0000,  0.0000],\n",
      "          [-7.0000, -5.0000, -3.0000],\n",
      "          [ 5.0000,  7.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-4.0000,  2.0000,  2.0000],\n",
      "          [ 6.0000,  7.0000,  0.0000],\n",
      "          [ 5.0000,  3.0000,  0.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  0.0000],\n",
      "          [-0.0000, -1.0000, -1.0000],\n",
      "          [-0.0000, -2.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -1.0000, -1.0000],\n",
      "          [ 0.0000, -1.0000, -1.0000],\n",
      "          [ 0.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[ 6.0000,  6.0000,  2.0000],\n",
      "          [ 4.0000,  3.0000, -1.0000],\n",
      "          [ 0.0000, -0.0000, -4.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -5.0000],\n",
      "          [-5.0000,  2.0000,  7.0000],\n",
      "          [-2.0000,  5.0000,  4.0000]],\n",
      "\n",
      "         [[-1.0000,  0.0000, -6.0000],\n",
      "          [-1.0000, -0.0000, -4.0000],\n",
      "          [ 3.0000,  2.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -1.0000, -0.0000]],\n",
      "\n",
      "         [[-2.0000, -2.0000,  2.0000],\n",
      "          [ 1.0000, -5.0000,  4.0000],\n",
      "          [ 4.0000, -2.0000,  4.0000]]],\n",
      "\n",
      "\n",
      "        [[[-2.0000, -0.0000, -1.0000],\n",
      "          [-2.0000, -1.0000, -0.0000],\n",
      "          [ 2.0000,  1.0000,  1.0000]],\n",
      "\n",
      "         [[-2.0000, -2.0000, -1.0000],\n",
      "          [-1.0000, -2.0000,  0.0000],\n",
      "          [-1.0000, -1.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000, -1.0000, -1.0000],\n",
      "          [ 0.0000, -1.0000, -1.0000],\n",
      "          [-0.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[ 1.0000, -1.0000,  6.0000],\n",
      "          [ 5.0000,  5.0000,  7.0000],\n",
      "          [-2.0000, -4.0000,  1.0000]],\n",
      "\n",
      "         [[ 3.0000,  5.0000,  6.0000],\n",
      "          [ 5.0000,  7.0000,  7.0000],\n",
      "          [ 1.0000,  6.0000,  7.0000]],\n",
      "\n",
      "         [[-3.0000, -4.0000, -2.0000],\n",
      "          [ 4.0000,  4.0000,  4.0000],\n",
      "          [ 5.0000,  4.0000,  5.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -1.0000],\n",
      "          [-0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000,  1.0000,  0.0000]],\n",
      "\n",
      "         [[-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000,  1.0000, -2.0000],\n",
      "          [-7.0000,  2.0000,  7.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  1.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000, -1.0000, -0.0000],\n",
      "          [ 0.0000, -1.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000,  0.0000, -1.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  1.0000, -1.0000]],\n",
      "\n",
      "         [[-0.0000, -1.0000, -0.0000],\n",
      "          [-1.0000, -2.0000, -1.0000],\n",
      "          [-0.0000, -2.0000, -2.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000, -1.0000],\n",
      "          [ 0.0000, -1.0000, -2.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -2.0000, -2.0000],\n",
      "          [-1.0000, -2.0000, -3.0000]],\n",
      "\n",
      "         [[-1.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -1.0000],\n",
      "          [-2.0000, -2.0000,  0.0000],\n",
      "          [-2.0000, -3.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 3.0000,  7.0000,  6.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[ 0.0000,  1.0000,  3.0000],\n",
      "          [ 2.0000,  6.0000,  7.0000],\n",
      "          [ 1.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[ 0.0000, -1.0000, -1.0000],\n",
      "          [-0.0000, -1.0000, -1.0000],\n",
      "          [-0.0000, -1.0000, -1.0000]],\n",
      "\n",
      "         [[-4.0000, -0.0000, -1.0000],\n",
      "          [-2.0000,  0.0000,  0.0000],\n",
      "          [-3.0000,  2.0000,  7.0000]],\n",
      "\n",
      "         [[-2.0000, -2.0000, -1.0000],\n",
      "          [-1.0000,  5.0000,  5.0000],\n",
      "          [ 4.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[-4.0000, -2.0000, -1.0000],\n",
      "          [-5.0000, -4.0000, -3.0000],\n",
      "          [-1.0000, -2.0000,  0.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -0.0000, -1.0000],\n",
      "          [-0.0000, -0.0000, -0.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -2.0000],\n",
      "          [ 7.0000, -3.0000, -2.0000],\n",
      "          [ 7.0000,  7.0000,  4.0000]]]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "w_bit = 4\n",
    "weight_q = model.features[27].weight_q # quantized value is stored during the training\n",
    "w_alpha = model.features[27].weight_quant.wgt_alpha   # alpha is defined in your model already. bring it out here\n",
    "w_delta = w_alpha/(2**(w_bit-1)-1)   # delta can be calculated by using alpha and w_bit\n",
    "weight_int = weight_q/w_delta # w_int can be calculated by weight_q and w_delta\n",
    "print(weight_int) # you should see clean integer numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "interior-oxygen",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  2.0000,  0.0000,  0.0000],\n",
      "          [ 7.0000, 15.0000, 15.0000, 11.0000],\n",
      "          [14.0000, 15.0000, 15.0000,  6.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  5.0000,  0.0000,  0.0000],\n",
      "          [ 4.0000, 15.0000, 15.0000, 12.0000],\n",
      "          [11.0000, 15.0000, 15.0000,  3.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0000,  4.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 2.0000,  2.0000,  2.0000,  2.0000],\n",
      "          [ 2.0000,  3.0000,  3.0000,  2.0000],\n",
      "          [ 2.0000,  3.0000,  3.0000,  2.0000],\n",
      "          [ 2.0000,  2.0000,  2.0000,  2.0000]]],\n",
      "\n",
      "\n",
      "        [[[14.0000, 15.0000, 15.0000,  1.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  8.0000],\n",
      "          [15.0000, 15.0000, 15.0000, 14.0000],\n",
      "          [15.0000, 15.0000, 15.0000, 10.0000]],\n",
      "\n",
      "         [[ 0.0000,  7.0000,  0.0000,  0.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  0.0000],\n",
      "          [15.0000, 15.0000,  0.0000,  0.0000],\n",
      "          [15.0000,  7.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.0000, 15.0000, 15.0000, 15.0000],\n",
      "          [ 9.0000, 15.0000, 15.0000, 15.0000],\n",
      "          [ 4.0000,  8.0000,  9.0000,  8.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 4.0000,  8.0000,  8.0000,  6.0000],\n",
      "          [ 6.0000, 11.0000, 12.0000,  8.0000],\n",
      "          [ 6.0000, 10.0000, 10.0000,  7.0000],\n",
      "          [ 4.0000,  6.0000,  6.0000,  4.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 5.0000, 15.0000, 14.0000,  0.0000],\n",
      "          [15.0000, 15.0000, 15.0000, 10.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  0.0000],\n",
      "          [15.0000, 15.0000, 10.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  9.0000,  0.0000,  0.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  0.0000],\n",
      "          [15.0000, 15.0000,  0.0000,  0.0000],\n",
      "          [ 6.0000,  5.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.0000, 11.0000, 11.0000,  7.0000],\n",
      "          [ 4.0000, 14.0000, 11.0000,  5.0000],\n",
      "          [ 3.0000,  8.0000,  5.0000,  1.0000],\n",
      "          [ 0.0000,  3.0000,  1.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 3.0000,  5.0000,  5.0000,  3.0000],\n",
      "          [ 4.0000,  7.0000,  7.0000,  4.0000],\n",
      "          [ 4.0000,  7.0000,  6.0000,  4.0000],\n",
      "          [ 3.0000,  4.0000,  4.0000,  3.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  6.0000,  3.0000,  0.0000],\n",
      "          [ 9.0000, 15.0000, 15.0000,  3.0000],\n",
      "          [ 1.0000, 15.0000,  5.0000,  1.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  2.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, 12.0000,  1.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  2.0000,  1.0000,  0.0000],\n",
      "          [ 0.0000,  1.0000,  0.0000,  0.0000],\n",
      "          [ 4.0000,  8.0000,  1.0000,  0.0000],\n",
      "          [ 7.0000, 12.0000,  7.0000,  1.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 2.0000,  2.0000,  2.0000,  2.0000],\n",
      "          [ 3.0000,  3.0000,  3.0000,  2.0000],\n",
      "          [ 3.0000,  5.0000,  4.0000,  3.0000],\n",
      "          [ 3.0000,  4.0000,  3.0000,  2.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0000, 15.0000, 11.0000,  0.0000],\n",
      "          [ 2.0000, 15.0000, 15.0000,  1.0000],\n",
      "          [ 8.0000, 15.0000, 15.0000,  5.0000],\n",
      "          [ 9.0000, 15.0000, 11.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  8.0000,  0.0000,  0.0000],\n",
      "          [ 2.0000, 12.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.0000, 10.0000, 13.0000, 11.0000],\n",
      "          [ 2.0000, 10.0000, 12.0000,  9.0000],\n",
      "          [ 1.0000,  6.0000,  6.0000,  3.0000],\n",
      "          [ 0.0000,  1.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 3.0000,  5.0000,  5.0000,  4.0000],\n",
      "          [ 3.0000,  6.0000,  6.0000,  5.0000],\n",
      "          [ 3.0000,  5.0000,  5.0000,  4.0000],\n",
      "          [ 2.0000,  3.0000,  3.0000,  2.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  3.0000,  0.0000,  0.0000],\n",
      "          [ 3.0000, 15.0000, 15.0000,  2.0000],\n",
      "          [ 0.0000, 11.0000,  8.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  1.0000,  0.0000,  0.0000],\n",
      "          [ 9.0000, 15.0000,  8.0000,  0.0000],\n",
      "          [ 0.0000,  3.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  4.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 2.0000,  2.0000,  2.0000,  2.0000],\n",
      "          [ 2.0000,  2.0000,  2.0000,  2.0000],\n",
      "          [ 2.0000,  2.0000,  2.0000,  2.0000],\n",
      "          [ 2.0000,  2.0000,  2.0000,  1.0000]]]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x_bit = 4    \n",
    "x = save_output.outputs[8][0]  # input of the 2nd conv layer\n",
    "x_alpha  = model.features[27].act_alpha\n",
    "x_delta = x_alpha/(2**x_bit-1)\n",
    "\n",
    "act_quant_fn = act_quantization(x_bit) # define the quantization function\n",
    "x_q = act_quant_fn(x, x_alpha)         # create the quantized value for x\n",
    "\n",
    "x_int = x_q/x_delta\n",
    "print(x_int) # you should see clean integer numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "designed-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### input floating number / weight quantized version\n",
    "\n",
    "conv_ref = torch.nn.Conv2d(in_channels = 8, out_channels=8, kernel_size = 3, padding=1, bias = False)\n",
    "conv_ref.weight = torch.nn.parameter.Parameter(weight_int)\n",
    "output_int = conv_ref(x_int)\n",
    "\n",
    "output_recovered = output_int*x_delta*w_delta\n",
    "relu = nn.ReLU(inplace=True)\n",
    "relu_output_recovered = relu(output_recovered)\n",
    "\n",
    "output_ref = save_output.outputs[9][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "157dffd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0432e-06, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "difference = abs( output_ref - relu_output_recovered )\n",
    "print(difference.mean())  ## It should be small, e.g.,2.3 in my trainned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-whole",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-significance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-witch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-barbados",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-serbia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "907cbb87cc825d52daa26d94a4f3471be4a7efbfbc56d778ed32b1cc3c9bdcfc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
